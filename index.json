[{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.1.3. 알고리즘 시간 복잡도의 주요 요소  반복문이 얼마나 시행되었는지에 따라 시간 복잡도의 성능이 결정된다고 할 수 있음. 입력의 크기가 커지면 커질 수록 반복문이 알고리즘 수행 시간을 지배함.  1.2. 복잡도 표기법 유형 1.2.1. Big-O 표기법 \\(O(N) \\)  알고리즘 최악의 실행시간을 표기함 아무리 최악의 상황이라도 이 정도의 성능은 보장한다는 의미 가장 많이(일반적으로) 사용함  1.2.2. 오메가 표기법 \\(\\Omega(N) \\)  최상의 알고리즘 실행 시간을 표기  1.2.3. 세타 표기법 \\(\\Theta(N) \\)  알고리즘 평균 실행 시간을 표기  1.3. Big-O 표기법 1.3.1. \\( O(n) \\)  입력 \\(n \\)에 따라 결정되는 시간 복잡도 함수.  \\(O(1) \\) \\(O(\\log n) \\) \\(O(n) \\) \\(O(n \\log n) \\) \\(O(n^2) \\) \\(O(2^n) \\) \\(O(n!) \\)   입력에 따라 기하급수적으로 시간 복잡도가 늘어날 수 있음.  \\(O(1) \u0026lt; O(\\log n) \u0026lt; O(n) \u0026lt; O(n \\log n) \u0026lt; O(n^2) \u0026lt; O(2^n) \u0026lt; O(n!) \\)    1.3.2. 계산법  \\(O(1) \\)  단순하게 입력 \\(n \\)에 따라 멸번 실행이 되는지 계산함. 실행은 무조건 2회(또는 상수회) 실행한다.  if n \u0026gt; 10: print(n)  \\(O(n) \\)  \\(n \\)에 따라 \\(n \\)번 또는 \\(k \\cdot n + b \\) 실행한다.  for idx in range(n): print(idx)  \\(O(n^2) \\)  \\(n \\)에 따라 \\(n^2 \\)번 또는 \\(k \\cdot n^2 + b \\) 등을 실행한다.  for num in range(n): for index in range(n): print(index)   1.3.3. 표기 방법  시간복잡도는 결국 입력값 \\(n \\)에 따라 성능이 결정됨. 결국 알고리즘 성능에 가장 영향을 끼치는 값을 기준으로 표기함. 따라서 상수 \\(k, b \\)는 표기할 때 생략함 \\(k \\cdot n^2 + b \\)의 경우 Big-O 표기법으로는 \\(O(n^2) \\)으로 표기함.  2. Reference  Fastcampus 알고리즘 / 기술면접 강의  ","permalink":"https://lucaseo.github.io/posts/2021-02-10-complexity/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.","title":"[KR] 알고리즘 복잡도"},{"content":"Hello World 스타트업에서 기업의 비재무 데이터 기반 지속가능 여신 및 신용평가 서비스를 기획하고 개발하는 일을 하는 무늬만데이터사이언티스트입니다.\n전공은 경영학, 첫 커리어는 HR(인사관리)로 시작했지만, 데이터분석 기반 의사결정에 매력을 느끼고 커리어를 전환하게 되었습니다. 저는 여전히 스스로가 야매라고 생각합니다. 적절한 시기에 꼭 정식으로 공부를 할 계획입니다.\n데이터 시각화, 대시보드 구축, 추천시스템, MIR, 음악과 관련된 딥러닝 모델 등 등등 중구난방이지만 호기심이 많습니다. (시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다. 평점은 왓챠에서 매기고, 넷플릭스, 왓챠 둘 다 봅니다. 디즈니도 언젠가는 꼭 \u0026hellip; ! HIPHOPLE.com 스태프를 했었습니다. 요새는 많이 뜸한 상태 스페인어를 아주 조금 할 줄 압니다. 독일어를 배우고 싶습니다. 언젠가는 유럽에 정착하고 싶습니다. 최근에 미디(MIDI) 걸음마를 뗐습니다. 유투브 보는 걸 정말 좋아합니다. 다수의 치즈냥 채널과 리트리버, 보더콜리, 웰시코기 채널을 구독 중입니다.  ","permalink":"https://lucaseo.github.io/about/","summary":"Hello World 스타트업에서 기업의 비재무 데이터 기반 지속가능 여신 및 신용평가 서비스를 기획하고 개발하는 일을 하는 무늬만데이터사이언티스트입니다.\n전공은 경영학, 첫 커리어는 HR(인사관리)로 시작했지만, 데이터분석 기반 의사결정에 매력을 느끼고 커리어를 전환하게 되었습니다. 저는 여전히 스스로가 야매라고 생각합니다. 적절한 시기에 꼭 정식으로 공부를 할 계획입니다.\n데이터 시각화, 대시보드 구축, 추천시스템, MIR, 음악과 관련된 딥러닝 모델 등 등등 중구난방이지만 호기심이 많습니다. (시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다.","title":"About"},{"content":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp\n점검해보자 1. 회사일만 하지 않기 모호하지만, 매우 중요한 항목이다. 작년에는 회사에서 기존에 하던 일과 말도 안 되는 정부과제, 그리고 갑작스러운 혁신금융선정의 삼위일체가 합심하여 나에게 전신마사지를 해주는 바람에 야근과 주말출근을 밥 먹듯이 했다. 결국 기존에 받는 연차 15일과 야근, 주말 근무에 대한 대체휴무가 쌓여, 연말에는 잔여 연차가 23일이 되었다(??) 1년 간 쉰 기억이 나지 않고, 기존 연차보다 잔여연차가 더 많은 상태로 1년을 마무리 하게 되어, 이게 대체 누구를 위한 무슨 짓인가 싶기도 했다. (모든 게 힘듦으로 가득하던 시절 그래서 슬기롭게 내 생활을 챙기기로 했다. 그렇다고 아주 여유롭진 않지만, 너무 힘들지는 않을 만큼, 어떻게 보면 좀 노하우가 생긴 것 같은 삶을 살고 있다.\n\u0026amp;nbsp\n2. 데이터 사이언스 대회 도전   데이콘: 원자력 발전소 상태 판단 알고리즘 대회. 데이콘에서 주관하는 원자력 발전소 상태 판단 대회에 참가했다. 자세한 후기는 이미 블로그에서도 작성한 바 있다. 대회에서 중간에 포기하지 않고 꾸준히 참여했다는 점에서 소기의 목적을 달성했다고 생각하고, 좀 더 열심히 했다면 더 좋은 성적을 거둘 수 있었을 것이라 생각한다.\n  카카오 아레나: Melon Playlist Continuation. 사실 카카오 아레나의 멜론 플레이리스트 추천 대회는 정말 많은 관심을 가지고 있는 분야인 만큼 의욕이 매우 높았던 대회였다. 추천시스템에 대한 개념도 잘 없었기 때문에, 참가와 동시에 추천시스템에 대한 공부를 차근차근 하고자 했다. 하지만 참가만 걸어 놓은 상태로 그 이상의 제출은 해보지 못 하고 대회가 마무리 되어 많은 아쉬움이 남는다. 애초에 제출 이전부터 선행되어야 하는 추천시스템에 대한 공부가 지지부진 했기 때문에, 내가 생각하기에도 변명의 여지가 없는 좀 한심한 결과가 나오게 됐다.\n  \u0026amp;nbsp\n3. 간단한 웹어플리케이션 구축 연초부터 사이드 프로젝트로 여러가지 역할을 수행할 수 있는 유연한 웹어플리케이션을 구축하고자 하는 와중에 Streamlit을 경험하고, 이것저것 시도해보게 되었다. 글또에서도 관련된 포스팅을 작성한 바 있다. 프론트엔드에 대한 걱정 없이 순수 파이썬으로 매우 간편하게 개발할 수 있기 때문에, 실제로 직장 업무에서도 개발팀이 참고할 수 있도록 데이터 QC, 레이블링 기능에 대한 프로토타입이나 대시보드 등을 만들어서 사내 공유하거나 함께 사용하고 있다.\n\u0026amp;nbsp\n4. 독일 가족 방문하기 마지막으로 독일을 방문한건 2018년. 올해만큼은 꼭 시간을 내서 누나의 가족들이 사는 독일을 방문하고자 했지만 망할 COVID-19 때문에 패스하게 되었다. 내가 가야 그나마 조카들이 한국말을 덜 까먹는데, 이렇게 조카들의 한국말은 더욱 뒷걸음질 치는건가 싶다. 언제쯤 가능하게 될까. 이건 기약이 없어서 더욱 안타까운 항목이다.\n\u0026amp;nbsp\n5. 글또 솔직히 말하자면, 글또 생활은 생각보다 어렵다. 글을 잘 쓰는 것도 어렵고, 내가 잘 모르는 내용에 대해 정독하고 피드백이나 감상을 남기는 것도 쉽지는 않다. 모자라서 그런건 아니고 아마 처음해봐서 그럴 수도 있다. 그래서 더더욱 차감 없는 슬기로운 글또 생활을 영위하고자 노력하고 있다. 꾸역꾸역 \u0026hellip; 이라는 의성어는 좀 뉘앙스가 그렇지만, 어쨌든 그런 비슷한 상황이다. 나쁘게 말하면 꾸역꾸역이고 좋게 말하면 조금조금씩 발전을 꾀하는 존버라고 할 수 있겠다. 아무튼 현재까지는 예치금 차감은 없다. 요태까지 그래와고 아패로도 개속.\n\u0026amp;nbsp\n(+) 지난 3월 방송통신대 정보통계학과 3학년에 편입했다. 해외 대학원을 알아보던 중, 비전공자인 나는 데이터사이언스와 직접적인 연관이 있는 학과목의 수강학점이 없기 때문에 지원부터가 제한이 있다는 걸 알게 되어서, 이 부분을 타개해나갈 방법을 찾고 있었다. 경영과 연계된 MIS관련 대학원, 부분 수강, MOOC를 통한 크레딧 등 다양한 방법을 알아봤지만, 역시 편법은 없었고, 3학년으로 편입하여 정식 대학과목을 수강하는 것이 가장 확실했다. 직장생활과 병행하는 방법은 방송통신대 뿐이었는데, 한 학기 등록금이 30만원대로 매우 저렴했기 때문에, 부담 없이 경험해보는 셈 치고 편입을 결정했다.\n방송통신대는 대학교 특성상 COVID-19로 인한 타격도 1도 없이 매우 정상적으로 학사일정이 진행되었다. (1은 있었다고 볼 수도 있다. 기말고사만큼은 오프라인으로 이루어지는데, 이 조차 과제물로 전환되었기 때문.)\n계획에 없었던 편입이고 주변 사람들도 우려를 많이 표했지만, 직장생활 병행과 대학원 진학을 위한 장기 계획의 일부분인 만큼, 잃은 것보다는 얻은 것이 많은 한 학기였다. 방송통신대에 대한 자세한 후기는 이번 2학기가 끝난 이후 포스팅할 계획이다.\n\u0026amp;nbsp\n마무리 하며 난 나름 뭔가 많이 했다고 생각했는데, 막상 돌아보니 매우 소박했다는 걸 느꼈다. 동시에 남은 5개월과 2021년에 대한 구상도 할 수 있게 되어 의미있는 회고였다. 역시 글또를 하지 않았다면 이런 글도 남기지 않았을 거란 생각이 들기도 한다. 남은 5개월은 또 얼마나 빨리 지나갈까. 적절한 타이밍에 남은 연차를 마저 소진하고 (잘 휴식하겠다는 뜻) 글또도 잘 마무리 할 수 있는 (예치금 차감 당하지 않겠다는 뜻) , 그리고 방송통신대 두번 째 학기도 초심 잃지 않고 수강할 수 있는 (A+를 받겠다는 뜻) 하반기가 되었으면 한다.\n","permalink":"https://lucaseo.github.io/posts/2020-08-02-review-2020-1st-half/","summary":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp","title":"[KR] 2020년 상반기가 지났다"},{"content":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)\n대회 개요    수력 원자력 공사인데 배경은 풍력 발전 ... ?   \u0026amp;nbsp\n원자력발전소 상태 판단 대회는 한국수력원자력(주)에서 제공한 발전소 모의 운전 데이터를 통해 원자력 발전소의 상태를 판단하는 것이 태스크로 주어진다.\n평가지표 평가 지표는 Log loss 이다. Log loss 값은 0 ~ 1 사이로 산출되며, 낮고 0에 가까울 수록 모델의 예측력이 좋음을 의미한다. (데이콘 측 평가지표 설명 영상)\n$$ \\text{logloss}(\\cdot) = \\frac{-1}{N}\\sum_i^N \\sum_j^M y_{ij} \\log{p_{ij}} $$\n데이터셋 원자력 발전소 모의 데이터는 기본적으로 828개의 발전소 운전 Train 데이터 파일과 각 파일에 부여된 198가지 상태 레이블이 매핑된 Label 파일이 주어진다. 압축을 해제하면 총 81GB에 달했다.\n![]({{ site.baseurl }}/images/2020-07-05-review-dacon-nuclear-competition/2.png)\n   \u0026amp;nbsp\n각각의 파일에 저장된 Train 데이터셋 위의 그림과 같으며, 발전소가 10분 동안 작동한, 즉 1행 당 1초 즉, 600행으로 이루어진 데이터가 주어진다. 모든 데이터셋은 발전소의 상태가 변하기 전 디폴트 상태_A(999) 와 상태가 변한 후 상태_B 데이터를 담고 있으며, 상태_A에서부터 시작된다. 데이터는 0초에서 15초 사이에서 상태가 변하기 시작한다. 따라서 데이터 상태의 변화가 0초에서 발생한다는 말은 상태_A가 없는 좌측의 데이터셋과 같고, 그 이외에는 우측과 같다고 보면 된다.\n   \u0026amp;nbsp\n다만 위의 그림과 같이, 실제로는 몇 초부터 상태가 변하는지에 대한 정보가 주어지지 않기 때문에, 각 데이터셋이 좌측과 같은지 우측과 같은지는 한 눈에 판단이 어렵다.\n데이터 전처리 EDA 과정에서 의미 있는 인사이트를 도출해내지 못 했다. 그리고 대회 경험이 적고 시간이 촉박했던 관계로, 바로 전처리에 돌입했다.\n Label이 999 인 경우 제외한다. 10분 간의 운전 데이터 기록 컬럼 내 unique한 값이 \u0026lt; 10 인 컬럼은 제외한다. 데이터셋 중 str타입의 데이터가 발생할 경우 NaN 치환한다. 마지막으로 NaN 데이터는 0으로 채운다. Train / Eval 데이터셋은 3:1의 비율로 분리한다.  #1 train = train[train[\u0026#39;label\u0026#39;]!=999].reset_index(drop=True) train_label = train.label train = train.drop([\u0026#39;id\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;label\u0026#39;], axis=1) #2 with open(\u0026#39;filter_col.txt\u0026#39;, \u0026#39;r\u0026#39;) as filehandle: list_ = filehandle.readlines() list_ = [col.replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) for col in list_] train= train[list_] # 3 for col in train.columns: if train[col].dtype != \u0026#39;float64\u0026#39;: train[col] = pd.to_numeric(train[col], errors=\u0026#39;coerce\u0026#39;) #4 train = train.fillna(value=0, axis=1) #5 X_train, X_valid = train_test_split(train, test_size = .25, random_state=42) y_train, y_valid = train_test_split(train_label, test_size = .25, random_state=42) 모델 모델은 LightGBM을 선택했다. Tabular dataset의 분류 문제에는 나무모형 기반의 모델이 가장 적합했고, LightGBM, XGBoost, Random Forest를 초기 테스트 한 결과 LightGBM이 가장 빠르고 성능이 좋았기 때문이다.\n부스팅 방법으로는 가장 기본적인 Gradient Boosted Decision Tree를 선택했다. 또한 데이터셋이 커서 연산이 큰 문제와 오버피팅을 방지하기 위해 bagging_fraction, feature_fraction 파라미터의 설정으로 데이터셋의 행과 열을 0.5 비율로 고정하여 학습 중 샘플링할 수 있도록 했다.\n그 외 objective, num_class, metric과 같이 대회의 목적에 맞게 변경한 파라미터를 제외하고는 default를 가져온 것들이 대부분이다.\nimport lightgbm as lgb X_train = X_train.to_numpy() X_valid = X_valid.to_numpy() y_train = y_train.to_numpy() y_valid = y_valid.to_numpy() train_data = lgb.Dataset(X_train, label=y_train) #, feature_name=X_train.columns) valid_data = lgb.Dataset(X_valid, label=y_valid) #, feature_name=X_valid.columns) param = { \u0026#39;objective\u0026#39;: \u0026#39;multiclass\u0026#39;, \u0026#39;num_class\u0026#39;: 198, \u0026#39;boosting\u0026#39;:\u0026#39;gbdt\u0026#39;, \u0026#39;num_leaves\u0026#39;:32, \u0026#39;max_depth\u0026#39;:20, \u0026#39;min_data_in_leaf\u0026#39;:20, \u0026#39;metric\u0026#39;:\u0026#39;multi_logloss\u0026#39;, \u0026#39;learning_rate\u0026#39; : 0.01, \u0026#39;verbose\u0026#39; : -1, \u0026#39;bagging_freq\u0026#39; : 1, \u0026#39;bagging_fraction\u0026#39; : 0.5, \u0026#39;feature_fraction\u0026#39; : 0.5, } evals_result={} num_round = 2000 lgbst = lgb.train(params=param, train_set=train_data, num_boost_round=num_round, valid_sets=[valid_data], evals_result=evals_result, early_stopping_rounds=1000, verbose_eval=10) lgbst.save_model(\u0026#39;model_lgb.txt\u0026#39;, num_iteration=lgbst.best_iteration) 결과 결과는 가채점 기준 36위 / 201팀, 최종 데이터셋 기준 채점 및 중복 및 부정 제출 등의 여부가 판결 뒤 산출된 최종 순위는 16위 / 187팀를 기록했다. 전체 참가팀만 놓고 보면 1091팀이지만, 실제로 제출한 팀은 20%에 그친 것을 확인할 수 있었다.\n   소감 무엇보다 아쉬운점은 초반의 의욕과는 달리, 데이터사이언스 대회의 best practice를 실습해보지 못 했고, 제출과 점수에 급급한 채로 마무리 했다는 점이다. 시간 부족과 의사소통 부재로 EDA 깊이 있게 하지 못 했고, Hyperparameter tuning과 grid search를 제대로 시행하지 못 햇다.\n마지막으로, 이번 원자력발전소 상태 판단 대회 참가는 데이터사이언스 관련 대회 경험을 쌓기 위해서였지만, 한편으로는 전혀 접하지 못 했던 원자력발전소 관련 데이터를 접하고 발전소 관련 도메인을 조금이나마 얻기 위함이기도 했다. 그러나 데이터셋의 컬럼은 모두 비식별 처리가 되어 있어 무엇을 의미하는지 데이터의 특성과 정보에 대한 접근이 불가했다는 점이 대회를 참가하는 와중에 흥미가 조금 깎이게 된 요인이 되지 않았나 하는 생각이 든다.\n어쨌거나 저쨌거나 완주를 했고, 상위 10% 내 라는 기대도 하지 않았던 성적으로 마무리를 했기 때문에 여기에 의의를 두며, 다음에 참가하는 데이터사이언스 대회는 이번에 아쉬웠던 점들이 꼭 보완될 수 있도록 다짐을 해본다.\n","permalink":"https://lucaseo.github.io/posts/2020-07-05-review-dacon-nuclear-competition/","summary":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)","title":"[KR] 데이콘 원자력발전소 상태 판단 대회 후기"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다. (영상)\n원본의 Figure. The Wagon Wheel Effect에서 하단의 sampling rate 슬라이더를 조절하며 왼쪽의 시계방향으로 회전하는 물체와, 오른쪽의 sampling rate에 따른 스냅샷(혹은 샘플)를 관찰해보자\n   \u0026amp;nbsp\n직관적으로 봤을 때, 왼쪽과 마찬가지로 오른쪽 스냅샷도 시계방향으로 회전하는 듯한 모션을 취하기 위해서는 최소한 왼쪽에서 한번 회전하는 동안 2번 이상의 스냅샷을 찍어야 한다. 만약 sampling rate을 엄청 낮게 설정한다면 스냅샷은 기존의 회전 방향과는 반대로 반시계 방향으로 찍히게 된다.\n회전 방향이 결정되지 않고 정지해 있는 듯한 특이한 케이스도 있다. 만약 sampling rate을 한바퀴당 1번으로 설정한다면 스냅샷에는 움직임이 없을 것이고, sampling rate을 한바퀴당 2번으로 설정한다면 스냅샷은 그냥 앞뒤로만 움직이기 때문에 진행 방향을 유추할 수 없게 될 것이다.\nSine Wave Aliasing : Multiples of the sampling rate Sive wave에는 다음과 같은 법칙이 있다.\n sampling rate이 \\(SR\\) 헤르쯔와 정수 \\(K\\)가 주어졌을 때,\n\\(F\\) 라는 frequency를 지닌 sine wave가 있고, \\(F+(k * SR)\\) 의 진동수를 지닌 sine wave를 샘플링했다고 할 때, 둘은 서로 구별이 가능하지 않다.\n 예를 들어, Sampling rate이 6Hz라고 했을 때, 다음 두 그룹의 sive wave는 서로 구별이 가능하지 않다.\n 샘플링을 거친 1Hz의 sine wave 샘플링을 거친 다음 세 가지의 sine wave  \\( 1+(1 * 6) = 7Hz \\) \\( 1+ (2 * 6) = 13Hz \\) \\( 1+(3 * 6) = 19Hz \\)    (사실 여기까지는 조금 이해가 안 갔는데 \u0026hellip;)\n원본의 Figure. All Sampled Sine Waves Have Aliases을 보면 더욱 직관적이다. Figure 1.의 Time domain 그래프는 1Hz sine wave(파랑색)와, 그 외 sine wave(회색)을 비교하고 있다. 샘플링을 거치고 나면(Frequency domain 그래프), 샘플링이 표현하는 부분은 파랑색과 회색 다 일치하기 때문에, 샘플링을 한 뒤에는 각각 어떤 frequency 였는지 구분을 할 수 가 없게 된다.\n   \u0026amp;nbsp\n이 말인즉슨, 샘플링을 거친 sine wave는 무한대의 alias를 가진다는 것이다. 그저 기존의 frequency에 sampling rate의 배수만 더해주면 기존 frequency에 대한 새로운 alias가 형성된다. 따라서 이 법칙은, 어떠한 신호라도 샘플링을 거치면 다른 샘플링 신호와 구별이 가능하지 않는 상황이 올 수 있다는 것을 의미한다.\n","permalink":"https://lucaseo.github.io/posts/2020-06-20-dsp-basic-s01-9/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Wagon Wheel Effect \u0026 Aliasing"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.\n 특정 신호 내 가장 높은 frequency를 알고 있다면, 샘플링 주기를 그 두배로 설정하면 된다는 뜻이다. 예를 들어 300Hz sine wave 를 샘플링하려 한다면, 우리는 두배 이상 즉, 600Hz 를 샘플링해야 한다. 반대로 샘플링 주파수가 두배보다 작을 경우, 간섭이 일어나며 앞에서 언급한 aliasing(참고)이 발생하게 된다.\n신호 위주가 아닌 sampling rate 위주로 법칙을 정하면 다음과 같다.\n 샘플링 주기 FS hertz에 대하여, FS/2 혹은 그 이상인 frequency를 가진 신호는 적절하게 샘플링할 수 없다.\n 특히 sampling rate의 절반 즉, FS/2 라는 값은 Nyquist Limit 또는 Nyquist Frequency라고도 불리며, 샘플링 법칙을 논할 때 매우 중요하게 다뤄지는 값이다.\n원글의 Figure 1.(링크)의 Sampling period 값을 조절하면서 직관적으로 이해해보자. Sampling rate이 16Hz일 경우, FS/2는 8이다. 모든 네가지 sine wave의 경우 8을 넘지 않으므로 샘플링이 적절하게 가능하다.\n   \u0026amp;nbsp\n하지만 sampling rate을 4Hz으로 설정하면 FS/2의 값은 2이다. 따라서 2Hz, 3Hz, 4Hz의 sine wave는 제대로 샘플링 되지 않는다.\n   \u0026amp;nbsp\n지난 글에서 인간은 20Hz ~ 20,000Hz 사이의 소리만 들을 수 있다고 말한 바 있다. 따라서 샘플링 법칙에 따라 인간이 들을 수 있는 범위 내에서 음악의 sampling rate를 정하고자 하면, 20,000의 두배 즉 40,000Hz 이상이 되어야 한다. 오디오와 음악의 주파수가 40,000Hz 근처(정확하게 말하자면 44,100Hz)인 것은 이 이유 때문이다.\n\u0026amp;nbsp\nNyquist Frequency를 초과하는 경우 샘플링 법칙은 Nyquist Frequency(샘플링주기의 절반)에 한해서는 어떤 신호든 정확히 샘플링할 수 있고, 반대로 샘플링하고자 하는 샘플이 Nyquist frequency보다 높은 frequency를 가지고 있다면, 이 샘플링이 제대로 이루어지지 않는다는 것을 알게 되었다. 그렇다면 Nyquist Frequency를 초과하는 신호는 샘플링이 이루어지지 않는 이유에 대해서 알아보자.\n원글의 Figure 1.(링크)에서 sampling rate은 24Hz이고 Nyquist frequcny는 24Hz의 절반인 12Hz이다. 플레이 했을 때, 신호가 Nyquist frequency를 지나면, 신호의 샘플들은 하나의 파랑색의 파형 뿐 아니라 새롭게 생성된, 점점 감소하는 frequency 형태의 회색의 파형까지도 sampling 결과를 통해 표현이 가능하게 된다.\n   \u0026amp;nbsp\n이렇게 신호의 frequency를 제한하지 않고 Nyquist limit을 넘도록 허용하면, 샘플링된 신호는 Nyquist frequency가 투영된 새로운 sine wave를 표현하게 된다. 결국 실제로는 존재하지도 않는 회색 신호를 샘플링에 포함시키게 되는 결과를 낳게 되는 것이다.\n다음 포스팅에서 alias에 대한 다른 예시를 더 깊게 다루어서 정리를 할 예정이다.\n Additional material https://youtu.be/5wyYgy6LPyQ\n","permalink":"https://lucaseo.github.io/posts/2020-06-07-dsp-basic-s01-8/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 나이퀴스트 샘플링 법칙 (Nyquist Sampling Theorum)"},{"content":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.5 ~ 5점 사이로 평가를 내릴 수 있다. 넷플릭스에서 보유한 사용자 평가 데이터에는 MAP가 사용될 수 있지만, 왓챠에서는 MAP가 적절하지 않을 것으로 생각된다.\n반면에 nDCG(Normalized Discounted Cumulative Gain)의 경우 복수의 컨텐츠가 relevance를 가지고 있다고 하더라도, 그 정도(점수)에 따라 어떠한 컨텐츠가 더 관련있고, 덜 관련 있는지를 평가할 수 있다. 대부분의 검색과 추천시스템의 경우 다수의 사용자들은 1, 2페이지 또는 상위의 리스트만 참조할 것이기 때문에 상위 리스트 사이에서 변별력을 갖춰야 하는 경우 nDCG를 통한 평가가 설득력을 얻는다.\n\u0026amp;nbsp\nCumulative Gain (CG) nDCG에서는 우선 CG이란 개념이 등장한다. CG는 Cumulative Gain 이라는 이름에서도 알 수 있듯, 전체 추천된 리스트에 대하여 gain의 총 합을 구한 것이다.\n$$CP_{p} = \\sum_{i=1}^{p} rel_{i}$$\n \\(p\\) : 추천된 아이템 \\(rel_{i}\\) : i번 째 아이템의 relevance 정도. Gain 이라고 한다.  예시) 추천시스템의 결과와 Relevance\n   Rank Relevance     1 3   2 3   3 3   4 4   5 2   6 2    $$CG = 17$$\n\u0026amp;nbsp\nDiscounted Cumulateive Gain (DCG) DCG 에서는 각 추천된 아이템의 relevance를 log함수로 나누어 값을 구한다. log 함수로 나누어주는 부분은, 랭킹의 위치에 따른 페널티를 주는 효과를 가진다. 순위의 값이 클 수록(즉, 순위가 낮을 수록) DCG의 값은 작아진다. 하지만, 높은 순위의 경우 간격이 크고, 낮은 순위의 경우 실제 체감하는 차이는 낮다.\n$$DCG_{p} = \\sum_{i=1}^p \\frac{rel_i}{log_2(i+1)} = rel_1 + \\sum_{i=2}^p \\frac{rel_i}{log_2i}$$\n예시) 각 추천된 컨텐츠 당 Discounted Gain\n   Rank Relevance Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\)   2 3 \\(3 / \\log_2(2+1) = 1.89\\)   3 3 \\(3 / \\log_2(3+1) = 1.5\\)   4 4 \\(4 / \\log_2(4+1) = 1.72\\)   5 2 \\(2 / \\log_2(5+1) = 0.77\\)   6 2 \\(2 / \\log_2(6+1) = 0.71\\)    $$DCG = 9.59$$\n\u0026amp;nbsp\nNormalized DCG (nDCG) DCG는 현재 추천시스템이 추천한 결과에 대한 상태를 보여주는데, DCG만 놓고 볼 경우 추천된 아이템의 갯수에 따라 DCG가 다를 수 있으므로, 이를 0~1사이의 값으로 정규화 해줄 필요성이 있다. 따라서 현재 추천된 리스트의 결과에 기반한 DCG를 현재 추천된 결과의 가장 이상적인 형태를 가정했을 때의 DCG(ideal DCG, iDCG)로 나누어서 정규화한다.\n사용자가 컨텐츠를 평가하지 않은 경우와, 관련성이 아예 없는 경우 manual적으로 값을 0으로 설정하거나, 적절하게 imputation을 취해주어야 한다는 취약점이 있다. 하지만, nDCG는 relevance가 등급이나 범위로 매겨지거나(graded relevance), 이분법적인 경우(binary relevance) 둘 다 평가가 가능하다. 또한 log함수를 통해 순서에 대한 가중치가 주어지므로 추천시스템에 적용하기 매우 적절한 평가지표라고 할 수 있다.\n$$IDCG_p = \\sum_{i=1}^{|REL_p|} \\frac{2^{rel_i} - 1}{log_2(i+1)}$$\n$$nDCG_p = \\frac{DCG_p}{IDCG_p}$$\n예시) Ideal Relevance와 Discounted Gain\n   Rank Relevance Discounted Gain Ideal Relevance Ideal Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\) 4 4   2 3 \\(3 / \\log_2(2+1) = 1.89\\) 3 1.89   3 3 \\(3 / \\log_2(3+1) = 1.5\\) 3 1.5   4 4 \\(4 / \\log_2(4+1) = 1.72\\) 3 1.16   6 2 \\(2 / \\log_2(6+1) = 0.71\\) 2 0.71   5 2 \\(2 / \\log_2(5+1) = 0.77\\) 2 0.77    $$ nDCG_p = \\frac{DCG_p}{IDCG_p} = \\frac{9.59}{10.03} = 0.95 $$\nReference  위키피디아 MRR vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them  ","permalink":"https://lucaseo.github.io/posts/2020-05-10-normalized-discounted-cumulative-gain/","summary":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.","title":"[KR] 추천시스템의 평가 지표 : nDCG"},{"content":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)\n추천 시스템을 통해 추천되는 아이템의 경우 추천의 정도가 동일하지 않다. 대부분의 추천 결과는 다음처럼 나올 수 있다고 생각해 볼 수 있다.\n 1순위 : 가장 관심을 가질만한 것.\n2순위 : 그 다음 차선책으로 관심을 가질만한 것.\n3순위 : 그 다음으로 사용자가 관심을 가질만한 것.\n4순위 : 또 그 다음 \u0026hellip; \u0026hellip;\n \u0026amp;nbsp\nMean Average Precision (MAP) 은 순서 또는 순위를 감안하는 부분을 반영하여 추천 시스템의 성능을 평가하는 지표로서, 과거 캐글의 Stander Product Recommendation, 카카오아레나의 브런치 사용자를 위한 글 추천 대회 등 추천 시스템 관련 컴퍼티션에서 채점 방식으로 적용되었다. 특히 분류 문제에서 흔히 언급되는 Precision과 Recall이 적용된 성능평가 방법으로, 아주 낯설지는 않다.\n\u0026amp;nbsp\nPrecision \u0026amp; Recall     Predict Positive Predict Negative     Actual Positive True Positive False Negative   Actual Negetave False Positive True Negative    MAP에 대한 개념는 Precision과 Recall에서부터 시작한다. 일반적으로 위와과 같이 confusion matrix가 있다고 할 때, Precision과 Recall은 다음과 같다. (더 자세한 설명은 링크를 참조하도록 하자)\n$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$$\n$$\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$$\n\u0026amp;nbsp\n추천시스템 관점에서의 Precision \u0026amp; Recall 추천시스템에서는 Precision과 Recall을 다음과 같이 해석할 수 있다. 추천시스템에서는 분자 부분을 relevant(관련있는) 라고 표현하기도 한다.\n     Precision 또는 \\(P\\):\n 추천한 아이템 중, 실제로 사용자의 관심사와 겹치는 아이템의 비율 \\(\\text{Precision} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{Total items from recommendation}}\\)    Recall 또는 \\(r\\):\n 실제로 사용자가 관심을 가진 아이템 중, 추천된 아이템이 겹치는 비율 \\(\\text{Recall} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{User\u0026rsquo;s interest}}\\)    \u0026amp;nbsp\nCutoff (@K) MAP에서는 Cutoff의 개념이 등장한다. Cutoff는 \u0026ldquo;잘라낸다\u0026quot;는 뜻으로, 쉽게 말하면 \u0026ldquo;상위 K개만 고려하고 그 아래로는 쳐내기\u0026rdquo; 라고 이해하면 된다. Cutoff를 가질 경우에는, @K 를 덧붙여서 표기한다.\n어떠한 사용자의 기록을 통해서 자동차 용품와 관련된 아이템을 추천한 결과가 다음과 같다고 하자.\n   순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 운전자 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    다음 예시의 추천시스템의 결과에 대하여 \\(k\\)개 Cutoff를 적용하여 Precision을 구한다면, 이를 Precision@K라고 한다. Precision@K는 Cutoff에 따라 달라질 수 있다.\n\u0026amp;nbsp\n Cutoff k=10인 경우 \\(\\rightarrow P(k=10) = 0.6\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 자동차 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    \u0026amp;nbsp\n Cutoff k=3인 경우 \\(\\rightarrow P(k=3) = 1\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답    \u0026amp;nbsp\n Cutoff k=5인 경우 \\(\\rightarrow P(k=5) = 0.8\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답    \u0026amp;nbsp\n Cutoff k=7인 경우 \\(\\rightarrow P(k=7) = 0.714\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답    \u0026amp;nbsp\nAverage Precision (AP@K) Cutoff가 \\(K\\)개인 Average Precision(AP@K)은 Precision@K의 평균을 구하는 과정이다.\n$$ AP@K = \\frac{1}{m} \\sum_{j=1}^K P(j) \\cdot rel(j) \\dots \\begin{cases} rel(j)=1 \u0026amp; \\text{if } j^{th} \\text{ item is relevant}, \\cr rel(j)=0 \u0026amp; \\text{if } j^{th} \\text{ item is not relevant}, \\cr \\end{cases} $$\n \\(K\\) : Cutoff 갯수 \\(m\\) : 추천 아이템 중 relevance가 있는 아이템의 갯수 (number of relevant document) \\(j\\) : 전체 추천 아이템 리스트 중, 해당 추천 아이템의 index \\(P(j)\\) : \\(j\\)번째 까지의 precision값 \\(rel(j)\\) : \\(j\\)번째의 relevance 여부  \u0026amp;nbsp\n위에서 예시로 들었던 자동차용품 추천결과를 통해, [\\(AP@5\\), \\(AP@7\\), \\(AP@9\\), \\(AP@10\\)] 을 계산해 보았다. 특히 \\(AP@7\\) 와 \\(AP@9\\) 의 결과에서 관찰할 수 있듯이, \\(\\frac{1}{m}\\)에서 \\(m\\)은 relevance가 있는 경우만을 포함하기 때문에, 뒤에서 오답이 추가되어도 AP의 값이 페널티를 받지는 않는다.\n$$AP@5 = \\frac{1}{4} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5}\\right) = 0.95$$\n$$AP@7 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} \\right) = 0.90$$\n$$AP@9 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 \\right) = 0.90$$\n$$AP@10 = \\frac{1}{6} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 + \\frac{6}{10}\\right) = 0.85$$\n\u0026amp;nbsp\n또한, 아래의 예시에서 Case A와 Case B를 비교해보면, 순위가 높은 추천 아이템이 정확할 수록 높은 AP값이 계산되므로, 추천의 순서 또는 순위가 평가 지표에 영향을 끼침을 알 수 있다.\n$$AP@5 = \\frac{1}{3} \\cdot \\left(\\frac{1}{1} + 0 + \\frac{2}{3} + 0 + \\frac{3}{5}\\right) = 0.75 \\dots \\text{(Case A)}$$\n$$AP@5 = \\frac{1}{3} \\cdot \\left(0 + \\frac{1}{2} + 0 + \\frac{2}{4} +\\frac{3}{5} \\right) = 0.53 \\dots \\text{(Case B)}$$\n\u0026amp;nbsp\nMean Average Precision (MAP@K) AP는 각각의 사용자(또는 쿼리)에 대하여 계산한 것이므로, 각 사용자에 따라 AP값이 산출된다. Mean Average Precision(MAP)은 AP값들의 Mean을 구한 것으로, 식은 다음과 같다.\n$$MAP@K = \\frac{1}{U} \\sum_{u=1}^{U} (AP@K)_u$$\n \\(U\\) : 총 사용자의 수  \u0026amp;nbsp \u0026amp;nbsp\n마무리하며 이번 MAP에 대해 알아보았다. 다음 포스트에서는 역시나 추천시스템의 평가지표로 자주 등장하는 DCG(Discounted Cumulative Gain)에 대해서 공부하고 정리 할 예정이다.\nAP, MAP의 파이썬 코드로 된 구현체는 링크를 통해 참조할 수 있다.\n\u0026amp;nbsp \u0026amp;nbsp\nReference   https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\n  http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n  http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html\n  ","permalink":"https://lucaseo.github.io/posts/2020-04-23-mean-average-precision/","summary":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)","title":"[KR] 추천시스템의 평가 지표 : MAP"},{"content":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움. 하지만 아무도 두려워하지 않는 듯해 보여서 더 두려운 고요한 두려움.\n\u0026ldquo;대체 언제까지 슬랙으로 엑셀 파일을 주고 받아야 하는가?\u0026ldquo;에 대한 해답을 찾던 중, 구글 스프레드시트(Google Spreadsheet)와 연동하는 것으로 몇가지 고민을 해결할 수 있게 되었다. gspread라는 라이브러리를 찾게 되었다. 별 거 아닌데, 왜 여태 사용해보지 않았을까. 이거다 싶었다.\n   바로 적용한 아주 간단한 예시를 들자면, 리서쳐들이 구글 스프레드시트에서 작업한 것들을 DB에 적재하는 과정을 자동화할 수 있었고, 데이터를 누가 작업했고, 업데이트가 되었는지에 대한 여부 또한 스프레드시트를 기반으로 작업하게 되니 해결 되었다. Python과 gspread 통해서 구글 스프레드시트와 연동하는 과정을 다룬 사내 튜토리얼을 다시 정리해보았다.\n\u0026amp;nbsp\n1. 튜토리얼 1.1. GCP에서 사용자 인증 설정 Python으로 스프레드시트를 연동하기 위해서는, GCP(Google Cloud Platform)에서 사용자 인증과 API 사용 인증이 준비되어야 한다.\n\u0026amp;nbsp\n1.1.1. 구글 클라우드 플랫폼에 로그인  회사에서는 팀원들과 드라이브를 공유하기에 Gsuite 계정으로 로그인했다.     \u0026amp;nbsp\n1.1.2. 새로운 프로젝트 생성  구글 드라이브, 구글 스프레드시트와 연동할 새로운 프로젝트를 새로 생성한다.              \u0026amp;nbsp\n1.1.3. 구글 드라이브 API 사용 설정  새로 생성한 프로젝트를 선택하고, API 개요 이동하여 구글 드라이브 API 사용을 설정한다.                     \u0026amp;nbsp\n1.1.4. 구글 드라이브 API에 대한 인증정보 생성.  구글 드라이브 API를 사용함에 있어서 필요한 사용자 인증정보를 추가한다. 새 서비스 계정을 만들면, 서비스 계정 및 Key가 JSON 형태의 파일이 받아지게 된다.  특히 추후 파이썬 코드 내에서 Key가 필요하게 되니, 적절한 디렉토리에 저장하도록 한다.                 \u0026amp;nbsp\n1.1.5. 구글 스프레드시트 API 사용 설정  구글 드라이브 API 사용 설정한 방법과 동일하게, 구글 스프레드시트 API 사용설정을 활성화 한다.        \u0026amp;nbsp \u0026amp;nbsp\n1.2. 파이썬 패키지 설치  파이썬에서는 gspread와 oauth2client가 필요하다.  gspread: 파이썬을 통해 구글 스프레드시트와 연동하고, 제어할 수 있게 하는 패키지. oauth2client: OAuth2을 통해 사용자 인증을 하기 위해 설치함.   터미널 또는 CMD에서 pip를 통해 설치하도록 한다.  pip install gspread pip install --upgrade oauth2client \u0026amp;nbsp \u0026amp;nbsp\n1.3. 구글 스프레드시트 파일 설정  파이썬을 통해서 접근하고 연동하고자 하는 해당 구글 시트에서 사용자 인증 정보를 설정하다. 앞서 내려 받은 JSON파일에서 client_email의 값을 복사한 뒤, 해당 스프레드시트 우측 상단 공유를 클릭한 뒤 입력하여 권한을 부여한다..         복잡하지는 않지만, 스프레드시트마다 이 작업을 해주어야 한다는 점이 약간 번거로운 부분이다. 공유 권한부여까지 완료되었다면, 이제 파이썬으로 스프레드시트의 데이터를 읽어와보자!  \u0026amp;nbsp \u0026amp;nbsp\n1.4. 파이썬에서 테스트 1.4.1. 인증과 연동  사용자 인증 파일(JSON)을 통해 연동을 한다.  from oauth2client.service_account import ServiceAccountCredentials scope = [\u0026#34;https://spreadsheets.google.com/feeds\u0026#34;, \u0026#34;https://www.googleapis.com/auth/spreadsheets\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive.file\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive\u0026#34;] creds = ServiceAccountCredentials.from_json_keyfile_name(\u0026#34;{your_JSON_filename}.json\u0026#34;, scope) \u0026amp;nbsp\n1.4.2. 스프레드 시트 선택  테스트를 위해서 샘플 스프레드시트를 생성한 상태이다.  빈 시트인 시트1, 그리고 상장기업종목코드와 기업명이 담긴 sample_data 시트가 있다.        gspread 패키지를 통해 인증 후, 접근하고자 하는 시트의 이름을 패스한다.  import gspread spreadsheet_name = \u0026#34;{your target spreadsheet name}\u0026#34; client = gspread.authorize(creds) spreadsheet = client.open(spreadsheet_name) \u0026amp;nbsp\n1.4.3. 시트 불러오기 for sheet in spreadsheet.worksheets(): print(sheet) \u0026lt;Worksheet '시트1' id:1966713574\u0026gt; \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.4. 시트 선택하기  이름 또는 인덱스를 통해 시트를 선택할 수 있다.  ## by name sheet = spreadsheet.worksheet(\u0026#34;sample_data\u0026#34;) ## OR by index sheet = spreadsheet.get_worksheet(1) print(sheet) \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.5. 시트 내 데이터 읽어오기  get_all_values() 함수는 시트 내 데이터를 모두 출력한다.  sheet.get_all_records()[:3] [{'CompanyStockCode': 'S030190', 'PA_CompanyID': 1, 'IA_CompanyID': 'ID00001', 'DelistStatus': '', 'CompanyKorName': 'NICE평가정보'}, {'CompanyStockCode': 'S038620', 'PA_CompanyID': 2, 'IA_CompanyID': 'ID00002', 'DelistStatus': '', 'CompanyKorName': '위즈코프'}, {'CompanyStockCode': 'S039020', 'PA_CompanyID': 3, 'IA_CompanyID': 'ID00003', 'DelistStatus': '', 'CompanyKorName': '이건홀딩스'}] \u0026amp;nbsp\n1.4.6. Pandas DataFrame 형태로 변환  간단한 함수를 작성하여 Pandas의 DataFrame형태로 데이터를 불러올 수 있다.  def gsheet2df(sheet): df = pd.DataFrame(columns=list(sheet.get_all_records()[0].keys())) for item in sheet.get_all_records(): df.loc[len(df)] = item return df df_sample_data = gsheet2df(sheet) df_sample_data.head()    \u0026amp;nbsp \u0026amp;nbsp\n2. 마무리하며 리서쳐와 데이터를 주고 받으며 일하던 와중에, gspread의 적용은 매우 빛과 소금 같은 시원한, 그리고 매우 신속했던 해결책이었다. 내 업무영역에 적용한 뒤, 바로 다른 분석가 동료들에게 튜토리얼을 통해 공유하는 세션을 가지기도 했던 만큼 적용되는 범위가 컸다.\n누구든 일을 하다 잠재적인 기술 부채를 맞닥뜨리면, 미래의 나를 위해서 어떻게든 털어내버리고 싶을 것이다. 하지만, 함께 해결해나갈 인력이 없는 환경에서 비개발자인 동료들도 불편해하지 않게끔 적절한 방법을 찾고 도입하는 것은 생각보다 쉽지 않았다. 답답할 때가 없지는 않지만, 이번 케이스처럼 또 하나하나씩 도입해나가면 더 매끄러워지지 않을까라는 생각을 해보며 또 다른 기술 부채를 맞닥뜨릴 마음의 준비를 해본다.\n\u0026amp;nbsp\n 3. Reference   gspread 공식 도큐먼트\n  Manage Google Spreadsheets with Python and Gspread\n  ","permalink":"https://lucaseo.github.io/posts/2020-04-12-python-spreadsheet-gspread/","summary":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움.","title":"[KR] Python으로 구글 스프레드시트 연동하기 (ft. gspread)"},{"content":"   Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.   Raw Data  테이블 형태의 데이터셋을 확인할 수 있다.   Map: Confirmed  1월 22일부터 현재까지 전세계의 확진자 현황을 지도로 시각화한다.       (데이터셋은 Johns Hopkins CSSE의 Github에 제공된 COVID-19 데이터를 사용하였다.)\n\u0026amp;nbsp\n 사전 준비 2: Heroku Heroku는 웹사이트나 어플리케이션을 빌드하고 배포할 수 있는 PaaS이다. 파이썬, 자바, 루비, PHP, Node.js, Go 등 여러 언어를 지원하고 있다. 소규모 어플리케이션이라면 무료로도 배포할 수 있다.\nHeroku 가입 Heroku를 통해 배포를 하기 위해서는 우선 계정이 있어야 한다. 아래 링크를 통해 Heroku 계정을 생성하자.\n https://www.heroku.com/  Heroku CLI 설치 설치 계정을 생성하였다면, 현재 개발 환경에 Heroku Command Line Interface를 설치한다. 아래의 링크에는 각자의 OS에 맞추어 Heroku CLI의 설치 가이드가 제공되어 있다.\n https://devcenter.heroku.com/articles/getting-started-with-python#set-up  CLI를 통한 로그인 설치가 완성되었다면, 쉘에서 heroku 커맨드를 사용할 수 있다. 다음 명령어로 Heroku에 로그인 해보자.\nheroku login    아래와 같은 결과가 출력되며, 브라우저를 열어 로그인 하도록 한다. Heroku에 대한 준비작업은 이로서 완료되었다.\n   \u0026amp;nbsp\n 배포를 위한 파일 생성하기 Heroku에 배포를 하기 위해서는 몇 가지 준비사항이 충족되어야 한다.\n app.py가 위치한 가상환경에서 Git Repository를 생성한다.  git init 배포를 위해 필요한 파일들을 생성한다.  파일 #1. requirements.txt pip freeze \u0026gt; requirements.txt 파일 #2. .gitignore venv *.pyc .DS_Store .env 파일 #3. setup.sh setup.sh에서는 Streamlit에 대한 config.toml 파일을 생성한다.\nmkdir -p ~/.streamlit/ echo \u0026#34;\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT\\n\\ \u0026#34; \u0026gt; ~/.streamlit/config.toml 파일 #4. Procfile Procfile에서는 Heroku로 하여금 웹어플리케이션을 시작시키기 위해 명령어를 순서대로 실행하도록 한다.\nweb: sh setup.sh \u0026amp;\u0026amp; streamlit run app.py (Procfile에 대한 자세한 사항은 다음 링크에서 참조할 수 있다.)\n\u0026amp;nbsp\n Heroku에 배포하기 배포를 위한 파일이 모두 준비되었다면, 이제 본격적으로 Heroku에 배포를 할 수 있게 되었다.\nHeroku App 생성 heroku create {your app name} 앞서 Heroku CLI를 생성했을 때와 같은 Heroku 로그인 과정을 거쳐, Heroku App이 생성 되었다.\n   생성된 App은 Heroku 사이트의 대시보드에서도 다음과 같이 확인이 가능하다.\n   Heroku repository 생성 Heroku는 배포를 위해 Git을 사용하므로, 방식도 같다.\ngit add . git commit -m \u0026#39;Init app boilerplate\u0026#39; git push heroku master       Heroku 의 프로세스 인스턴스를 1개로 제한하는 것을 권장한다. 프로세스가 많을 수록 유료가 될 수 있기 때문에, 무료인 상태에서 계속 사용하고 싶다면, 아래의 명령어를 실행시켜준다.\nheroku ps:scale web=1    Heroku 배포 확인 드디어 배포가 완료되었다! 이제 배포된 웹 어플리케이션을 https://your_app.herokuapp.com 에서 확인할 수 있다. 본 튜토리얼을 위해 배포된 웹 어플리케이션은 https://tuto-covid19-map.herokuapp.com/ 에서 확인할 수 있다.\n   \u0026amp;nbsp\n 마무리 하며 간단한 과정을 거쳐 웹 어플리케이션을 배포까지 해보았다. AWS와 같은 클라우드에 배포하는 과정은 익숙하지 않았고, 현재 Streamlit이 자체적으로 특정 배포 방식을 권장하고 있지는 않기에, Dash(Plotly에서 개발한 웹어플리케이션 프레임워크)에서 웹어플리케이션을 Heroku에 배포하는 가이드를 적용해 보았다. 큰 어려움 없이 배포를 완료하였기에, 많은 분들도 이 과정을 참고하시어 Streamlit을 활용한 다양한 프로젝트를 진행할 수 있으리라 생각된다.\n\u0026amp;nbsp\n 코드  Github  \u0026amp;nbsp\n참고자료  Dash 공식 페이지 Johns Hopkins CSSE Github Repository Mapping the Spread of Coronavirus COVID-19 with python and Plotly by. Babak Fard  ","permalink":"https://lucaseo.github.io/posts/2020-03-29-deploy-streamlit-to-heroku/","summary":"Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.","title":"[KR] Streamlit 웹 어플리케이션 배포하기 (feat. Heroku)"},{"content":" Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;\n웹개발을 몰라도 된다.\n 웹개발에 대해 아는 것이 전혀 없는 나 같은 사람도 페이지를 띄울 수 있다. 주로 사내용으로 이용되기 때문에 UI/UX적인 측면에서 뛰어나지 않아도 되기 때문에 일정 수준의 미적인 요소들이 기본적으로 적용되있는 점이 매우 편리하다. 간결하고 명확한 API 덕분에 다른 웹프레임워크와 비교해서 상대적으로 진입장벽이 낮다. 일정한 수준의 결과를 내기 위해 투자하는 시간이 매우 절약된다.  전달력이 매우 좋다.\n 웹어플리케이션은 사용자에게도 진입장벽이 낮다. 특히 interactive한 성향 덕분에, 슬라이드 포맷의 레포트나 자료보다 전달력과 만족도가 높은 것을 볼 수 있었다.  \u0026amp;nbsp\n덕분에 Streamlit은 조직 내부적으로 탐색적 데이터 분석(EDA) 결과를 공유하거나, 간단한 ML 모델을 배포하고 테스트를 하는 용도에 부합하는 툴이라고 할 수 있다. Streamlit API에서 제공하는 기능들을 간단하게 훑어보자.\n Streamlit 간단 맛 보기 설치 pip를 통해 설치하기 $ pip install streamlit \u0026amp;nbsp\n실행하기 Streamlit은 8501 포트에 앱이 실행된다. 일단 지금 아무 것도 없는 상황에서는 우측 상단 버튼만 있는 페이지를 볼 수 있다.\n$ streamlit run {your app}.py You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://{your_network}:8501    Streamlit 불러오기 Streamlit은 st라는 alias로 불러온다.\n# import Streamlit Library import streamlit as st 소스에 변경이 생길 경우 경우 상단에 알림이 뜬다. Rerun을 해주도록 하자.\n   \u0026amp;nbsp\n텍스트 출력 Header와 Text  Title, Header \u0026amp; Subheader  Header와 Subheader를 다음과 같이 달 수 있다. 다만 Header의 경우 subheader 레벨까지만 가능하다. Title, Header, Subheader이 각각 Header, Subheader, Subsubheader인 것으로 인식하면 될 것 같다.    ## Title st.title(\u0026#39;Streamlit Tutorial\u0026#39;) ## Header/Subheader st.header(\u0026#39;This is header\u0026#39;) st.subheader(\u0026#39;This is subheader\u0026#39;) ## Text st.text(\u0026#34;Hello Streamlit! 이 글은 튜토리얼 입니다.\u0026#34;)    \u0026amp;nbsp\nMarkdown Streamlit도 Dash와 마찬가지로 Markdown을 지원한다.\n## Markdown syntax st.markdown(\u0026#34;# This is a Markdown title\u0026#34;) st.markdown(\u0026#34;## This is a Markdown header\u0026#34;) st.markdown(\u0026#34;### This is a Markdown subheader\u0026#34;) st.markdown(\u0026#34;- item 1\\n\u0026#34; \u0026#34; - item 1.1\\n\u0026#34; \u0026#34; - item 1.2\\n\u0026#34; \u0026#34;- item 2\\n\u0026#34; \u0026#34;- item 3\u0026#34;) st.markdown(\u0026#34;1. item 1\\n\u0026#34; \u0026#34; 1. item 1.1\\n\u0026#34; \u0026#34; 2. item 1.2\\n\u0026#34; \u0026#34;2. item 2\\n\u0026#34; \u0026#34;3. item 3\u0026#34;) 2020-03-13-intro-to-streamlit/4\n\u0026amp;nbsp\nLatex Latex의 경우 백슬래시(\\)를 빈번히 사용되기 때문에, 일반 string 대신 raw string을 붙여주는 편이 좋다.\n## Latex st.latex(r\u0026#34;Y = \\alpha + \\beta X_i\u0026#34;) ## Latex-inline st.markdown(r\u0026#34;회귀분석에서 잔차식은 다음과 같습니다 $e_i = y_i - \\hat{y}_i$\u0026#34;)    \u0026amp;nbsp\n메세지와 에러메세지, 예외처리 메세지 기본적으로 포맷된 메세지 박스 기능을 제공한다.\n## Error/message text st.success(\u0026#34;Successful\u0026#34;) st.info(\u0026#34;Information!\u0026#34;) st.warning(\u0026#34;This is a warning\u0026#34;) st.error(\u0026#34;This is an error!\u0026#34;) st.exception(\u0026#34;NameError(\u0026#39;Error name is not defined\u0026#39;)\u0026#34;)    \u0026amp;nbsp \u0026amp;nbsp\n데이터프레임과 테이블 출력. 데이터를 출력하는 방법에는 3가지 방법이 있다.\n  st.table:\n 단순히 입력 테이블 전체를 리턴한다.    st.dataframe:\n 적절히 10개의 행을 기준으로 스크롤을 통해 데이터를 관찰 할 수 있고 각 열마다 정렬도 가능하다. 각 테이블의 우측 상단의 확대 버튼을 통해 테이블을 더 크게 볼 수 있고,    st.write:\n st.dataframe과 똑같은 결과를 리턴한다.    ## Load data import pandas as pd from sklearn.datasets import load_iris iris = load_iris() iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df[\u0026#39;target\u0026#39;] = iris[\u0026#39;target\u0026#39;] iris_df[\u0026#39;target\u0026#39;] = iris_df[\u0026#39;target\u0026#39;].apply(lambda x: \u0026#39;setosa\u0026#39; if x == 0 else (\u0026#39;versicolor\u0026#39; if x == 1 else \u0026#39;virginica\u0026#39;)) ## Return table/dataframe # table st.table(iris_df.head()) # dataframe st.dataframe(iris_df) st.write(iris_df)    \u0026amp;nbsp \u0026amp;nbsp\n이미지, 오디오, 비디오 파일 출력. 이미지, 영상, 오디오 파일을 열어서 재생할 수 있다.\n st.image : 파이썬 이미지 라이브러리와 함께 쓸 수 있다. st.video :  파일의 포맷을 지정해야 하며, 디폴트로는 video/mp4가 설정되어 있다. start_time 변수를 통해 재생시작점을 조절할 수 있다.   st.audio :  파일 포맷은 audio/wav 가 디폴트로 설정되어 있다. 마찬가지로 start_time 변수를 통해 재생시작점을 조절할 수 있다.    ##Show image from PIL import Image img = Image.open(\u0026#34;files/example_cat.jpeg\u0026#34;) st.image(img, width=400, caption=\u0026#34;Image example: Cat\u0026#34;) ## Show videos vid_file = open(\u0026#34;files/example_vid_cat.mp4\u0026#34;, \u0026#34;rb\u0026#34;).read() st.video(vid_file, start_time=2) ## Play audio file. audio_file = open(\u0026#34;files/loop_w_bass.mp3\u0026#34;, \u0026#34;rb\u0026#34;).read() st.audio(audio_file, format=\u0026#39;audio/mp3\u0026#39;, start_time=10)    \u0026amp;nbsp \u0026amp;nbsp\n위젯 st.checkbox - 체크박스 ## Checkbox if st.checkbox(\u0026#34;Show/Hide\u0026#34;): st.write(\u0026#34;체크박스가 선택되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.radio - 라디오버튼 ## Radio button status = st.radio(\u0026#34;Select status.\u0026#34;, (\u0026#34;Active\u0026#34;, \u0026#34;Inactive\u0026#34;)) if status == \u0026#34;Active\u0026#34;: st.success(\u0026#34;활성화 되었습니다.\u0026#34;) else: st.warning(\u0026#34;비활성화 되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.selectbox - 드랍다운 선택 ## Select Box occupation = st.selectbox(\u0026#34;직군을 선택하세요.\u0026#34;, [\u0026#34;Backend Developer\u0026#34;, \u0026#34;Frontend Developer\u0026#34;, \u0026#34;ML Engineer\u0026#34;, \u0026#34;Data Engineer\u0026#34;, \u0026#34;Database Administrator\u0026#34;, \u0026#34;Data Scientist\u0026#34;, \u0026#34;Data Analyst\u0026#34;, \u0026#34;Security Engineer\u0026#34;]) st.write(\u0026#34;당신의 직군은 \u0026#34;, occupation, \u0026#34; 입니다.\u0026#34;)       \u0026amp;nbsp\nst.multiselect - 드랍다운 다중 선택 ## MultiSelect location = st.multiselect(\u0026#34;선호하는 유투브 채널을 선택하세요.\u0026#34;, (\u0026#34;운동\u0026#34;, \u0026#34;IT기기\u0026#34;, \u0026#34;브이로그\u0026#34;, \u0026#34;먹방\u0026#34;, \u0026#34;반려동물\u0026#34;, \u0026#34;맛집 리뷰\u0026#34;)) st.write(len(location), \u0026#34;가지를 선택했습니다.\u0026#34;)       \u0026amp;nbsp\nst.slider - 슬라이더 ## Slider level = st.slider(\u0026#34;레벨을 선택하세요.\u0026#34;, 1, 5)    \u0026amp;nbsp\nst.button - 버튼 ## Buttons if st.button(\u0026#34;About\u0026#34;): st.text(\u0026#34;Streamlit을 이용한 튜토리얼입니다.\u0026#34;)    \u0026amp;nbsp\n텍스트 입력 # Text Input first_name = st.text_input(\u0026#34;Enter Your First Name\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;first_name\u0026#39;): result = first_name.title() st.success(result) # Text Area message = st.text_area(\u0026#34;메세지를 입력하세요.\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;message\u0026#39;): result = message.title() st.success(result)    \u0026amp;nbsp\n날짜와 시간 입력 ## Date Input import datetime today = st.date_input(\u0026#34;날짜를 선택하세요.\u0026#34;, datetime.datetime.now()) the_time = st.time_input(\u0026#34;시간을 입력하세요.\u0026#34;, datetime.time())       \u0026amp;nbsp\n코드와 JSON 출력  with st.echo(): 이하의 코드는 코드블럭으로 출력된다.  ## Display Raw Code - one line st.subheader(\u0026#34;Display one-line code\u0026#34;) st.code(\u0026#34;import numpy as np\u0026#34;) # Display Raw Code - snippet st.subheader(\u0026#34;Display code snippet\u0026#34;) with st.echo(): # 여기서부터 아래의 코드를 출력합니다. import pandas as pd df = pd.DataFrame() ## Display JSON st.subheader(\u0026#34;Display JSON\u0026#34;) st.json({\u0026#39;name\u0026#39; : \u0026#39;민수\u0026#39;, \u0026#39;gender\u0026#39;:\u0026#39;male\u0026#39;, \u0026#39;Age\u0026#39;: 29})    \u0026amp;nbsp\n사이드바 st.sidebar에서도 대부분의 위젯을 지원하므로, 다양하게 사이드바를 구성할 수 있다. (단, st.echo, st.spinner, st.write제외)\n## Sidebars st.sidebar.header(\u0026#34;사이드바 메뉴\u0026#34;) st.sidebar.selectbox(\u0026#34;메뉴를 선택하세요.\u0026#34;, [\u0026#34;데이터\u0026#34;, \u0026#34;EDA\u0026#34;, \u0026#34;코드\u0026#34;])    \u0026amp;nbsp\n차트 그리기 Streamlit은 자체 내장된 기본적인 차트 외 matplotlib, plot.ly, altair, vega_ilte, bokeh, deck_gl, pydeck, graph_viz 등 다양한 시각화 패키지를 지원한다.\n(Streamlit은 EDA 용도로 많이 사용되는 만큼, 시각화 부분은 따로 다룰 계획이다.)\n## Plotting st.subheader(\u0026#34;Matplotlib으로 차트 그리기\u0026#34;) iris_df[iris_df[\u0026#39;target\u0026#39;]==\u0026#39;virginica\u0026#39;][\u0026#39;petal length (cm)\u0026#39;].hist() st.pyplot()    \u0026amp;nbsp \u0026amp;nbsp\n 마무리 Streamlit의 API를 훑어보면서, 전체적으로 많은 부분이 간결하고, 쉽다고 느껴졌다. Flask나 Django로 개발하는 개발자 입장에서 Streamlit 같은 프레임워크는 자유도가 제한된다고 느껴질 수도 있겠다. 하지만 등장한지 얼마 안 되는 만큼 커뮤니티 포럼에서는 활발한 토론과 기능 추가에 대한 요청이 이어지고 있는 중이다. Streamlit 개발자들이 적극적으로 피드백을 반영하는 모습을 보이고 있으므로, 앞으로의 발전이 더 기대된다.\n이번 소개 글에 이어, 개인적으로 Streamlit으로 개발하면서 얻은 팁이나, 클라우드 또는 Heroku에 배포하는 과정, 데이터사이언스 프로젝트를 위해 웹어플리케이션을 구성하는 팁 등을 시리즈로 작성할 예정이다. 나처럼 웹개발은 모르지만 데이터분석 결과를 그럴 듯하게 구성하고 싶은 분들께 도움이 되었으면 한다.\n\u0026amp;nbsp\n코드  Github  \u0026amp;nbsp\n참고자료  Streamlit 공식 페이지  ","permalink":"https://lucaseo.github.io/posts/2020-03-13-intro-to-streamlit/","summary":"Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;","title":"[KR] 파이썬 웹어플리케이션 맛보기 (feat. Streamlit)"},{"content":" 신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다. 회사에 시니어가 없어서 그런지 이 루트로 도움을 많이 받고 있음을 체감한다. 자연스럽게 ‘나도 할 수 있을까?’ 라는 생각이 들었다. 데이터를 다루는 일을 하고 있지만, 내가 하는 일을, 내가 아는 것들을 글로서 풀어낼 수 있을지 스스로 물어보면 돌아오는 답은 ‘자신이 없다’ 였다. 솔직히 아직은 자신이 많이 없다.\n   \u0026#34;핑계는 수백만개라도 댈 수 있죠\u0026#34; Source: The Office   \u0026amp;nbsp\n이런 와중에 글또라는 모임은 1년 전부터 여러 커뮤니티와 변성윤님을 통해 몇 차례 접하고 있었다. 글 쓰는 또라이가 세상을 바꾼다라니… 너무 멋있다고 생각했다 (역시 대단한 사람들!). 마침 글또 4기의 모집 글을 접하게 되었고, 글쓰는 것에 대해 여러 생각을 하는 중 타이밍이 맞았다. 다른 때 같았으면 모집글만 보고 “내가 저런 거 할 시간이 어딨어” 등등 갖가지 이유를 대며 안 할 수도 있긴 했다. 그래서 일단 질렀다. 글또 지원은 2020년에 강림하신 첫 지름신이었다.\n글또를 통해 이루고자 하는 것 글쓰는 꾸준한 습관 몇 년 전부터 글을 써보고자 다양한 플랫폼에서 시도했었다. 페이스북, 워드프레스 블로그, 인스타그램, 힙합엘이…. 여러 가지 이유가 있었겠지만, 가장 주요한 이유를 냉정하게 꼽아보자면 끈기가 부족했다. 글 쓰는 체력이 부족하고, 쓰다가 아니다 싶으면 접어버리는 안 좋은 습관도 있다는 것을 인지하게 됐다.\n반면에 글또는 다 함께 정한 규칙과 약간의 강제성이 있고, 글을 읽고 피드백을 해주는 분들이 있다. 나 또한 다른 분들의 글을 읽고 피드백을 해야 한다. 지금 나에게 필요한 것은 약간의 강제성을 부여받고 다른 멋진 분들을 통해 자극을 받는 것이니까, fit이 딱 맞는다. 예치금을 삭감당하지 않겠다. 기필코.\n\u0026amp;nbsp\nClear할 때까지 글또를 통해서 얻고자 또 다른 목표는, 글쓰기를 통해 이해도와 설명력을 끌어올리는 것이다. 설명력은 높은 이해를 기반으로 이루어지는 것으로 생각하는데, 여태까지 회사에서 일하고 개인적으로 공부를 하면서 늘 어중간하게 알고 있는 것 같은 찝찝함을 버리지 못했다. 반면에 공부를 잘하는 친구들은 (1) 교과서만 봤어요, (2)질문을 많이 했어요, (3)다른 친구들에게 (혹은 부모님을 앉혀놓고) 설명을 해주면서 스스로 공부가 많이 됐어요 라고 들 많이 하더라. 생각해보면 난 학창 시절에 저런 걸 하나도 안 했다. 하지만 글또를 통해서 나 자신이 더 공부하고 성장을 할 수 있을 것이라 기대한다. 무언가에 대해 글을 쓰고 남에게 보이려면, 어쭙잖게 쓰면 안 된다는 것은 본능적으로 알고 있으니까.\n\u0026amp;nbsp\n글또 4기에서 쓰고자 하는 글 글또에 참여하시는 다른 분들을 보고, 나는 어떤 글을 써볼지 고민해봤다. 무엇보다 내가 배우고 느낀 점을 기록하는 것이 현시점에서는 우선이라 신속한 정보 전달의 목적은 2순위로 두기로 했다. 일단 떠오르는 주제가 여러 가지 있지만 좀 더 구체적인 정리가 더 필요하다.\n\u0026amp;nbsp\n스스로 돌아보는 반기 별 회고 글\n 회고를 하지 않다 보니 작년 한 해 동안 대체 무엇을 한 건지…. 남는 게 많이 없었고 개인적으로 허무하다는 느낌을 많이 받았다. 이번에는 회고 글을 정리하면서 작년과 무엇이 다른지 한번 느껴보고 싶다.  \u0026amp;nbsp\n회사에서 삽질한 경험\n 시니어가 없고, 늘 개인적으로 고군분투하다 보니까 이러한 경험도 글로 남기면 좋겠다고 생각해본다. 나만 그런 것은 아닐 테니까. 그리고 증거로 남겨야겠다.  \u0026amp;nbsp\n데이터 사이언스 대회 참가 후기\n 한 번도 on-going 데이터 사이언스 대회에 참가해 본 적이 없는데, 이 또한 갖가지 핑계를 대며 하지 않았던 거라고 자평해본다. 데이터 사이언스 대회 참가를 통해 배운 점, 삽질한 점들을 돌아보고 정리해보고자 한다.  \u0026amp;nbsp\n개발 관련 책 리뷰\n 책은 자주 많이 사는데, 늘 훑어보기만 했던 게 좀 찝찝했다. 책에 대한 리뷰글을 틈틈이 쓰고자 한다. 출판사 분들 지켜봐 주세요  \u0026amp;nbsp\n글을 마치며 작년 말 느꼈던 성장에 대한 고민, 답답함을 기반으로 올해 1, 2월 사이 내린 결정들이 있는데, 글또 참여도 그중 하나였다. 벌린 일이 많아 걱정은 되지만, 앞으로 글또에서 만나게 될 많은 멋진 분들이 계시니까 아주 큰 걱정은 아니라고 생각된다. 개복치처럼 터지지 말고, 꾸준히 존버하자.\n","permalink":"https://lucaseo.github.io/posts/2020-02-23-init-geultto-4th/","summary":"신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다.","title":"[KR] 글또 4기 다짐글"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.) 원을 두 바퀴, 세 바퀴 돌면 720도, 1080도가 되는데 이런 식의 계산은 편리하지가 않다. 두바퀴는 2 * 2pi randians으로 계산하는 것이 더 편하다고 하는데 일단 지켜보자.\n아래의 Figure 1.에서처럼 45도의 각도는 45/360 * 2pi = 0.785 radians 로 표현할 수 있다.\n   \u0026amp;nbsp\n삼각함수 복습 삼각함수를 좀 복습해야겠다. 위의 그림의 삼각형을 보고 다음과 같이 부른다면\n Hypotenuse 빗변 (파랑, 원의 반지름) : r Adjacent 밑변 (빨강, x축 길이) : x Opposite 높이 (초록, y축 길이) : y  피타고라스의 정리를 통해 다음과 같이 말할 수 있다.\n$$x^2 + y^2 = r^2$$\n그렇다면, sine, cosine도 다음과 같이 구할 수 있다.\n$$\\sin = \\dfrac{y}{r}$$\n$$\\cos = \\dfrac{x}{r}$$\n단위원(unit circle)를 기준으로 본다면 r=1 이므로 다음과 같다.\n$$\\sin = \\dfrac{y}{1}$$\n$$\\cos = \\dfrac{x}{1}$$\n","permalink":"https://lucaseo.github.io/posts/2020-01-25-dsp-basic-s01-7/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Trigonomatry"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.\n      \u0026amp;nbsp\n선이 회전하는 속도는 곧 frequency로 나타낼 수 있다. 앞서 frequency는 1초에 몇 사이클을 회전할 수 있는지를 통해 측정한다고 했는데, 원문의 Figure 1.을 통해서 확인할 수 있듯이 frequency가 높을 수록 회전하는 선이 속력이 빨라지고, 주기도 빨라지게 된다. 반대로 frequency가 낮으면 주기가 느려진다.\n      \u0026amp;nbsp\nThe Cosine Wave : The Counterpart to Sine 반대로 cosine wave(코사인파)는 원을 그리며 회전하는 선과 X축에서 수평의 길이와 관계 있다. 사실 sine wave와 cosine wave는 90도로 회전하면 서로 같은 모양을 가지고 있다.\n   \u0026amp;nbsp\nSine wave와 cosine wave는 밀접한 관계를 맺고 있으며, 둘 다 주기성을 띄는 주기함수(periodic signal)임을 이해하는 것이 매우 중요하다.\n","permalink":"https://lucaseo.github.io/posts/2020-01-23-dsp-basic-s01-6/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sine Wave"},{"content":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,\ndf = pd.DataFrame({ \u0026#39;A\u0026#39;: [1,2,3], \u0026#39;B\u0026#39;: [\u0026#34;goodbye\u0026#34;, \u0026#34;cruel\u0026#34;, \u0026#34;world\u0026#34;], \u0026#39;C\u0026#39;: [False, True, False] }) df.info() 결과물 출력 비교\npandas 0.x.x\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): A 3 non-null int64 B 3 non-null object C 3 non-null bool dtypes: bool(1), int64(1), object(1) memory usage: 179.0+ bytes pandas 1.0.0\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 A 3 non-null int64 1 B 3 non-null object 2 C 3 non-null object dtypes: int64(1), object(2) memory usage: 200.0+ bytes \u0026amp;nbsp\n.to_markdown() : DataFrame을 Markdown 형식으로 DataFrame을 바로 Markdown 형식으로 출력할 수 있게 되었다. 문서화 작업을 할 때 항상 markdown table generator 같은 도구를 썼었는데, 매우 반갑고 편리한 기능!\ndf = pd.DataFrame({\u0026#34;A\u0026#34;: [1, 2, 3], \u0026#34;B\u0026#34;: [1, 2, 3]}, index=[\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) print(df.to_markdown()) | | A | B | |:---|----:|----:| | a | 1 | 1 | | a | 2 | 2 | | b | 3 | 3 | \u0026amp;nbsp\ningore_index : index reset 파라미터 추가 기존에는 정렬이나 중복값 제거 후 .reset_index(drop=True)를 추가적으로 해줘야 했으나, ignore_index 파라미터를 통해 index를 리셋할 수 있게 되었다. default는 False. 다음 기능들에서 찾아볼 수 있다.\n .sort_values() .sort_index() .drop_duplicates()  \u0026amp;nbsp\npd.NA : 새로운 missing value의 실험 기존에 Pandas에서 missing value를 처리할 때는 np.nan이나 None이라는 싱글턴이 사용되었다. 그러나 data type이 float일 때는 np.nan , object일 경우에는 np.nan이나 None, datetime일 경우에는 np.NaT가 사용된다. 데이터타입마다 Null 데이터의 표현이 각기 달랐기 때문에, pd.NA는 datatype이 각기 달라도 missing data를 통일되게 표현할 수 있기 위해 도입되었다. 현재는 pd.NA는 data type 중 integer, boolean, 그리고 새로 도입된 string에서 사용 가능하다. pd.NA 값은 \u0026lt;NA\u0026gt;로 리턴된다. 실험적으로 도입했다고 하니, 지켜보면 좋을 듯.\n\u0026amp;nbsp\nstring : 새로운 data type 도입 기존에는 object 이라는 data type으로 뭉뚱그려진 느낌이 있었으나, string 이라고 따로 지정할 수 있게 됨으로서 EDA나 wrangling 측면에서 더욱 편해질 것 같다. (pd.NA도 확인 가능함)\npd.Series([\u0026#39;abc\u0026#39;, None, \u0026#39;def\u0026#39;], dtype=pd.StringDtype()) 0 abc 1 \u0026lt;NA\u0026gt; 2 def Length: 3, dtype: string \u0026amp;nbsp\nbool : missing value 표현 가능 기존에 boolean은 True / False 만 표기가 가능했으나, Pandas 1.0.0 에서는 missing value도 가능하다. (pd.NA도 가능함)\npd.Series([True, False, None], dtype=pd.BooleanDtype()) 0 True 1 False 2 \u0026lt;NA\u0026gt; Length: 3, dtype: boolean \u0026amp;nbsp\n Reference https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html\n","permalink":"https://lucaseo.github.io/posts/2020-01-16-pandas-new-release/","summary":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,","title":"[KR] Pandas 1.0.0 : 바뀐 점을 ARABOJA"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다. 음색이라는 개념은 음의 높이나 강도와는 별개의 특성이다. 플룻과 바이올린으로 똑같은 음을 연주하더라도, 소리는 서로 확연이 다른 것과 연관이 있다. 음색은 주로 배음(overtone)과 고조파(harmonic)의 유무에 따라 결정 된다.\n대부분의 음악은 fundamental frequency와 다수의 fundamental frequency에 위치한 harmonics로 이루어져 있다. 예를 들어 A4의 fundamental frequency는 440Hz이고, harmonics는 880Hz, 1320Hz, 1760Hz \u0026hellip; 의 주기로 이루어져 있다. 악기들은 대개 연주가 될 때, fundamental frequency와 harmonics에 위치한 소리를 생성한다.\n신호처리에서 sound wave를 논할 때는 주로 3가지 기본 sound wave를 지칭한다.\n sine wave: overtone이 없는 순음(pure tone) squire wave: fundamental frequency와 fundamental frequency의 홀수 배음 harmonics saw wave: fundamental frequency와 fundamental frequency의 전체 harmonics  신호를 관찰해보면 fundamental frequency가 가장 소리가 큰 부분을 차지하고 있고, harmonics는 frequency가 증가할 수록 감소한다.\n원본의 실습에서는 각기 다른 sound wave의 소리를 직접 들어보고 이에 따른 스펙트럼(spectrum)을 확인할 수 있다. 스펙트럼이란 특정 신호 안에 담긴 frequency들을 시각화된 형태이다.\n   \u0026amp;nbsp\n실제로 실행을 하고 스펙트럼을 확인해보면, 앞에서 설명한 바와 같이 Sine Wave는 단 하나의 440Hz frequency만 있는 순음이다. 반대로 Noise Wave는 어떠한 형태의 frequency도 구별을 할 수가 없다.\nSquare Wave 와 Saw Wave를 보면 Sine Wave를 여러번 반복한 형태와 비슷하게 보인다. 또한 spectrum에서도 여러 frequency 가 공존하는 것을 볼 수 있다.\n뒤 따르는 내용에서는 어떻게 하면 여러 개의 sine wave가 모여 complex wave(복합파형)를 이루는 지를 알아보자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-15-dsp-basic-s01-5/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Timbre"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다. 이런 밀고 당김의 연쇄작용이 진동을 발생하고 공기 중으로 전파가 나아갈 수 있게 한다.\n원문의 Figure 1.을 직접 참고해보기를 바란다.\n   \u0026amp;nbsp\n왼쪽 상단의 회색 직사각형이 파동의 진원지인 물체이고 무수히 많은 점들이 공기 중의 입자라고 보자. 원문의 이미지에서 이 입자들의 움직임을 잘 관찰해보면 물체의 움직임에 따라 특정 공간 내에서만 좌우로 움직이고 입자 자체가 자리를 이동하지는 않는 것을 볼 수 있다. 움직이는 물체로 인해 입자들도 비슷한 움직을을 보이게 되는데, 이것을 공명한다고 한다.\n입자 예시 아래 보이는 곡선은 기압의 정도를 나타낸다. 곡선이 수평축의 위를 지나갈 때는 공기의 입자들이 서로 컴팩트하게 모여있어 기압이 높은 상태(compression: 압축, 고밀도)이고, 반대인 경우 기압이 낮은 상태(rarefaction: 저밀도, 희박한 상태)이다.\n진동의 속도를 frequency(주파수)라고 한다. Frequency는 1초에 몇 사이클을 지났는지를 측정한 것이고, Hertz(Hz : 헤르쯔)라는 단위를 쓴다. Compression과 rarefaction의 반복 사이클을 1초에 몇번 왔다갔다 했는지를 측정하면 frequency를 구할 수 있다. 아래 Figure 1a.를 보면, compression과 rarefaction을 지나간 사이클 1번 이라고 볼 수 있겠다.\n   \u0026amp;nbsp\nFigure 1.을 통해 이것저것 시도해 보다 보면, frequency가 증가할 수록 wavelength(파장)이 줄어드는 것을 볼 수 있다. 파형(wave)의 길이(length)는 각 파형의 꼭지점이나 골짜기 사이의 거리를 측정하면 되는데, 이 거리는 frequency와 반비례 한다. 또한, frequency가 아무리 변해도 파형이 이동하는 속도는 일정하다. 해수면을 기준으로 소리는 공기에서 340m/s, 물과 같은 물질에서는 1500m/s 정도로 이동한다.\n직접 들어보자 Figure 1.처럼 부드럽게 진동하는 파형의 경우, 우리의 청각은 이 파형을 순음(pure tone)으로 받아들인다. Frequency가 낮은 음파는 낮은 음역(bass), 높은 frequency는 높은 음역(treble)이라고 하는데, 청각이 뛰어난 사람의 경우에 20 ~20,000Hz의 음역대를 들을 수 있다. 나이가 들어가면서 청력이 저하되면 들을 수 있는 음역대도 당연히 줄어들게 된다. 원문의 Figure 2.를 통해 여러가지 frequency를 한번 시도해보자.\n(실행이 되지 않는다면 아래의 링크에서 따로 실험해보자. 20,000 이상의 소리를 들어보자. 나는 들리지 않는다.)\nOnline Tone Generator - generate pure tones of any frequency\n   \u0026amp;nbsp\nX축을 보면, frequency의 눈금이 linear하지 않고 logarithmic한 것을 알 수 있다.\n440Hz는 우리가 흔히 말하는 음 \u0026ldquo;라\u0026quot;인데 A4라고 표현한다. 880Hz도 마찬가지로 \u0026ldquo;라\u0026quot;인데 한 옥타브가 높은 A5라고 한다. 여기에서 자세한 설명은 없지만, 왜 간격이 linear하지 않고 logarithmic한지 직관적으로 느낌은 알 수 있을 듯 하다\n우리의 귀는 대부분 20~8,000Hz 사이의 음역대에 반응하고, 사람의 목소리는 300~3,000Hz 사이에 속한다. 88건반 피아노는 22~4,000Hz의 기본 주파수의 소리를 내는데, 그 이상의 소리를 내는 경우도 있으며, 이를 overtone이라 한다.\n","permalink":"https://lucaseo.github.io/posts/2020-01-13-dsp-basic-s01-4/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sound Waves"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다. 그 외의 대답은 모두 거짓말이다.\n현재 우리가 가지고 있는 이산신호의 맥락에서 봤을 때, 65분 당시의 고도에 대한 기록은 어디에도 없으므로, 우리는 확신을 가지고 이야기 할 수가 없다.\naltitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] 위와 같이 10분 주기로 기록된 현재의 이산신호는 빨강, 파랑, 주황, 초록으로 기록된 아래의 다양한 비행고도의 변화를 모두 표현(represent)할 수 있다. 연속신호로 봤을 때는 고도의 변화가 제각기 다르지만, 샘플링 주기만 봤을 때는 모두 같은 신호로 볼 수 있다. 따라서 60분과 70분의 점을 이어서 31,000 이라고 답을 하는 것은 엄청난 착각을 불러일으킬 수 있다.\n   각기 다른 네 가지 연속신호를 나타내는 빨강, 파랑, 주황, 초록 곡선      네 가지 연속신호를 10분 샘플링 주기의 이산신호로 표현했을 때   \u0026amp;nbsp\nAlias 위의 빨강, 파랑, 주황, 초록 곡선은 10분 샘플링 주기의 이산신호로 표현 될 때 서로 구별이 불가능하며, 이를 각기 서로의 alias라고 부른다. 위의 네가지 곡선은 서로 다르지만, 10분 주기로 샘플링 되었을 때, 정확히 똑같아 보이기 때문이다.\n본 자료에서 두 점을 선으로 잇지 말라고 강조하고 있으나, 이해하기 쉽도록 시각화를 하기 위해서 앞으로도 지속적으로 점과 점 사이를 선으로 이어서 예를 들 것임. 글쓴이가 선을 잇는다고 해서 선을 잇는 행동은 하지 말 것 추천함.\nRemoving Uncertainty : Frequency and Context 샘플과 샘플 사이에 간격이 있다면, 불확실성(uncertainty)가 동반된다. 샘플 간격 사이에 벌어지는 급격한(rapid fluctuation) 변화에 대한 정보가 손실 될 수 있기 때문에, 간격이 크면 클 수록 우리는 샘플링이 된 현재의 이산신호가 실제 현상을 그대로 표현한다고 확신할 수가 없게 된다. 이러한 불확실성을 줄이는 방법은 반대로 간격을 줄이는 것 즉, 샘플링 주기를 줄이는 것이다.\n비행고도 예시에서, 샘플링 주기를 10분이 아닌 5분 단위로 설정한다고 해보자. 이렇게 되면 샘플링 주기가 적용된 파랑색 곡선의 이산신호를 봤을 때, 빨강, 주황, 초록 곡선은 더 이상 파랑 곡선의 alias라고 할 수 없다. (이제는 서로 구별할 수 있다!)\n   \u0026amp;nbsp\n샘플링 주기를 무작정 줄일 수도 있겠지만, 샘플링에는 감수해야할 부분도 있다. 샘플들이 저장되는 메모리가 확보되어야 한다는 것. 따라서 샘플링 주기를 줄일 때에는 꼭 주의를 기울어야 한다.\n샘플링 주기를 제대로 설정하는 방법은 말은 쉽지만 정보의 손실이 없을 정도로 설정하는 것이다. 만약 필요한 만큼보다 더 자잘하게 샘플링 주기를 설정한다면 오버샘플링(oversampling)이 되어 메모리나 연산 리소스를 낭비하게 되고, 너무 뜸하게 설정하면 언더샘플링(undersampling)으로 정보 손실이 발생할 수 있게 된다.\n샘플링에 대한 이론은 frequency(주파수) 개념을 이해하면 좀 더 쉬워질 수 있을 것 같다. 다음에는 주파수에 대한 개념을 접할 수 있는 sound wave(음파)를 한번 알아보자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-12-dsp-basic-s01-3/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sampling \u0026 Aliasing"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다. (원문 Figure 1. 참조)\n   altitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] Figure 1.의 파랑색 점 하나하나가 샘플이며, 그래프말고도 다음과 같이 이산값의 리스트로도 표현하고, 인덱싱(indexing)을 할 수도 있다.\n altitude[4] → 35,000 altitude[8] → 27,000  Sampling Period (샘플링 주기) 샘플링 주기(sampling period)는 연속적인 신호 사이의 지속기간(duration)을 뜻한다. 위의 비행 고도 샘플링 예시에서 각 고도를 기록하는 주기를 10분으로 놨었는데, 이때 샘플링 주기가 10분이고, 아래와 같이 나타낼 수 있다.\n$$\\text{sampling period} = 10\\text{ minutes} / 1\\text{ sample}$$\n샘플링 주기를 알 수 있다면, index에 샘플링 주기를 곱함으로서 몇 번 째 샘플이 언제 기록 되었는지를 추적(?)할 수 있다.\n$$\\text{time of 3rd sample} = 2 \\cdot 10 \\text{ minutes} / 1 \\text{ sample} = 20 \\text{ minutes}$$\n이산신호를 해석하기 위해서는 context를 파악하는 것이 매우 중요하다. 샘플링 주기를 알아야지만, 이산신호의 값들이 의미를 가지고 make sense할 수 있는 것이고, sampling period를 알지 못 한다면, 이러한 값들이 의미를 잃게 된다.\n추가적으로 \u0026hellip;    원본의 Figure 1.에서 비행기의 실제 고도를 주의깊게 봤다면, 60-70분 사이 급격한 하강 후 고도를 회복하는 부분이 있었음을 관찰할 수 있는데, 우리의 샘플링 주기는 10분이었기 때문에 정작 샘플링 당시에는 기록되지 못 했다. 샘플링 주기가 적절하지 않았기 때문에 중요한 정보가 손실 된 것이다.\n따라서 특정 물리적인 현상을 관측하기 위한 이산신호를 기록하려면, 이산신호가 그 현상을 제대로 나타낼 수 있도록 샘플링 주기를 적절하게 선택해야 한다. 이렇듯 샘플링 주기의 결정은 신호처리 분야에서도 매우 중요하게 다루는 부분 중 하나이다.\n다음 번에는 신호주기를 매우 뜸하게 설정 했을 때 나타날 수 있는 결과에 대해 좀 더 알아보도록 하자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-11-dsp-basic-s01-2/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Discrete Signals"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다. 컴퓨터는 인간과 비교해서 겁나 빠르다는 장점이 있지만 또 반대로 컴퓨터는 겁나 단순해서 오직 이산값(discrete values)만 읽고 처리가 가능하다.\n하지만 현실은 그렇지 않다. 실생활에서 발생하는 대부분 연속적(continuous) 신호이다. 따라서 컴퓨터에서 분석하기 이전에 연속 신호를 이산적인, 디지털의, 딱딱 떨어지는 값으로 변환(translate)하는 과정을 거쳐야 한다.\n원문의 Figure2를 보면 이산 신호를 통해서 연속 신호를 완벽하게 재현하는 것은 불가능 해보일 수 있다. 그리고 실제로 이는 근사화(approximation)에 그친다고 주장하는 사람도 있다.\n      하지만 DSP를 공부한다면, 이산 지점를 이용해서 연속 신호를 완벽하게 표현하는 것이 불가능하지 않다고 하니, 열심히 공부해보자!\n","permalink":"https://lucaseo.github.io/posts/2020-01-10-dsp-basic-s01-1/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 Seeing Circles, Sines And Signals 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 신호란?"},{"content":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.name: my-local-es node.name: local ### For Response by External Request network.host: 0.0.0.0 ### For Head http.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/elasticsearch-6.6.1 $ nohup bin/elasticsearch \u0026amp; \u0026amp;nbsp\n정상적으로 실행 중인지 확인 $ ps ax | grep elasticsearch $ curl localhost:9200 http://localhost:9200 실행\n\u0026amp;nbsp\n 02. 키바나 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-linux-x86_64.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-darwin-x86_64.tar.gz \u0026amp;nbsp\n압축 풀기 $ tar -xzvf kibana-6.6.1-linux-x86_64.tar.gz $ rm kibana-6.4.0-linux-x86_64.tar.gz \u0026amp;nbsp\n환경 설정 $ cd elasticstack/kibana-6.6.1-linux-x86_64/config $ vi kibana.yml server.host : 0.0.0.0 elasticsearch.url : \u0026#34;http://localhost:9200\u0026#34; kibana.index : \u0026#34;.kibana\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/kibana-6.6.1-linux-x86_64 $ bin/kibana \u0026amp;nbsp\n실행확인 http://localhost:5601 실행\n","permalink":"https://lucaseo.github.io/posts/2020-01-09-elasticsearch-kibana-local-env/","summary":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.","title":"[KR] 로컬 환경에 엘라스틱서치, 키바나 설치하기"},{"content":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인. 특히 로그를 운반하는 역할.   Beats 데이터 수집, 운송 Logstash와 비슷하나, 변환 기능이 제외되어 있음. 보다 가볍게 사용할 수 있음.    이외에 Elastic Cloud와 X-pack이 추가로 있으며, 기업을 대상으로 한 Enterprise 솔루션도 확대되고 있는 추세다.\n X-pack의 경우 유료이며, 보안을 강화하여 유저에게 권한까지 부여 가능하다. X-pack의 머신러닝 기능은 현재로서는 데이터의 이상징후를 탐지하는 수준이다.  Elastic, 어떻게 좋은가?   Near Realtime\n 데이터 색인 후 약 1초 후 Refresh 시점부터 거의 실시간으로 검색결과에 반영됨    Fast\n 기본적으로 모든 Field에 대해 Indexing(색인) 처리를 하기 때문에 검색 처리 시간이 짧다    Horizontal Scalability\n Elastic Cluster에 Elasticsearch Node를 1개씩 추가하며 수평적으로 확장하기에 용이하다    Distributed Operation\n 데이터를 조각(shard)로 세분화 하여 분산 저장하기 때문에 처리 속도가 향상된다.    Replica Shard\n 데이터 조각을 복제하여 다른 Node에도 저장하기 때문에, 특정 Node가 다운되거나 손실이 생겨도 데이터 유실 없이 운영할 수 있다.    Elastic Stack에서의 용어 비교    RDBMS Excel Elastic Elastic에서의 개념     Database Excel File Index 최상위 데이터 계층. Document의 덩어리   Table Sheet Type Document를 담고 있는 컨테이너 (*)   Row Row Document 데이터 검색을 위한 최소의 단위   Column Column Field JSON으로 이루어진 데이터의 property   Schema 없음 Mapping Index Document의 저장 규칙을 의미     RDBMS, Excel과는 달리 엘라스틱에서는 1 Index에 1개의 Type만 할당되어 사실상 의미가 사라진 상태이며, 7.0버전으로 업그레이드 시 Type이란 개념은 폐지 될 예정이다.  Elastic의 Work Flow      Elasticsearch\n Mapping 설정    Logstash\n 데이터 전처리 \u0026amp; 전송    Elasticsearch\n 데이터 저장    Kibana\n Index 등록 EDA 차트 선택 Aggregation 선택 데이터 시각화 대시보드 생성    ","permalink":"https://lucaseo.github.io/posts/2020-01-05-elastic-stack/","summary":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인.","title":"[KR] 엘라스틱스택(Elastic Stack) 소개"},{"content":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.\n","permalink":"https://lucaseo.github.io/posts/2020-01-02-first-post/","summary":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.","title":"[KR] 첫 포스팅"}]