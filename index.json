[{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.1.3. 알고리즘 시간 복잡도의 주요 요소  반복문이 얼마나 시행되었는지에 따라 시간 복잡도의 성능이 결정된다고 할 수 있음. 입력의 크기가 커지면 커질 수록 반복문이 알고리즘 수행 시간을 지배함.  1.2. 복잡도 표기법 유형 1.2.1. Big-O 표기법 \\(O(N) \\)  알고리즘 최악의 실행시간을 표기함 아무리 최악의 상황이라도 이 정도의 성능은 보장한다는 의미 가장 많이(일반적으로) 사용함  1.2.2. 오메가 표기법 \\(\\Omega(N) \\)  최상의 알고리즘 실행 시간을 표기  1.2.3. 세타 표기법 \\(\\Theta(N) \\)  알고리즘 평균 실행 시간을 표기  1.3. Big-O 표기법 1.3.1. \\( O(n) \\)  입력 \\(n \\)에 따라 결정되는 시간 복잡도 함수.  \\(O(1) \\) \\(O(\\log n) \\) \\(O(n) \\) \\(O(n \\log n) \\) \\(O(n^2) \\) \\(O(2^n) \\) \\(O(n!) \\)   입력에 따라 기하급수적으로 시간 복잡도가 늘어날 수 있음.  \\(O(1) \u0026lt; O(\\log n) \u0026lt; O(n) \u0026lt; O(n \\log n) \u0026lt; O(n^2) \u0026lt; O(2^n) \u0026lt; O(n!) \\)    1.3.2. 계산법  \\(O(1) \\)  단순하게 입력 \\(n \\)에 따라 멸번 실행이 되는지 계산함. 실행은 무조건 2회(또는 상수회) 실행한다.  if n \u0026gt; 10: print(n)  \\(O(n) \\)  \\(n \\)에 따라 \\(n \\)번 또는 \\(k \\cdot n + b \\) 실행한다.  for idx in range(n): print(idx)  \\(O(n^2) \\)  \\(n \\)에 따라 \\(n^2 \\)번 또는 \\(k \\cdot n^2 + b \\) 등을 실행한다.  for num in range(n): for index in range(n): print(index)   1.3.3. 표기 방법  시간복잡도는 결국 입력값 \\(n \\)에 따라 성능이 결정됨. 결국 알고리즘 성능에 가장 영향을 끼치는 값을 기준으로 표기함. 따라서 상수 \\(k, b \\)는 표기할 때 생략함 \\(k \\cdot n^2 + b \\)의 경우 Big-O 표기법으로는 \\(O(n^2) \\)으로 표기함.  2. Reference  Fastcampus 알고리즘 / 기술면접 강의  ","permalink":"https://lucaseo.github.io/posts/2021-02-10-complexity/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 알고리즘 복잡도 1.1. 개념 1.1.1. 알고리즘 복잡도 계산이 필요한 이유  하나의 문제를 푸는 방법(알고리즘)은 다양할 수 있음. 여러가지 방법 중 어느 방법이 더 좋은지를 분석하기 위해 복잡도를 정의하고 계산함. 어느 것이 더 좋은 알고리즘인지 판단하는 기준이 됨.  1.1.2. 알고리즘 복잡도를 계산하는 방식  공간 복잡도 (space complexity)  알고리즘이 사용하는 메모리 사이즈   시간 복잡도 (time complexity)  알고리즘 실행 속도 특히, 시간 복잡도에 대한 이해는 필수    1.","title":"[KR] 알고리즘 복잡도"},{"content":"Hello World 스타트업에서 기업의 비재무 데이터 기반 지속가능 여신 및 신용평가 서비스를 기획하고 개발하는 일을 하는 무늬만 데이터사이언티스트입니다.\n전공은 경영학, 첫 커리어는 HR(인사관리)로 시작했지만, 데이터분석 기반 의사결정에 매력을 느끼고 커리어를 전환하게 되었습니다. 저는 여전히 스스로가 야매라고 생각합니다. 적절한 시기에 꼭 정식으로 공부를 할 계획입니다.\n데이터 시각화, 대시보드 구축, 추천시스템, MIR, 음악과 관련된 딥러닝 모델 등 등등 중구난방이지만 호기심이 많습니다. (시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )\n\u0026amp;nbsp\nTMI  스포티파이를 좋아하지만 애플뮤직을 씁니다. 평점은 왓챠에서 매기고, 넷플릭스, 왓챠 둘 다 봅니다. 디즈니도 언젠가는 꼭 \u0026hellip; ! HIPHOPLE.com 스태프를 했었습니다. (요즘은 뜸한 상태) 스페인어를 아주 조금 할 줄 압니다. 독일어를 배우고 싶습니다. 언젠가는 유럽에 정착하고 싶습니다. 최근에 미디(MIDI) 걸음마를 뗐습니다. 유투브 보는 걸 정말 좋아합니다. 다수의 치즈냥과 리트리버, 보더콜리, 웰시코기 채널을 구독 중입니다.  ","permalink":"https://lucaseo.github.io/about/","summary":"Hello World 스타트업에서 기업의 비재무 데이터 기반 지속가능 여신 및 신용평가 서비스를 기획하고 개발하는 일을 하는 무늬만 데이터사이언티스트입니다.\n전공은 경영학, 첫 커리어는 HR(인사관리)로 시작했지만, 데이터분석 기반 의사결정에 매력을 느끼고 커리어를 전환하게 되었습니다. 저는 여전히 스스로가 야매라고 생각합니다. 적절한 시기에 꼭 정식으로 공부를 할 계획입니다.\n데이터 시각화, 대시보드 구축, 추천시스템, MIR, 음악과 관련된 딥러닝 모델 등 등등 중구난방이지만 호기심이 많습니다. (시험기간에는 시험공부 빼곤 다 재미있고, 일 빼고 다 재미있는 법이죠\u0026hellip; )","title":"About"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 링크드 리스트 : Linked List 의 개념 1.1. 링크드 리스트의 구조  데이터와 데이터 사이를 화살표로 연결하여 관리하는 데이터 구조. 배열(Array)와의 차이점  1.2. 링크드 리스트와 배열(Array)와 차이점  배열 : 번호가 붙여진(인덱싱이 된) 칸에 원소들을 채워 넣어 관리함. 링크드 리스트: 각 원소들을 줄줄이 엮어서 관리함.  1.2.1. 배열과 링크드 리스트의 비유적 비교 (출처: 생활코딩)  메모리라는 개념을 우리는 건물에 비유할 수 있을 것 같습니다. 아래 예시는 배열을 사용하는 것과 linked list를 사용하는 것을 비유해서 보여주고 있습니다. 여러분의 회사가 한 건물의 일부를 임대해서 사용한다고 생각해주세요.\n    Source https://opentutorials.org/module/1335/8821    Array list의 첫 번째 회사는 모든 직원이 한곳에 모여있어야 한다는 철학이 있기 때문에 사무실이 모여있습니다. 배열은 건물을 이런 식으로 사용하는 것과 비슷합니다. 만약 회사가 성장해서 사무실이 좁아지면 더 이상 새로운 직원을 뽑을 수 없습니다. 붙어있는 공간이 없기 때문이죠. 만약 더 많은 공간이 필요하다면 더 많은 사람을 수용할 수 있는 공간을 찾아서 전체가 이사해야 합니다. Array list는 엘리먼트가 같은 곳에 모여있습니다. 만약에 3번째 자리로 가고 싶다면 한번에 3번째 방으로 갈 수 있습니다. 찾고자 하는 사무실이 몇 번째에 있는지 알고 있다면 Array list는 매우 빠릅니다.\n    Source https://opentutorials.org/module/1335/8821    Linked List의 두 번째 회사는 한 건물 내에서 한 회사가 임대한 사무실이 서로 떨어져 있습니다. 덕분에 직원이 늘어도 큰 걱정이 없습니다. 건물에서 비어있는 곳 아무데나 임대해서 들어가면 되니까요. 그런데 방문자가 사무실을 찾는 방법이 좀 비효율적입니다. 위의 그림에 있는 방문자가 3번째 사무실을 찾아가려면 우선 첫 번째 화살표의 사무실을 찾아가야 합니다. 이 사무실의 직원에게 다음 사무실이 어딘지 물어봅니다. 그럼 알려주는 사무실로 이동 한 후에 다시 물어봐서 그다음 사무실로 이동합니다. 이렇게 물어물어 사무실을 찾아가야 하는 방식이 Linked List입니다. 그래서 Linked List에서는 몇 번째 엘리먼트를 찾는 것이 느립니다.\n 1.3. 링크드 리스트 관련 용어  노드(node):  데이터가 저장되는 단위. [데이터값, 포인터]로 구성   포인터(pointer):  다음 데이터의 주소를 담고 있는 공간. 노드에서 다음 또는 이전 노드와의 연결 정보를 가지고 있는 공간.    1.4. 링크드 리스트의 장단점 장점:\n 데이터 공간을 미리 할당하지 않아도 됨. (array는 미리 데이터 공간을 할당해야 함.) 삽입과 삭제가 빠름. 따라서 삽입/삭제가 빈번히 일어날 때 많이 사용됨.  단점:\n 데이터 구조 표현에 소요되는 저장공간이 비교적 큼.  연결을 위한 별도의 데이터공간이 필요하기 때문에 저장공간 효율이 높지 않음.   데이터를 찾는 시간이 오래 걸림.  인덱싱이 된 배열와는 달리, 특정 N번째 원소에 접근하려면 링크드 리스트의 처음부터 순차적으로 원소를 훑으며 N번째 원소를 찾아가야 함.   중간에 위치한 데이터 삭제 시, 앞뒤 데이터의 연결을 다시 구현해야하는 부가적인 작업이 필요함.  2. 파이썬에서의 링크드 리스트 pt.1  링크드 리스트는 C언어에서 주요한 데이터구조이지만, 파이썬에서는 리스트 타입이 링크드 리스트의 기능을 모두 지원함.  2.1. 노드 구현하기 # 포인트가 없는 노드를 구현 class Node: def __init__(self, data): self.data = data self.next = None # 포인터가 있는 노드를 구현하기 class Node: def __init__(self, data, next=None): self.data = data self.next = next # 포인터 \u0026amp;nbsp\n2.2. 포인터를 활용하여 노드와 노드를 연결하기 node1 = Node(1) node2 = Node(2) head = node1 # 링크드리스트의 첫 시작은 node1로 지정함. node1.next =node2 # node1의 포인터에 node2를 저장하여 연결함. print(head.data) # node1의 데이터 출력 print(node1.next.data) # node2의 데이터 출력 1 2 \u0026amp;nbsp\n2.3. 링크드 리스트로 데이터 추가하기 class Node: def __init__(self, data, next=None): self.data = data self.next = next def add(data): node = head while node.next: node = node.next node.next = Node(data) node1 = Node(1) # 첫 번째 노드 생성 head = node1 # 첫 번째 노드를 head로 지정 for idx in range(2, 10): # 추가할 데이터를 iteration하며 추가하기 add(idx) 2.4. 링크드 리스트 데이터 접근하여 출력하기 # 한개씩 접근하기 print(head.data) # 1번째 노드 print(head.next.data) # 2번째 노드 print(head.next.next.data) # 3번째 노드 print(head.next.next.next.data) # 4번째 노드 1 2 3 4 \u0026amp;nbsp\n# 한꺼번에 접근하기 node = head while node.next: # 노드의 데이터 순차적으로 출력 print(node.data) node = node.next print(node.data) # next가 없는 마지막 노드의 데이터 출력 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n2.5. 링크드 리스트의 중간 위치에 데이터 추가하기 2.5.1. 단순하게 추가해보기 # 기존의 데이터 출력하기 node = head while node.next: print(node.data) node = node.next print(node.data) 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\nnode3_5 = Node(3.5) # 2와 3 사이에 위치할 데이터(2.5)가 담긴 노드 생성 node = head search = True while search: if node.data == 3: # 새 데이터(3.5)의 직전 데이터가 되는 노드(3)를 찾음. search = False # search 종료 else: node = node.next node_next = node.next # 노드(3)의 다음 노드(4)를 따로 빼기 node.next = node3_5 # 노드(3.5)를 노드(3)의 다음 노드로 연결 node3_5.next = node_next # 노드(4)을 노드(3.5)의 다음 노드로 연결 node = head while node.next: print(node.data) node = node.next print(node.data) 1 2 3 3.5 4 5 6 7 8 9 \u0026amp;nbsp\n3. 파이썬에서의 링크드 리스트 pt.2 3.1. 객체지향 프로그램으로 구현해보기 class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next 링크드 리스트의 노드 모두 순회하며 데이터를 출력하기\nlinkedlist_1 = NodeManagement(0) linkedlist_1.print_all() 0 \u0026amp;nbsp\nfor data in range(1, 10): linkedlist_1.add(data) linkedlist_1.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n3.2. 특정 데이터를 가진 노드 검색하기  search_node() 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: # 노드의 데이터가 찾고자 하는 데이터가 맞다면 그대로 리턴 return node else: node = node.next # 다음 노드로 순회 \u0026amp;nbsp\nlinkedlist_2 = NodeManagement(0) for data in range(1, 10): linkedlist_2.add(data) linkedlist_1.print_all() linkedlist_2.search_node(4).data # 데이터가 4인 노드의 데이터 출력 0 1 2 3 4 5 6 7 8 9 4 \u0026amp;nbsp\n3.3. 특정 인덱스의 노드 검색하기  get_node 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node \u0026amp;nbsp\nlinkedlist_3 = NodeManagement(0) for data in range(1, 10): linkedlist_3.add(data) linkedlist_1.print_all() # 전체 노드 데이터 출력 linkedlist_3.get_node(3).data # 3번째 노드의 데이터 출력  0 1 2 3 4 5 6 7 8 9 3 \u0026amp;nbsp\n3.2. 특정 노드 삭제하기  delete ㅁㅔ서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node def delete(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 삭제 \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: print(\u0026#34;No node avaible\u0026#34;) return if self.head.data == data: # 삭제하고자 하는 데이터가 가장 첫 번째 노드일 때 temp = self.head self.head = self.head.next del temp else: node = self.head while node.next: if node.next.data == data: # 삭제하고자 하는 데이터가 다음 노드에 있을 때 temp = node.next node.next = node.next.next # 다음 노드를 건너 뛰고, 다음다음 노드를 바로 연결 del temp pass else: node = node.next 노드를 한개(헤드) 만들어서 delete 메서드 실험하기\nlinkedlist_4 = NodeManagement(0) linkedlist_4.print_all() 0 \u0026amp;nbsp\nlinkedlist_4.head # 헤드가 존재함 \u0026lt;__main__.Node at 0x107e94d10\u0026gt; \u0026amp;nbsp\nlinkedlist_4.delete(0) # 헤드를 삭제 linkedlist_4.head # 헤드 삭제 후에는 헤드가 출력되지 않음 linkedlist_4.print_all() # 링크드 리스트에 노드가 존재하지 않음 \u0026amp;nbsp\n여러 노드를 추가해서 delete 메서드 실험하기\nlinkedlist_5 = NodeManagement(0) for data in range(1, 10): linkedlist_5.add(data) linkedlist_5.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\nlinkedlist_5.delete(6) # 노드 한개를 삭제함 linkedlist_5.print_all() 0 1 2 3 4 5 7 8 9 \u0026amp;nbsp\nlinkedlist_5.delete(3) linkedlist_5.print_all() 0 1 2 4 5 7 8 9 \u0026amp;nbsp\n3.3. 새 노드 중간에 삽입하기  insert 메서드  class Node: def __init__(self, data, next=None): self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.nodeCount = 1 def add(self, data): \u0026#34;\u0026#34;\u0026#34; 노드를 \u0026#39;순차적으로\u0026#39; 추가함. \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: self.head = Node(data) else: node = self.head while node.next: node = node.next node.next = Node(data) self.nodeCount += 1 def print_all(self): \u0026#34;\u0026#34;\u0026#34; 노드를 순회하며 출력함. \u0026#34;\u0026#34;\u0026#34; node = self.head while node: print(node.data) node = node.next def search_node(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 검색하기 \u0026#34;\u0026#34;\u0026#34; node = self.head while node: if node.data == data: return node else: node = node.next def get_node(self, position): \u0026#34;\u0026#34;\u0026#34; 지정한 인덱스의 노드 가져오기 \u0026#34;\u0026#34;\u0026#34; if position \u0026lt; 0 or position \u0026gt; self.nodeCount: print(\u0026#34;Error: Position not in range of length of the linked list.\u0026#34;) return None else: idx = 0 node = self.head while idx \u0026lt; position: node = node.next idx += 1 return node def delete(self, data): \u0026#34;\u0026#34;\u0026#34; 특정 데이터를 가진 노드 삭제 \u0026#34;\u0026#34;\u0026#34; if self.head == \u0026#34;\u0026#34;: print(\u0026#34;No node avaible\u0026#34;) return if self.head.data == data: # 삭제하고자 하는 데이터가 가장 첫 번째 노드일 때 temp = self.head self.head = self.head.next del temp else: node = self.head while node.next: if node.next.data == data: # 삭제하고자 하는 데이터가 다음 노드에 있을 때 temp = node.next node.next = node.next.next # 다음 노드를 건너 뛰고, 다음다음 노드를 바로 연결 del temp pass else: node = node.next def insert(self, data, position_index): \u0026#34;\u0026#34;\u0026#34; 지정한 위치에 데이터 노드 삽입하기 \u0026#34;\u0026#34;\u0026#34; if position_index == 0: # head 위치에 삽입하기 temp = self.head self.head = Node(data) self.head.next = temp self.nodeCount += 1 elif position_index \u0026gt; self.nodeCount + 1: print(\u0026#34;Error: Insert index exceeds the length of linked list\u0026#34;) return else: node = self.head search_index = 0 search_status = True while search_status: search_index += 1 if search_index == position_index: search_status = False insert_node = Node(data) node_next = node.next node.next = insert_node insert_node.next = node_next self.nodeCount += 1 else: node = node.next linkedlist_6 = NodeManagement(0) for data in range(1, 10): linkedlist_6.add(data) linkedlist_6.print_all() # 노드 순회하며 데이터 출력하기 linkedlist_6.nodeCount # 노드 개수 출력하기 0 1 2 3 4 5 6 7 8 9 10 \u0026amp;nbsp\nlinkedlist_6.insert(17, 7) # 17이라는 데이터를 7번째 인덱스에 넣기 linkedlist_6.print_all() linkedlist_6.nodeCount # 노드 삽입 후 노드 개수 출력 0 1 2 3 4 5 6 17 7 8 9 11 \u0026amp;nbsp\n4. 링크드 리스트의 개선된 타입 4.1. 더블 링크드 리스트 : Doubly-Linked List  링크드 리스트의 단점을 보완하기 위해 등장함.  4.1.1. 기본 구조  양방향으로 연결되어 있어, 노드 탐색이 양쪽으로 모두 가능한 구조 즉, 다음 노드를 가리키는 포인터(next) 뿐만 아니라, 이전 노드를 가리키는 포인터(prev)도 존재함.  \u0026amp;nbsp\n4.1.2. 더블 링크드 리스트의 장단점 장점:\n 양방향으로 탐색이 가능함.  단점:\n 이전 노드를 가리키는 포인터가 추가되기 때문에 메모리 사용량이 늘어남. 삽입/삭제 연산에 있어, 앞/뒤 연결링크를 조정해줘야 하기 때문에 구조가 복잡해짐.  \u0026amp;nbsp\n4.1.2. 파이썬으로 구현한 더블 링크드 리스트  더블 링크드 리스트의 노드는 이전 노드와 다음 노드를 가리키는 포인터를 포함하는 것이 핵심.  class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next \u0026amp;nbsp\ndouble_linked_list = NodeManagement(0) for data in range(1, 10): double_linked_list.add(data) double_linked_list.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n4.2. 더블 링크드 리스트의 검색과 삽입 4.2.1. 특정 노드 이전에 새로운 노드 삽입하기 class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next def search_from_head(self, data): if self.head == None: return None node = self.head while node: if node.data == data: return node else: node = node.next return None def search_from_tail(self, data): if self.head == None: return None node = self.tail while node: if node.data == data: return node else: node = node.prev return None def insert_before(self, data, prev_data): if self.head == None: self.head = Node(data) return None else: node = self.tail while node.data != prev_data: node = node.prev if node == None: return None new_node = Node(data) prev_new = node.prev prev_new.next = new_node new_node.prev = prev_new new_node.next = node node.prev = new_node return None \u0026amp;nbsp\ndouble_linked_list = NodeManagement(0) for data in range(1, 10): double_linked_list.add(data) double_linked_list.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\n# 뒤에서부터 찾기 node_3 = double_linked_list.search_from_tail(3) node_3.data 3 \u0026amp;nbsp\n# 앞에서부터 찾기 node_8 = double_linked_list.search_from_head(8) node_8.data 8 \u0026amp;nbsp\ndouble_linked_list.insert_before(4.5, 5) double_linked_list.print_all() 0 1 2 3 4 4.5 5 6 7 8 9 \u0026amp;nbsp\nnode_4_5 = double_linked_list.search_from_tail(4.5) node_4_5.data 4.5 \u0026amp;nbsp\n4.2.2. 특정 노드 이후에 새로운 노드 삽입하기 class Node: def __init__(self, data, prev=None, next=None): self.prev = prev self.data = data self.next = next class NodeManagement: def __init__(self, data): self.head = Node(data) self.tail = self.head def add(self, data): if self.head == None: self.head = Node(data) self.tail = self.head else: node = self.head while node.next: node = node.next new = Node(data) node.next = new new.prev = node self.tail = new def print_all(self): node = self.head while node: print(node.data) node = node.next def search_from_head(self, data): if self.head == None: return None node = self.head while node: if node.data == data: return node else: node = node.next return None def search_from_tail(self, data): if self.head == None: return None node = self.tail while node: if node.data == data: return node else: node = node.prev return None def insert_before(self, data, prev_data): if self.head == None: self.head = Node(data) return None else: node = self.tail while node.data != prev_data: node = node.prev if node == None: return None new_node = Node(data) prev_new = node.prev prev_new.next = new_node new_node.next = node return None def insert_after(self, data, next_data): if self.head == None: self.head = Node(data) return None else: node = self.head while node.data != next_data: node = node.next if node == None: return None new_node = Node(data) next_new = node.next new_node.next = next_new new_node.prev = node if new_node.next == None: self.tail = new_node else: node.next = new_node return True double_linked_list_2 = NodeManagement(0) for data in range(1, 10): double_linked_list_2.add(data) double_linked_list_2.print_all() 0 1 2 3 4 5 6 7 8 9 \u0026amp;nbsp\ndouble_linked_list_2.insert_after(2.5, 2) True \u0026amp;nbsp\ndouble_linked_list_2.print_all() 0 1 2 2.5 3 4 5 6 7 8 9 5. 정리해보기  배열 리스트는 일정한 공간을 할당 후, 그 공간 안에서 물리적인 순서를 가지는 자료구조이다. 반대로 링크드 리스트(연결리스트)는 데이터를 담고 있는 노드들의 논리적인 순서를 위한 자료구조이다. 논리적인 구조는 이전/다음 노드를 가리키는 포인터를 통해 정의한다. 링크드 리스트는 처음부터 특정 공간을 할당하지 않아 메모리 활용도가 유연하고, 메모리 낭비를 막을 수 있다. 삽입과 삭제가 빠르다. 구현 작업은 다소 복잡할 지 모르나, 구현 후 실행은 간단하다. 하지만 포인터라는 요소까지 포함하여야 하기 때문에 비교적 더 큰 공간을 잡아먹는다. 또한 배열와는 달리, 순차적으로 노드를 훑으며 노드를 찾아가야 하기 때문에 시간이 좀 더 걸린다. 단순한 링크드 리스트는 한방향으로만 노드를 탐색하기 때문에 비효율적일 수 있다. 양방향 링크드 리스트는 앞/뒤에서부터 동시에 노드를 탐색하기 때문에 단순 링크드 리스트의 단점을 보완할 수 있다.  6. Reference  Fastcampus 알고리즘 / 기술면접 강의 프로그래머스 프로그램밍 강의 어서와! 자료구조는 처음이지? 생활코딩  ","permalink":"https://lucaseo.github.io/posts/2021-02-01-python-datastructure-linked-list/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 1. 링크드 리스트 : Linked List 의 개념 1.1. 링크드 리스트의 구조  데이터와 데이터 사이를 화살표로 연결하여 관리하는 데이터 구조. 배열(Array)와의 차이점  1.2. 링크드 리스트와 배열(Array)와 차이점  배열 : 번호가 붙여진(인덱싱이 된) 칸에 원소들을 채워 넣어 관리함. 링크드 리스트: 각 원소들을 줄줄이 엮어서 관리함.  1.2.1. 배열과 링크드 리스트의 비유적 비교 (출처: 생활코딩)  메모리라는 개념을 우리는 건물에 비유할 수 있을 것 같습니다.","title":"[KR] 자료구조 \u0026 알고리즘 : 링크드 리스트(Linked List)"},{"content":" 애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 0. 자료구조? 알고리즘?  자료구조 Data Structure  대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조    \u0026amp;nbsp\n 체계적인 데이터 구조화의 필요성  코드 상에서 효율적인 데이터 처리하기 위함 어떤 데이터 구조를 사용하느냐에 따라 효율이 달라짐.    \u0026amp;nbsp\n 알고리즘이란  어떠한 문제를 풀기 위한 절차 / 방법 특정 문제에 해당하는  특정 입력을 넣으면 특정 출력을 얻을 수 있도록 하는 프로그래밍      \u0026amp;nbsp\n 문제를 푸는 방법은 각양각색이지만, 다음을 고려하여 계산을 하고 최적의 방법을 찾는다.  어느 정도의 시간을 쓰는가? 어느 정도의 저장 공간을 활용하는가?    \u0026amp;nbsp\n 자료구조와 알고리즘이 중요한 이유  어떤 자료구조와 알고리즘을 쓰느냐에 따라 성능 면에서 아주 큰 차이가 생김.     1. 배열 : Array  같은 종류의 데이터를 순차적으로 저장하는 형태의 데이터타입  1.1. 배열의 필요성  같은 종류의 데이터를 효율적으로 관리 같은 종류의 데이터를 순차적으로 데이터를 저장  1.2. 배열의 장단점 (파이썬이 아닌 C로 봤을 때)\n장점:\n 구현이 쉬움. 빠른 접근이 가능함.  인덱스index가 매겨지기에, 첫 데이터(인덱스 0)의 위치를 기준으로 상대적인 위치의 데이터에 빠르게 접근 가능 즉, 일단 만들어 놓으면 빠른 접근이 가능.   검색에 용이함.  \u0026amp;nbsp\n단점:\n 데이터 추가와 삭제가 어려움.  미리 최대 길이를 지정해야 하기 때문   데이터를 추가하거나 삭제를 하면 길이에 변화가 생김.  변수를 새로 만드는 수 밖에 없음   즉, 일단 만들어 놓으면 수정이 어렵고 메모리 재사용이 불가함.  1.3. 파이썬에서의 배열  파이썬에서는 리스트(list) 타입  # 1차원 배열 array_1d = [1, 2, 3, 4, 5] # 2차원 배열 array_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] print(array_1d) print(array_2d) [1, 2, 3, 4, 5] [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \u0026amp;nbsp\n### 연습(1) # 2차원 배열에서 9, 8, 7을 순서대로 출력해보기 array_2d = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] print(array_2d[2][2]) print(array_2d[2][1]) print(array_2d[2][0]) 9 8 7 \u0026amp;nbsp\n### 연습(2) # 아래 데이터셋에서 전체 이름 안에서 M은 몇 번 나왔는지 빈도수 출력 dataset = [\u0026#39;Braund, Mr. Owen Harris\u0026#39;, \u0026#39;Cumings, Mrs. John Bradley (Florence Briggs Thayer)\u0026#39;, \u0026#39;Heikkinen, Miss. Laina\u0026#39;, \u0026#39;Futrelle, Mrs. Jacques Heath (Lily May Peel)\u0026#39;, \u0026#39;Allen, Mr. William Henry\u0026#39;, \u0026#39;Moran, Mr. James\u0026#39;, \u0026#39;McCarthy, Mr. Timothy J\u0026#39;, \u0026#39;Palsson, Master. Gosta Leonard\u0026#39;, \u0026#39;Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\u0026#39;, \u0026#39;Nasser, Mrs. Nicholas (Adele Achem)\u0026#39;, \u0026#39;Sandstrom, Miss. Marguerite Rut\u0026#39;, \u0026#39;Bonnell, Miss. Elizabeth\u0026#39;, \u0026#39;Saundercock, Mr. William Henry\u0026#39;, \u0026#39;Andersson, Mr. Anders Johan\u0026#39;, \u0026#39;Vestrom, Miss. Hulda Amanda Adolfina\u0026#39;, \u0026#39;Hewlett, Mrs. (Mary D Kingcome) \u0026#39;, \u0026#39;Rice, Master. Eugene\u0026#39;, \u0026#39;Williams, Mr. Charles Eugene\u0026#39;, \u0026#39;Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\u0026#39;, \u0026#39;Masselmani, Mrs. Fatima\u0026#39;, \u0026#39;Fynney, Mr. Joseph J\u0026#39;, \u0026#39;Beesley, Mr. Lawrence\u0026#39;, \u0026#39;McGowan, Miss. Anna \u0026#34;Annie\u0026#34;\u0026#39;, \u0026#39;Sloper, Mr. William Thompson\u0026#39;, \u0026#39;Palsson, Miss. Torborg Danira\u0026#39;, \u0026#39;Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\u0026#39;, \u0026#39;Emir, Mr. Farred Chehab\u0026#39;, \u0026#39;Fortune, Mr. Charles Alexander\u0026#39;, \u0026#39;Dwyer, Miss. Ellen \u0026#34;Nellie\u0026#34;\u0026#39;, \u0026#39;Todoroff, Mr. Lalio\u0026#39;] count = 0 for data in dataset: for idx in range(len(data)): if data[idx]==\u0026#39;M\u0026#39;: count += 1 print(count) 38  2. 큐 : Queue 2.1. 큐의 구조  가장 먼저 넣은 데이터를 가장 먼저 꺼낼 수 있는 구조  FIFO 또는 LILO 정책을 씀. FIFO를 더 많이 씀.  FIFO : First in, First out LILO : Last in, Last out      2.2. 큐와 관련된 용어  Enqueue: 큐에 데이터를 넣는 기능 Dequeue: 큐에서 데이터를 꺼내는 기능  2.3. 큐가 많이 사용되는 예시  재귀 알고리즘 역추적을 해야할 때 (i.e. 문서 작업 시 실행 취소) 운영체제 멀티태스킹을 위한 프로세스 스케쥴링을 구현할 때  2.4. 큐의 시간 복잡도  삽입 / 삭제  원소를 삽입하거나 삭제하는 경우  O(1)      2.5. 큐의 장단점 장점:\n 데이터의 삽입과 삭제가 빠름  \u0026amp;nbsp\n단점:\n 정책에 따라 가장 위쪽의 원소만 접근 가능함.  i.e. FIFO의 경우 맨 위의 원소만 접근이 가능함.   탐색이 상당히 비효율적임. 다 꺼내보면서 탐색해야 함.  2.6. 파이썬에서의 Queue  queue 라이브러리를 사용.  Queue() : 가장 일반적인 큐(FIFO) 자료구조 LifoQueue() : 나중에 입력된 데이터일 수록 먼저 출력되는 구조 (스택) PriorityQueue() : 입력된 데이터마다 우선순위를 설정, 우선순위 순으로 데이터 출력    2.6.1. queue.Queue() : FIFO # queue 라이브러리 import queue fifo_queue = queue.Queue() # FIFO fifo_queue.put(3) # enqueue fifo_queue.put(15) fifo_queue.put(26) fifo_queue.put(56) print(fifo_queue.qsize()) # 크키 print(fifo_queue.queue) 4 deque([3, 15, 26, 56]) \u0026amp;nbsp\nfifo_queue.get() # dequeue 3 \u0026amp;nbsp\nfifo_queue.get() 15 \u0026amp;nbsp\nfifo_queue.get() 26 \u0026amp;nbsp\nfifo_queue.get() 56 \u0026amp;nbsp\n2.6.2. queue.LifoQueue() : LIFO (Last in, First out) lifo_queue = queue.LifoQueue() # LIFO lifo_queue.put(3) # enqueue lifo_queue.put(15) lifo_queue.put(26) lifo_queue.put(56) print(lifo_queue.qsize()) print(lifo_queue.queue) 4 [3, 15, 26, 56] \u0026amp;nbsp\nlifo_queue.get() # dequeue 56 \u0026amp;nbsp\nlifo_queue.get() 26 \u0026amp;nbsp\nlifo_queue.get() 15 \u0026amp;nbsp\nlifo_queue.get() 3 2.6.3. queue.PriorityQueue() : 우선순위에 따라 dequeue pri_queue = queue.PriorityQueue() pri_queue.put((34, \u0026#39;a\u0026#39;)) # 튜플 형태로 넣음 (우선순위, 데이터) pri_queue.put((14, \u0026#39;b\u0026#39;)) pri_queue.put((72, \u0026#39;c\u0026#39;)) pri_queue.put((11, \u0026#39;d\u0026#39;)) pri_queue.put((48, \u0026#39;e\u0026#39;)) print(pri_queue.qsize()) # size print(pri_queue.queue) # 숫자가 낮을 수록 우선순위가 높음 (i.e. 1순위, 2순위) 5 [(11, 'd'), (14, 'b'), (72, 'c'), (34, 'a'), (48, 'e')] \u0026amp;nbsp\npri_queue.get() # dequeue (11, 'd') \u0026amp;nbsp\npri_queue.get() (14, 'b') \u0026amp;nbsp\npri_queue.get() (34, 'a') \u0026amp;nbsp\npri_queue.get() (48, 'e') \u0026amp;nbsp\npri_queue.get() (72, 'c') \u0026amp;nbsp\n연습\nqueue라이브러리가 아닌, 파이썬의 리스트를 가지고 enqueue, dequeue 구현해본다.\nclass queue_list: def __init__(self): self.queue = list() def __size__(self): return len(self.queue) def __items__(self): return self.queue def enqueue(self, data): self.queue.append(data) def dequeue(self): target_data = self.queue[0] del self.queue[0] return target_data ls_queue = queue_list() ls_queue.enqueue(4) ls_queue.enqueue(9) ls_queue.enqueue(1) print(ls_queue.__size__()) print(ls_queue.__items__()) 3 [4, 9, 1] \u0026amp;nbsp\nls_queue.dequeue() 4 \u0026amp;nbsp\nls_queue.dequeue() 9 \u0026amp;nbsp\nls_queue.dequeue() 1 \u0026amp;nbsp\n 3. 스택 : Stack  데이터를 제한적으로 넣을 수 있는 구조  한쪽 끝에서만 자료를 넣거나 뺄 수 있는 구조    큐와의 차이점\n 큐 : FIFO 정책 스택 : LIFO 정책 \u0026ndash;\u0026gt; Last in, First out  3.1. 스택의 구조 스택은 LIFO, FILO 구조이지만, LIFO, FILO라고 말하기보다는 통상 이러한 구조를 스택 Stack 그 자체로 부름.\n 가장 마지막에 넣은 것을 가장 먼저. 가장 먼저 넣은 것을 가장 마지막에.  LIFO : 마지막에 넣은 데이터를 가장 먼저 추출하는 데이터 관리 정책. FILO : 처음 넣은 데이터를 가장 마지막에 추출하는 데이터 관리 정책.    3.2. 스택 관련 용어  push() : 데이터를 스택에 삽입하는 연산 (넣기) pop() : 데이터를 스택에서 삭제하는 연산 (꺼내기)| stack underflow : 비어있는 스택에서 데이터를 꺼내려고 할 때 생기는 오류 stack overflow : 가득 차있는 스택에 데이터를 삽입하려고 할 때 생기는 오류  3.3. 스택의 활용  컴퓨터 내부의 프로세스의 함수들이 동작하는 방식에 쓰임 실행 취소 : 가장 최근에 했던 작업부터 거슬러 올라가며 취소 웹 브라우저 뒤로 가기 기능 : 가장 최근에 봤던 페이지 순으로 거슬러 가며 브라우징  3.4. 스택의 구현 방법  배열(array)  장점:  구현이 쉬움 접근이 빠름   단점:  데이터의 최대 개수를 미리 정해야 함. 데이터 삽입/삭제 시 매우 비효율적.     연결리스트(linked list)  장점:  데이터 최대 개수가 정해져 있지 않음. 데이터 삽입 삭제가 용이함.   단점:  데이터 접근이 한번에 가능하지 않음. 따라서 시간이 걸림.      3.5. 파이썬에서의 Stack  리스트를 통해서 배열(array) 기반의 스택을 구현해볼 수 있음.  .append() : push .pop() : pop    data_stack = list() data_stack.append(1) # push data_stack.append(3) print(data_stack) [1, 3] \u0026amp;nbsp\ndata_stack.pop() # pop 3 \u0026amp;nbsp\nprint(data_stack) [1] \u0026amp;nbsp\nReference  Fast Campus 알고리즘 / 기술면접 강의 [자료구조] 스택(Stack), 큐(Queue), 덱(Deque)  ","permalink":"https://lucaseo.github.io/posts/2021-01-27-python-datastructure-array-que-stack/","summary":"애매모호하게만 알고 있는 자료구조를 다시 공부하고 정리하는 포스트입니다. 잘 못 이해하고 있는 부분이 있다면 주저없이 지적 부탁 드립니다 :)\n 0. 자료구조? 알고리즘?  자료구조 Data Structure  대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조    \u0026amp;nbsp\n 체계적인 데이터 구조화의 필요성  코드 상에서 효율적인 데이터 처리하기 위함 어떤 데이터 구조를 사용하느냐에 따라 효율이 달라짐.    \u0026amp;nbsp\n 알고리즘이란  어떠한 문제를 풀기 위한 절차 / 방법 특정 문제에 해당하는  특정 입력을 넣으면 특정 출력을 얻을 수 있도록 하는 프로그래밍      \u0026amp;nbsp","title":"[KR] 자료구조 \u0026 알고리즘 : 배열(array), 큐(queue), 스택(stack)"},{"content":"0. 데이터와 librosa 실제로 소리 데이터를 다뤄보기 위해서 음악 데이터를 준비하겠습니다. 음악 장르 분류 데이터셋으로 유명한 GTZAN Dataset을 다운받아 음악 파일을 하나 선택했습니다.\n그리고 Librosa는 오디오와 음악 분석을 위 빠질 수 없는 파이썬 패키지입니다. 음원이나 소리 파일을 불러와 waveform을 시각화 하거나 다른 형태로 변환할 수 있는 기능을 제공합니다.\n( Librosa는 pip install librosa 명령어를 통해 설치할 수 있습니다. )\nimport warnings warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt import IPython.display as ipd import librosa import librosa.display file_path = \u0026#39;disco.00054.wav\u0026#39; # 실습에 사용할 음악 파일 어떤 음악인지 한번 들어볼까요?\nipd.Audio(file_path)   1. Waveform 음원을 아래처럼 간단하게 읽어와보겠습니다. 튜플인 결과값에서 첫 번째는 numpy array형태의 waveform의 amplitude값이고, 두 번째 값은 sampling rate으로, 초당 샘플 갯수를 의미합니다. Sampling rate는 파일을 읽을 때, parameter로 설정할 수 있습니다. Default값은 22050입니다.\nwav, sr = librosa.load(file_path) print(\u0026#34;Amplitude: \\n\u0026#34;, wav) print(\u0026#34;Sampling rate: \u0026#34;, sr) Amplitude: [-0.08001709 -0.07550049 -0.08358765 ... 0.08270264 0.10083008 0.10562134] Sampling rate: 22050 시각화를 통해 데이터가 아래와 같은 형태의 waveform을 띄고 있음을 확인해 볼 수 있습니다.\nfig = plt.figure(figsize = (14,5)) librosa.display.waveplot(wav, sr=sr) plt.ylabel(\u0026#34;Amplitude\u0026#34;) plt.show()    2. FFT (Fast Fourier Transform) 앞서 다룬 포스트에서 time-domain의 waveform을 FFT(Fast Fourier Transform)을 통해 분해(decompose)하고, frequency-domain으로 변환하여 원본 소리 데이터를 형성하는 주파수(frequency)의 정도를 파악하고 시각화 할 수 있다고 이해했습니다. 이번에는 numpy를 통해 앞서 구한 waveform amplitude에 FFT를 적용할 수 있습니다.\nFFT를 적용하여 시각화한 결과는 아래와 같습니다. 주로 1000Hz 이하로 많이 분포해 있는 것을 확인할 수 있네요.\nfft = np.fft.fft(wav) magnitude = np.abs(fft) frequency = np.linspace(0, sr, len(magnitude)) left_frequency = frequency[:int(len(frequency)/2)] left_magnitude = magnitude[:int(len(magnitude)/2)] fig = plt.figure(figsize = (14,5)) plt.plot(left_frequency, left_magnitude) plt.xlabel(\u0026#34;Frequency\u0026#34;) plt.ylabel(\u0026#34;Magnitude\u0026#34;) plt.show()    3. STFT (Short-Time Fourier Transform) STFT(Short-Time Fourier Transform)은 시간 정보가 유실되는 것을 방지하기 위해, 사전에 정의한 시간의 간격(window 또는 frame) 단위로 쪼개어 푸리에 변환을 적용하는 기법입니다. STFT는 librosa를 통해 적용할 수 있습니다. 이때, window의 크기(n_fft)와 window 간에 겹치는 사이즈(hop_length)를 설정해줍니다. 일반적으로는 n_fft의 1/4 정도가 겹치도록 설정한다고 합니다.\nn_fft = 2048 hop_length = 512 stft = librosa.stft(wav, n_fft = n_fft, hop_length = hop_length) spectrogram = np.abs(stft) print(\u0026#34;Spectogram :\\n\u0026#34;, spectrogram) Spectogram : [[1.42030740e+00 7.47260690e-01 5.37097007e-02 ... 1.88164175e-01 1.21684396e+00 2.43966293e+00] [1.24079692e+00 6.81115210e-01 6.51928782e-02 ... 2.08189130e-01 1.22743416e+00 2.52433753e+00] [1.09137118e+00 4.82469022e-01 1.85490116e-01 ... 6.99194148e-02 1.43492615e+00 2.53518319e+00] ... [3.84226470e-04 1.63909295e-04 9.80101977e-05 ... 2.23124225e-04 2.80503096e-04 1.98973445e-04] [3.00532440e-04 1.77996873e-04 1.29194887e-04 ... 9.72686321e-05 2.01086092e-04 9.30428141e-05] [2.59254826e-04 9.42422412e-05 5.96536411e-05 ... 7.49909232e-05 1.41018099e-04 1.10232315e-04]] 3.1. Spectogram STFT를 적용하여 구한 spectogram을 아래와 같이 시각화 해봤습니다. x축은 시간, y축은 주파수, 그리고 주파수의 정도를 색깔로 확인할 수 있습니다. 그런데 값이 너무 미세해서 차이를 파악하고 관찰하기 적합하지 않습니다.\nfig = plt.figure(figsize = (14,5)) librosa.display.specshow(spectrogram, sr=sr, hop_length=hop_length) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.plasma() plt.show()    3.2. Log-spectogram 그래서 보통 푸리에변환 이후 dB(데시벨) scaling을 적용한 Log-spectogram을 구합니다. 다분히 시각적인 이유뿐만 아니라, 사람의 청각 또한 소리를 dB scale 로 인식하기 때문에, 이를 반영하여 spectogram을 나타내는 것이 분석에 용이합니다.\nlibrosa.amplitude_to_db()를 통해 Log-spectogram을 구하여 시각화 한 결과입니다. 대부분의 에너지가 1024Hz이하의 낮은 주파수대역에 모여 있는 것을 볼 수 있네요.\nlog_spectrogram = librosa.amplitude_to_db(spectrogram) fig = plt.figure(figsize = (14,5)) librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length, x_axis=\u0026#39;time\u0026#39;, y_axis=\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;) plt.show()    4. MFCC 마지막으로 MFCC(Mel Frequency Cepstral Coefficient)를 구하고 시각화해보겠습니다. MFCC는 오디오 신호 처리 분야에서 많이 사용되는 소리 데이터의 특징값(Feature)으로, 사람의 청각이 예민하게 반응하는 정보를 강조하여 소리가 가지는 고유한 특징을 추출한 값입니다.\n마찬가지로 librosa.feature.mfcc()를 통해 feature값을 아래와 같이 추출할 수 있습니다. 파라미터 중 n_mfcc는 추출하고자 하는 mfcc의 개수입니다. 이번 실습에서는 13개로 설정했습니다.\nMFCCs = librosa.feature.mfcc(wav, sr = 22050, n_fft = n_fft, hop_length = hop_length, n_mfcc = 13) # number of coefficient we want to extract print(\u0026#34;MFCCs Shape: \u0026#34;, MFCCs.shape) print(\u0026#34;MFCCs: \\n\u0026#34;, MFCCs) MFCCs Shape: (13, 1293) MFCCs: [[-176.91516 -173.07141 -171.01598 ... -144.9992 -153.77185 -155.61522 ] [ 118.94415 117.39079 108.01162 ... 111.50748 108.44453 113.359665 ] [ -12.249197 -16.364796 -20.116379 ... -68.0366 -40.615326 -32.124104 ] ... [ -7.0000467 -8.825797 -10.732431 ... -16.528994 -17.69807 -21.954914 ] [ 10.861979 10.393564 7.8947186 ... -8.206779 -3.6493917 -4.4267316] [ -12.490692 -10.728968 -14.610505 ... -2.9667187 -12.053108 -9.9868355]] fig = plt.figure(figsize = (14,5)) librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length, x_axis=\u0026#39;time\u0026#39;,) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Frequency\u0026#34;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;) plt.show()    5. 한줄 요약  Librosa라는 킹갓제너럴 파이썬 패키지를 이용해서 소리 데이터를 불러오고, 변형할 수 있다!  6. Reference  audio-processing-wave by scpark20  ","permalink":"https://lucaseo.github.io/posts/2021-01-22-hands-on-preprocess-audio-data/","summary":"0. 데이터와 librosa 실제로 소리 데이터를 다뤄보기 위해서 음악 데이터를 준비하겠습니다. 음악 장르 분류 데이터셋으로 유명한 GTZAN Dataset을 다운받아 음악 파일을 하나 선택했습니다.\n그리고 Librosa는 오디오와 음악 분석을 위 빠질 수 없는 파이썬 패키지입니다. 음원이나 소리 파일을 불러와 waveform을 시각화 하거나 다른 형태로 변환할 수 있는 기능을 제공합니다.\n( Librosa는 pip install librosa 명령어를 통해 설치할 수 있습니다. )\nimport warnings warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import numpy as np import matplotlib.pyplot as plt import IPython.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(3) - 파이썬으로 음악 데이터 읽어오기"},{"content":"2020년 올해는 \u0026hellip; \u0026amp;nbsp\n1. 올해의 가장 큰 변화 재택근무\nCOVID-19 방역에 다들 지쳐가고 서서히 경각심도 조금씩 희미해질 때쯤, 줄지 않는 확진자수에 결국 재택근무가 주기적으로 자리잡게 되었습니다. 사실 나는 개인적으로 재택근무가 별로라고 생각합니다. 집에서는 집중이 안 되기도 하고, 점심을 제 돈으로 해결해야 합니다. 집에서는 의자도 불편한데 새 의자를 사자니 가격이 만만치 않습니다. 재택근무를 원활하기 하기 위한 문화가 정착이 되지 않다보니, 으레 팀원들끼리 커뮤니케이션도 덜하게 되고 일하다가도 뭔지 모를 답답함을 느끼기도 했습니다.\n그래도 적응해야지 어쩌겠습니까. 춥지만 기분전환을 하기 위해 환기를 하거나 동네를 한바퀴 돌고 들어옵니다. 점심도 돈 아낀다고 직접 준비하기보다는 그냥 나가서 포장을 하구요, 나간 김에 바깥 공기도 좀 쐬고, 커피도 미리 테이크아웃을 해옵니다. 내려놓는게 있으니, 확실히 마음이 편해지는 부분이 확실히 있습니다. 커뮤니케이션도 마찬가지였습니다. 재택 근무 중 동료로부터 응답이 오지 않거나 슬랙에서 2~3 문장 이상의 커뮤니케이션이 이어질 때면, 바로 전화를 걸거나 구글 행아웃 링크를 바로 보내줍니다. 바로 어젠다에 대해 이야기를 나누거나, 바쁘면 조금 이따 이야기 하자고 답변이 옵니다. 전에는 왜 안 그랬는지 잘 모르겠네요. (대충 속이 시원해지는 개비스콘 짤.)\n부모님을 포함해서 제 주변에서는 재택하면 다들 와 좋겠다 하는 반응이지만, 그러나 저러나 저는 여전히 오피스가 더 좋기는 합니다. 그래도 시국이 시국인만큼 재택을 해야하니, 내년에도 계속해서 저만의 방식을 찾아가야겠습니다.\n\u0026amp;nbsp\n2. 올해의 깨달음 하\u0026hellip; 일단은 그냥 내가 하자 될 진 모르겠지만\n사실 2020년은 제가 속해있는 프로젝트의 부족한 부분과 기술부채를 해결했어야 하는 해입니다. 제가 혼자서 전부 하기에는 무리가 있던 부분이었고, 그래서 팀에 도움을 많이 요청했었습니다. 새로운 시니어 분이 오셔서 개발의 전반적인 부분을 손봐주시는 만큼 저도 기대하는 부분이 많았는데요. 하지만 영업으로 인해 생기는 ad-hoc 요청사항과 더 우선순위로 취급되는 프로젝트들이 등장하면서 기존의 프로젝트에는 제동이 걸렸습니다. 점점 우선순위에서 밀려나는걸 느끼게 되면서 스트레스를 많이 받기도 했습니다. 애매한 상태로 몇 개월이 그냥 지나가게 되었고 멘탈 관리도 조금 힘들었던 시기였습니다.\n그렇게 저는 지속적으로 서포트를 요청을 드리고, 이것도 해야하고 저것도 해야하는 상대방은 상대방대로 스트레스를 받았습니다. 교통정리가 되지 않은 채 다들 당장 눈 앞에 놓인 요청사항들을 해결하느라 경주마 같이 달리는 것을 보고, 이게 지금 조직의 체질이라는 것을 깨닫게 되었습니다. 체질이란 건 쉽게 바뀌지 않죠. 저도 지금 회사에서 2년을 일했으니, 그 체질이 형성되는데 기여하던 부분이 분명 있을 것 입니다.\n그래서 늦었지만 일단 저 혼자 어떻게든 해보기로 했습니다. 제가 도입해보고, 문서를 만들어서 공유를 조금씩 하고 있습니다. 시행착오가 많지만 이렇게라도 안 하면 아무것도 개선할 수 없을 것 같았습니다. 나중에 봤을 때 이게 좋아보이면 따라와 줄 것이고, 구리면 피드백을 해줄 것이라 생각합니다. 이도저도 아니고 그냥 아무 말도 없으면 \u0026hellip; 그건 일은 없기를 바랍니다 ㅠㅠ\n사실 이런 문제점은 마지막에 이렇게저렇게 해서 문제를 없앨 수 있었다! 하고 뿌듯하게 마무리 해야하는데, 그게 아니라 이제 시작인 것 같습니다. 그래서 2020년은 답답했고, 2021년에는 답답해하기보다 느려도 나 혼자 해보겠다. 인 것입니다.\n\u0026amp;nbsp\n3. 올해의 사서고생 방송통신대 두번째학기\n2020년 2월에는 방송통신대 정보통계학과에 3학년으로 편입했고, 어느 새 2번째 학기가 끝났습니다. 기존의 온라인 강의 방식을 고수하면서, 코로나로 인해 대면 기말고사는 기말과제물로 대체되었습니다. 호기롭게 7과목을 신청했다가 한 학기 내내 과제물로 허덕인 것이 많이 기억납니다.\n솔직히 방송통신대의 커리큘럼은 다른 4년제 대학교 동일 학부의 커리큘럼과 비교했을 때 깊이가 떨어지는 것이 사실입니다. 누군가에게는 우스워보일 수도 있을 것 같아요. 이렇게까지 해야하나 싶을 정도로 반복적이고 손이 많이 가는 노가다스러운 과제가 있는 반면, 영상 강의에서는 너무 성의없이 대충 짚고 넘어가서, 도저히 이해가 되지 않아 따로 찾아가며 공부해야 했던 과제도 있었습니다. 그렇지만 직장을 다니며 바쁜 와중에도 성실하게 강의를 듣고 과제를 제출했습니다.\n어제 성적이 나왔는데 모든 과목에서 A+를 받았습니다. 중고등학교, 대학교때는 정말 공부를 안 해서 상상도 못 했던 평점인데 말이죠. 사실 기존처럼 객관식 시험을 봤다면 이렇게 좋은 성적을 기대하지 못 했을 수도 있습니다. 성실하게 과제를 작성한 덕분인 것 같아요.\n이번 학기에 들었던 과목들 중 인상적이었던 몇몇 과목에 대해 조금 소감을 남겨보겠습니다.\n 파이썬과 R: 가볍게 생각한 과목이었고 난이도도 높지 않습니다. 하지만 담당이신 K교수님은 역시나 기대를 저버리지 않으시고 어마어마한 양의 과제를 내주셨습니다. 게다가 파이썬과 R로 동일한 문제를 풀고, 스크린샷을 찍고, 코드를 옮겨적고, 설명까지 적어야 하니\u0026hellip; 손가락이 많이 뻐근해졌달까요. 빅데이터의 이해: 이론적인 부분이 많고, 교재의 내용이 비교적 오래됐기 때문에 인상적이지 않았습니다. 하지만 과제물을 작성하면서 많이 생소했던 MapReduce에 대한 직접 그래프를 그려가며 작동원리를 알아갔던 부분과, 추천시스템 케이스 스터디를 하면서 제가 개인적으로 좋아하는 스포티파이에 대해 조사하고 정리했던 것이 정말 재미있었습니다. 시간 가는 줄 몰라서 시간 관리가 안 됐던 것은 함정 \u0026hellip; 통계학의 개념 및 제문제: 내용은 수리통계학인데 왜 과목명을 이렇게 정했는지 의문인 과목이었습니다. 그리고 역시나 많이 어려워서, 종강 후에도 마음을 놓지 않고 다시 관련 서적을 사서 제대로 공부해야겠다는 생각이 들었던 과목이었습니다.  아무튼 이렇게 방송통신대 정보통계학과에서의 두번째 학기가 지나갔습니다. 다음 학기부터는 통계,데이터과학과로 이름이 바뀐다고 하네요. 아마 다음 학기부터 저는 컴공 과목을 주로 듣게 될 것 같습니다.\n\u0026amp;nbsp\n4. 올해의 엉겁결 ADsP 자격증 취득\nADsP는 그냥 한번 봐볼까 하는 생각으로 접수했다가, 응시날이 코앞에 다가와 벼락치기를 했습니다. 퇴근길에 서점에서 교재 뒤쪽에 붙어있던 모의고사를 한번 풀어보는 방식으로 공부하고, 부족한 부분은 인터넷으로 찾아가며 부족한 부분을 위주로 준비했습니다.\nADsP는 데이터 이해, 데이터분석 기획, 데이터분석 의 세가지 과목으로 나뉘어져 있습니다. 응시일까지 과락을 걱정하며 가장 많이 걱정했던 과목은 데이터분석 기획 입니다. 데이터분석의 경우 방송통신대에서 수강한 데이터마이닝 과목과 겹치는 내용이 많기 때문에 걱정이 없었지만, 데이터 이해와 데이터분석 기획의 경우 실무에서는 많이 다루지 않는 암기위주의 이론적인 내용이 대부분이었기 때문에 체화하는 게 쉽지 않았고 오답이 많이 발생했었습니다.\n하지만 합격기준은 60/100점 이상과 과목별 40% 이상 취득 이기 때문에, 합격 기준만 넘자는 심정으로 선택과 집중을 한 결과, 합격을 할 수 있게 되었습니다. 엉겁결에 접수해놓고 너무 바빠서 완전히 까먹고 있었는데, 벼락치기 한 것 치고는 너무 다행이었던 그런 살짝은 부끄러운 자격증 취득이었습니다. 응시일이 생일날이었어서, 아침부터 투덜투덜 응시장 간 건 안 비밀 \u0026hellip;\n\u0026amp;nbsp\n5. 번외 어워즈 시상식 올해의 노래 뱃사공 - 다와가\n쓸데없이 아둥버둥 바득바득 치열하게 살고 있는 것은 아닌지, 나 자신을 다시 되돌아보게 해준 노래.\n\u0026amp;nbsp\n올해의 음식점 익선동 르블란서\n생일을 맞이해서 갔던 곳인데 코로나 스트레스가 날라갔던, 즐거운 시간만 기억나는 곳.\n\u0026amp;nbsp\n올해의 카페 커피온리 영등포구청점\n900원 아이스아메리카노. 재택하면서 아이스아메리카노를 매번 사먹어야 하는 걱정을 조금이나마 해소해준 테이크아웃 커피집.\n( + 커피 맛을 잘 몰라서 신경 안 씀)\n\u0026amp;nbsp\n올해의 IT기기 맥북 프로 16인치\n근데 이제 어마어마한 성능의 M1 맥북이 출시된.\n( + 그리고 떡락하는 중고가)\n\u0026amp;nbsp\n올해의 손떨림 생애 첫 대출로 1억을 넘어가는 후덜덜한 전세대출\n( + 그리고 대출받자마자 추락한 신용등급)\n\u0026amp;nbsp\n올해의 소망 COVID-19 종식과 해외여행\n\u0026amp;nbsp\n 2021년에 기대하는 것. 글또 5기 글또는 글쓰는 또라이가 세상을 바꾼다!고 하는 글쓰는 개발자 모임입니다. 4기에 이어 5기에 참여하고 있고, 글또 활동을 통해 2021년에도 글쓰는 습관을 기를 수 있도록 꾸준히, 제가 만족할 수 있는 퀄리티의 글을 쓰는 것이 목표입니다.\nCS 관련 기초 공부 방송통신대에서의 3번째 학기. 컴퓨터과학과 위주의 수강과목들 생각입니다. 그리고 알고리즘과 코딩테스트 준비도 할 계획입니다.\nSound of AI - Open Source Research Project Sound of AI는 Musimap의 시니어 데이터사이언티스트인 Valerio Velardo 씨가 운영하는 AI \u0026amp; Audio 커뮤니티입니다. Valerio의 튜토리얼을 따라 Audio와 AI에 관련된 공부를 하면서, 2021년에 진행될 Open Source Research 프로젝트에도 참여할 예정입니다. 아직 초기 단계이지만, 많은 기대가 됩니다.\n가짜연구소 스터디 가짜연구소는 머신러닝을 중심으로 스터디, 밋업 등의 이벤트가 이루어지고 있는 커뮤니티입니다. 2021년에는 가짜연구소의 스터디를 통해 캐글에도 도전해 볼 계획입니다.\n","permalink":"https://lucaseo.github.io/posts/2020-12-31-review-2020-2nd-half/","summary":"2020년 올해는 \u0026hellip; \u0026amp;nbsp\n1. 올해의 가장 큰 변화 재택근무\nCOVID-19 방역에 다들 지쳐가고 서서히 경각심도 조금씩 희미해질 때쯤, 줄지 않는 확진자수에 결국 재택근무가 주기적으로 자리잡게 되었습니다. 사실 나는 개인적으로 재택근무가 별로라고 생각합니다. 집에서는 집중이 안 되기도 하고, 점심을 제 돈으로 해결해야 합니다. 집에서는 의자도 불편한데 새 의자를 사자니 가격이 만만치 않습니다. 재택근무를 원활하기 하기 위한 문화가 정착이 되지 않다보니, 으레 팀원들끼리 커뮤니케이션도 덜하게 되고 일하다가도 뭔지 모를 답답함을 느끼기도 했습니다.","title":"[KR] 2020년 하반기가 지났다"},{"content":"이번 포스트에서는 소리의 파형을 분석하기 위해 사용되는 기법인 푸리에 변환과 특징 추출값으로 사용되는 MFCC의 개념에 대해서 알아보겠습니다.\n1. 소리는 주파수의 합산    Piano in Waveform   \u0026amp;nbsp\n위의 이미지는 실제 피아노 소리 파일을 파형(waveform) 형태로 시각화 한 것입니다. 간단한 피아노 소리이지만 매우 복잡한 파형을 그리고 있는 것을 볼 수 있는데요. 사실 우리가 흔히 들을 수 있는 이러한 \u0026ldquo;소리\u0026quot;라는 것은 각기 다른 단일 주파수를 가진 무수히 많은 정현파(sinewave)가 합산되어 형성된 것입니다. 제 경우에는 처음에 이해가 잘 되지 않았는데, 이런 시각화들이 많은 도움이 되었습니다. 직관적인 이해가 되시나요?\n\u0026amp;nbsp\n2. Fourier Transform 무수히 많은 정현파의 모음이 \u0026ldquo;소리\u0026quot;를 구성한다는 것은 이해를 했습니다. 하지만 이와 반대로 \u0026ldquo;소리\u0026quot;를 분석하기 위해서는 복잡한 소리(complex sound)가 어떠한 단일주파수들로 이루어져있는지를 분해(decompose)해봐야겠죠. 이를 위해 원본 소리에 행하는 작업을 푸리에변환(Fourier transform) 이라고 합니다. 다른 말로는 푸리에변환을 통해서 어떤 정현파가 얼마나 원본 소리를 구성하는지 파악할 수 있습니다.\n2.1. Spectrum 파형의 형태를 띄는 원본에 푸리에 변환을 적용하여 산출되는 결과물은 **스펙트럼(spectrum)**입니다. 스펙트럼이란, 각 주파수의 정도를 시각화하여 보이는 기법입니다. 아래 그림은 파형에 푸리에변환 기법 중 하나인 FFT(Fast Fourier Transform)을 적용한 결과를 보여주고 있는데요, 스펙트럼에서 X축은 0~12000 가량의 주파수이고, Y축은 각 주파수의 진폭(amplitude) 또는 그 정도(magnitude)를 나타냅니다.\n푸리에 변환을 통해서 파형이 스펙트럼으로 표현될 때 주목해야하는 점은, 파형은 시간의 흐름에 따라 변화하는 time-domain의 성질을 띄는 반면, 스펙트럼은 각 frequency마다 그 정도가 달라지는 frequency-domain의 성질을 띈다는 것입니다. 아래 그림의 스펙트럼에서는 전체 9초 가량의 시간대에 대한 주파수와 진폭을 모두 보여주고 있습니다. 즉, 스펙트럼에는 시간의 흐름에 따른 정보는 유실되는 것이죠.\n   푸리에변환의 결과 예시   2.2. STFT **STFT(Short-Time Fourier Transform)**는 푸리에변환의 한계를 보완하는 기법입니다. STFT는 전체 길이보다는 짧은 어떠한 시간 간격(window)을 설정한 후, 이 간격을 시간의 흐름에 따라 움직여가며(slide) 복수의 변환을 행하여, 시간의 흐름에 따른 주파수 정보를 얻습니다. 쉽게 말해, 9초의 소리가 있다면, 1초 간격으로 쪼갠 후 1초 간격으로, 푸리에변환을 하는 식인 것이죠. STFT의 산출물은 주파수, 진폭과 더불어 시간의 정보도 포함된 스펙토그램(spectogram)입니다. 아래의 스펙토그램 시각화에서 X축은 시간, Y축은 주파수, 그리고 주파수의 정도가 데시벨(색깔)로 표현되었습니다.\n   STFT의 동작 예시   \u0026amp;nbsp\n3. Mel Frequency Cepstral Coefficient (MFCC) 실제 소리나 음성을 분석할 때는 푸리에 변환만을 하지 않고, 그로부터 차원울 축소하고 분석에 용이한 특징을 추출하는 과정(Feature Extraction)을 거칩니다. 특히 오디오 신호 처리 분야에서 많이 사용되는 소리 데이터의 특징값(Feature)으로는 MFCC가 있습니다.(MFCC의 세부적인 내용과 구하는 방법은 다른 포스트에서 따로 다루도록 하겠습니다) MFCC는 갖가지 신호가 합쳐 생성된 \u0026ldquo;소리\u0026quot;가 가지는 고유한 특징을 추출한 값이라는 장점이 있습니다. MFCC를 이해하기 위해서 멜-스펙토그램과 캡스트럼 분석에 대해서 짚고 가겠습니다.\n3.1. Mel-Spectogram \u0026amp; Mel-scale MFCC를 설명하기 전에 멜-스펙토그램(Mel-Spectogram)에 대해서 간단히 알아보겠습니다. 멜스펙토그램이란, 사람의 달팽이관의 특성을 반영한 Mel-scale을 적용한 스펙토그램 표현법입니다. 달팽이관은 저주파 대역을 감지하는 구간이 조밀하고, 고주파 대역을 감지하는 구간은 넓게 이루어져 있습니다. 따라서 저주파 대역에 의미 있는 정보가 집중되어 있으며, 인간의 청각은 저주파 대역에서 더 민감하게 반응을 한다는 점을 반영하여, 주파수의 대역에 차등적으로 중요도를 적용하는 Mel-scale이 제안 되었습니다.\n$$ \\text{Mel}(f) = 2595 \\log_{10} \\left(1+ \\frac{f}{700}\\right) $$\n3.2. Cepstral Analysis 캡스트럼 분석(Cepstral Analysis)이란, 스펙토그램으로부터 소리의 대표적인 특징을 추출하는 기법입니다. 소리가 큰 진폭으로 진동할 때, 이를 공명(resonance)라고 합니다. 예를 들어 성대가 떨림으로 인해서 어떠한 소리가 최초로 생성될 때, 이를 공명이라고 하죠. 스펙트럼에서 공명은 뾰족한 봉우리(peak)로 표현이 되고, 이 봉우리들을 포먼트(formant)라고 합니다. 우리가 듣는 소리의 대표적인 특징이 스펙토그램의 포먼트로 표현된다는 것이죠. 따라서 캡스트럼 분석은 아래의 그림처럼 스펙토그램으로부터 포먼트를 찾아서 분리하는 기법입니다. (Spectral 에서 spec을 뒤집으면 Cepstral 이 되는 것은 우연일까요?)\n캡스트럼은 스펙트럼에 역푸리에변환(IFT: Inverse Fourier Transform)을 취하여 구합니다.\n   Cepstral Analysis (https://brightwon.tistory.com/11)   3.3. MFCC 구하기 MFCC를 구하는 과정은 다음과 같습니다.\n  주어진 신호(오디오 데이터)를 일정한 간격(window)로 나누어 푸리에변환을 적용하여 스펙토그램을 구한다. 스펙토그램의 제곱(파워 스펙토그램)에 Mel-scale을 기반으로 필터뱅크(Filter banks)를 구성하고 필터뱅크에 로그 변환을 수행한다. 이렇게 하면 Mel-scale에 따라 나누어진 구간 별로 분포한 정보를 확보한다. 주어진 Log Mel-spectogram에 역푸리에 변환을 적용하여 Cepstrum을 구한다. MFCC는 이렇게 구한 결과의 진폭(amplitude)이다.   3.4. MFCC의 의미 MFCC를 구하기 위해 적용된 기법과 과정에 대해 알아보니 머리가 조금 아프네요(\u0026hellip;) 하지만 그 의미를 되새겨보자면 다음과 같이 요약할 수 있을 것 같습니다.\n MFCC는 무수히 많은 단일주파수로 복잡하게 구성된 소리로부터 (1)사람의 청각이 예민하게 반응하는 정보를 강조하고 (2)소리의 대표적인 부분만을 취사 선택한 피처(feature)이다.\n \u0026amp;nbsp\n4. 세줄 \u0026hellip; 아니 쓰다 보니 네줄 요약  소리는 무수히 많은 단일주파수의 합산으로 이루어져있다. 푸리에변환을 소리가 어느 주파수로 이루어져 있는지 분해할 수 있다. 푸리에변환만 수행하면 시간의 정보가 유실되므로 STFT를 적용한다. MFCC는 소리를 분석하는데 많이 사용되는 피쳐로, 인간의 청각에 유의미한 주파수대역을 강조하고, 소리의 대표적인 특징을 추출한 값이다.   이번 포스트에서는 푸리에변환과 MFCC의 개념에 대해서 간단히 알아보았습니다. 다음에는 파이썬으로 직접 소리 데이터를 읽고 처리하는 과정에 대해 정리해보겠습니다.\n\u0026amp;nbsp\n5. Reference  https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%AA%85 https://en.wikipedia.org/wiki/Mel-frequency_cepstrum#cite_note-2 https://en.wikipedia.org/wiki/Mel_scale https://en.wikipedia.org/wiki/Cepstrum https://brightwon.tistory.com/11 https://ratsgo.github.io/speechbook/docs/fe/mfcc https://tech.kakaoenterprise.com/66  ","permalink":"https://lucaseo.github.io/posts/2020-12-26-understanding-audio-data-techniques/","summary":"이번 포스트에서는 소리의 파형을 분석하기 위해 사용되는 기법인 푸리에 변환과 특징 추출값으로 사용되는 MFCC의 개념에 대해서 알아보겠습니다.\n1. 소리는 주파수의 합산    Piano in Waveform   \u0026amp;nbsp\n위의 이미지는 실제 피아노 소리 파일을 파형(waveform) 형태로 시각화 한 것입니다. 간단한 피아노 소리이지만 매우 복잡한 파형을 그리고 있는 것을 볼 수 있는데요. 사실 우리가 흔히 들을 수 있는 이러한 \u0026ldquo;소리\u0026quot;라는 것은 각기 다른 단일 주파수를 가진 무수히 많은 정현파(sinewave)가 합산되어 형성된 것입니다.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(2) - Fourier Transform, MFCC"},{"content":"1. 소리 데이터란 소리는 다음 과정에서 생산된 것을 의미합니다.\n (1) 어떠한 물체 또는 매질(object)의 진동(vibration)으로 인해 공기 입자들이 밀고 당겨지는 반복적인 과정(oscilation)에서 생긴 파동(wave) (2) 공기의 압력이 낮아지면 빈 공간이 생기면서 다른 입자들로 채워지고, 압력이 높아지만 입자들을 밀어내는, 밀고 당기는 반복적인 연쇄 작용(oscillation)으로 인해 생기는 파동(wave)  그리고 위에서 정의한 파동은 아래와 같은 파형(waveform)으로 나타낼 수 있습니다.\n   \u0026amp;nbsp\n2. 소리 데이터의 표현 2.1. 파형의 요소 파형을 통해서 우리는 다음과 같은 정보를 파악할 수 있습니다. 주로 파동을 표현하는 요소들이죠.\n    시간(time): 파동이 진행되는 시간. 주로 X축입니다. 진폭(amplitude): 진동의 중심에서 최대까지의 거리를 나타냅니다. 진동의 크기를 의미하고, $A$라고 나타냅니다. 주기(period): 1회 진동하는데 걸리는 시간입니다. \\(T\\) 라고 나타냅니다. 진동수(또는 주파수. frequency): 한 점이 1초동안 진동한 횟수. \\(f\\) 라고 나타냅니다.  주기와 진동수는 서로 역수인 관계를 보입니다. (\\(f = \\frac{1}{T}\\) 또는 \\(T = \\frac{1}{f}\\)) 따라서, 주기가 길수록 진동수는 작고 주기가 짧을 수록 진동수는 큰 관계를 보입니다.    \u0026amp;nbsp\n2.2. 파형의 수학적 표현 파형은 사인함수를 통해 다음과 같이 수학적으로도 표현할 수 있습니다.\n\\(y(t) = A \\sin(2 \\pi f t + \\phi)\\)\n \\(A\\) : 진폭(amplitude) \\(sin\\) : 사인함수 \\(f\\) : 진동수(frequency) \\(t\\) : 시간(time) \\(\\phi\\) : 위상(phase). 위상이란 진동체의 상대적인 위치변화를 나타내는 부분입니다. 위상값에 따라 파형이 왼쪽으로 또는 오른쪽으로 이동(shift)된 형상인지를 알 수 있습니다.  \u0026amp;nbsp\n2.3. 진폭, 진동수의 관계 진동과 진폭이 소리와 무슨 관계가 있는지 아직 많이 낯선데요. 우선 진동 수와 음의 높낮이(pitch)의 관계에 대해 알아보겠습니다. 진동수가 클 수록 음이 높아집니다. 즉, 진동한 횟수가 많을 수록 높은 음이 구현된다고 이해할 수 있습니다. 반대로 진동한 횟수가 적다면, 그만큼 음의 높이가 낮은 저음이 구현됩니다. 아래와 같이 정리해볼 수 있습니다.\n frequency \\(\\leftrightarrow\\) pitch  longer periods(\\(T\\)) \\(\\rightarrow\\) lower frequency(\\(f\\)) \\(\\rightarrow\\) lower pitch shorter periods(\\(T\\)) \\(\\rightarrow\\) higher frequency(\\(f\\)) \\(\\rightarrow\\) higher pitch    이번에는 진폭과 소리의 크기(loudness)의 관계에 대해 알아보겠습니다. 진폭이 클 수록 소리는 크고, 진폭이 작을 수록 소리도 작아집니다. 아래와 같이 정리해볼 수 있습니다.\n amplitude \\(\\leftrightarrow\\) loudness  larger amplitude(\\(A\\)) \\(\\rightarrow\\) louder smaller amplitude(\\(A\\)) \\(\\rightarrow\\) quietter    \u0026amp;nbsp\n3. ADC (Analog digital conversion) 우리가 듣는 소리 그 자체, 즉 아날로그 소리는 연속적(continuous)입니다. 따라서 소리를 분석하기 위해서는 아날로그 소리를 디지털적인 형태로 변환하는 작업을 거쳐야 하는데 이를 Analog digital conversion(ADC)이라고 합니다. 말 그대로 아날로그를 디지털로 변환하는 작업이죠.\n3.1. Sampling \u0026amp; Quantization ADC에는 샘플링(Sampling)과 양자화(Quantization)이라는 두 가지 과정을 통해 연속적인 아날로그 소리를 이산적인 데이터로 변환합니다.\n Sampling: 샘플링은 시간의 흐름에 따라 진행되는 연속적인 신호를 특정 주기(time intervial)에 맞추어 신호를 이산적인 시간으로 쪼개는 과정입니다. Quantization: 양자화는 샘플링되어 저장된 데이터를 연속적이지 않은 대표값으로 정수화하여 이산적인 값으로 변환합니다.     Müller, Fundamentals of Music Processing, Springer 2015   \u0026amp;nbsp\n3.2. Sample rate \u0026amp; Bit depth 샘플링과 양자화는 적용되는 방법에 따라, 변환된 데이터의 품질에 영향을 끼칠 수 있습니다. 예를 들어 샘플링 주기나 양자화가 너무 크게 적용된다면, 기존의 아날로그 소리가 지닌 세세한 정보가 손실될 수도 있습니다. 이러한 품질을 결정하는 기준에는 샘플링주기(Samplig rate)과 비트뎁스(Bit depth)가 있습니다.\n   Bit Depth and Sample Rate https://youtu.be/-0rIU9FHiU0    Sampling rate: 1초 동안 담긴 샘플의 개수. 샘플링주기가 높을 수록 시간의 흐름에 따른 신호의 손실이 적습니다. Bit depth: 진폭을 쪼개는 개수. 비트(bit) 개수가 클 수록 기존의 신호에 유사한 형태로 데이터를 저장할 수 있습니다.     \u0026amp;nbsp\n4. 세줄 \u0026hellip; 아니 쓰다 보니 다섯줄 요약 우리가 귀로 듣는 소리에 대한 간단한 정의와, 소리를 분석하기 위해 디지털적인 정보로 변환하는 작업에 대해 알아보았습니다.\n 소리는 공기의 진동(vibration)과 반복적인 움직임(oscillation)으로 인해 압력이 변화하면서 생긴 파형이며 연속적인 아날로그 신호이다. 아날로그 소리를 분석하기 위해서는 ADC를 통해 디지털적이고 이산적인 데이터로 변환한다. 샘플링(sampling)은 아날로그 소리의 시간흐름을 특정 주기로 쪼개어 변환하는 작업이며, 샘플링주기(sampling rate)에 따라 1초 동안 기록되는 샘플의 개수를 달리할 수 있다. 양자화(quantization)는 아날로그 소리의 크기를 특정 구간으로 쪼개어 변환하는 작업이며, 비트뎁스(bit depth)에 따라 그 구간의 세밀함을 조절할 수 있다. 샘플링 주기와 비트뎁스는 아날로그 소리를 디지털 데이터로 변환되는 품질을 결정하며, 세밀할 수록 정보의 손실을 줄이고 기존의 소리에 유사한 데이터를 기록할 수 있다.   다음 포스트에서는 파형을 분석하기 위해 사용되는 푸리에 변환 (Fourier Transform)에 대해서 정리해보도록 하겠습니다.\n","permalink":"https://lucaseo.github.io/posts/2020-12-13-understanding-audio-data-sound-waveform-adc/","summary":"1. 소리 데이터란 소리는 다음 과정에서 생산된 것을 의미합니다.\n (1) 어떠한 물체 또는 매질(object)의 진동(vibration)으로 인해 공기 입자들이 밀고 당겨지는 반복적인 과정(oscilation)에서 생긴 파동(wave) (2) 공기의 압력이 낮아지면 빈 공간이 생기면서 다른 입자들로 채워지고, 압력이 높아지만 입자들을 밀어내는, 밀고 당기는 반복적인 연쇄 작용(oscillation)으로 인해 생기는 파동(wave)  그리고 위에서 정의한 파동은 아래와 같은 파형(waveform)으로 나타낼 수 있습니다.\n   \u0026amp;nbsp\n2. 소리 데이터의 표현 2.1. 파형의 요소 파형을 통해서 우리는 다음과 같은 정보를 파악할 수 있습니다.","title":"[KR] ML/DL을 위한 소리 데이터 이해하기(1) - Waveform, ADC"},{"content":"항상 내 맘 같지 않은 기술 블로그 개발이 비교적 익숙하지 않은 데이터분석가 또는 데이터사이언티스트가 Github Pages를 활용하여 기술블로그를 운영하기 위해서는 몇 가지 난관이 있습니다. 자료를 찾아보면 주로 Jekyll, Hugo, Hexo, Gatsby.JS 와 같이 낯설고 어려운 프레임워크을 사용해야 합니다. 튜토리얼은 간신히 따라갔다 하더라도, 기존의 테마를 내 입맛에 맞게 커스터마이징을 하거나 기능을 추가하기 위해서는 html, CSS 또는 NodeJS 같은 프레임워크를 알아야 합니다. 마음에 드는 테마가 있어도, 오랜 기간 관리가 되지 않아 Latex 엔진이 제대로 작동하지 않는 경우가 생기기도 하죠. 번거로웠던 적이 많았습니다. 그냥 조용히 다른 쉬운 플랫폼을 선택할 걸 하는 후회가 드는 순간도 있었습니다. 진짜 많은거 바라지 않으니, 가장 심플하게 기본만 딱 하는 테마는 없을까 찾아보기도 했구요. 이러한 와중에 fastpages를 접하게 되었습니다.\n 이번에는 fastpages    Source: https://github.com/fastai/fastpages   장점 fast.ai 에서 개발한 fastpages는 Jekyll을 기반으로 하는 툴입니다. 제가 생각하는 fastpages의 장점은 다음과 같습니다.\n 디자인이 군더더기 없이 깔끔하다. 부족한 것도 없지만 더 덧붙일 것도 없다. 따라서 코드를 수정할 필요가 없다.  Latex, Syntax highlighting 모두 깔끔하다. Altair, Plotly 와 같은 interactive visualization도 embedd 할 수 있다.   html을 쓰지 않아도 된다.  기존의 방식과 같이 markdown(.md) 파일로 블로그 포스트를 작성할 수 있다. jupyter notebook(.ipynb) 파일까지도 그대로 블로그 포스트로 변환해준다. (!!!!!) 아마 많이 사용할 일은 없겠지만, word 파일까지도 가능하다고 한다.   다른 프레임워크 기반 명령어가 따로 필요 없이, 그저 add, commit, push 명령어만으로 포스트를 간편하게 업데이트할 수 있다. 나머지는 Github Action을 배포과정 알아서 진행하기 때문.  단점 단점은 \u0026hellip; 아직까지는 없습니다. 단지, {github username}.github.io 형태의 주소는 지원하지 않고, {github username}.github.io/{user가 생성한 fastpages 블로그 레포지토리 이름} 을 통해서만 블로그가 생성된다는 점이 한계인 것 같네요.\n 한번 만들어봅시다 fastpages 블로그용 레포지토리 생성하기.  fastpages 공식 레포지토리에서 Use this template 버튼을 클릭하고 새로운 레포지토리를 생성합니다. 이때, 레포의 이름은 아무것이나 정해되 상관 없되, {계정명}.github.io는 피해야 합니다.  Generate a copy of this repo by clicking on this link. Name your repo anything you like except {your-username}.github.io.\n         \u0026amp;nbsp\n 잠시 기다리면, \u0026ldquo;Initial Setup\u0026quot;이라는 새로운 Pull Request가 자동으로 생성된 것을 확인할 수 있습니다.  GitHub Actions will automatically open a PR on your new repository ~ 30 seconds after the copy is created. Follow the instructions in that PR to continue.\n      \u0026amp;nbsp\nPR 가이드 따라가기  다음 링크에서 prive key와 public 키를 생성합니다. 이때, 옵션은 RSA와 4069를 선택 후 \u0026ldquo;Generate SSH-Keys\u0026rdquo; 버튼을 클릭합니다.  Create an ssh key-pair. Open this utility. Select: RSA and 4096 and leave Passphrase blank. Click the blue button Generate-SSH-Keys.\n      \u0026amp;nbsp\n 두번째 주어진 링크에서 새 \u0026ldquo;New repository secret\u0026rdquo; 버튼을 클릭하고, Value 입력 칸에 앞서 생성한 Private key 전체를 복사하여 붙여넣습니다. Name 입력 칸에는 \u0026ldquo;SSH_DEPLOY_KEY\u0026quot;라고 입력하고 저장합니다.  Navigate to this link and click New repository secret. Copy and paste the Private Key into the Value field. This includes the \u0026ldquo;\u0026mdash;BEGIN RSA PRIVATE KEY\u0026mdash;\u0026rdquo; and \u0026ldquo;\u0026ndash;END RSA PRIVATE KEY\u0026mdash;\u0026rdquo; portions. In the Name field, name the secret SSH_DEPLOY_KEY.\n   \u0026amp;nbsp\n  세번째 링크에서는 \u0026ldquo;Add deploy key\u0026rdquo; 버튼을 클릭하고, 앞서 생성한 Public key를 복사하여 붙여넣습니다. 이름은 아무렇게나 지정해도 된다고 합니다. 그리고 제일 아래 \u0026ldquo;Allow write access\u0026rdquo; 박스를 꼭 체크합니다.\n Navigate to this link and click the Add deploy key button. Paste your Public Key from step 1 into the Key box. In the Title, name the key anything you want, for example fastpages-key. Finally, make sure you click the checkbox next to Allow write access (pictured below), and click Add key to save the key.\n      \u0026amp;nbsp\n  마지막으로 PR을 merge합니다. 이후 fastpages 블로그가 배포되는 과정은 Github actions에서 아래와 같이 확인할 수 있습니다.         \u0026amp;nbsp\n 완료되면, {github username}.github.io/{레포지토리 이름} 에서 fastpages 기반의 블로그가 배포되었음을 확인할 수 있습니다.     \u0026amp;nbsp\n이것만 알아도 블로깅 문제 없다! (나머지는 저도 아직 잘 몰라요. 몰라도 크게 상관 없더라구요 \u0026hellip;)\n _config.yml  블로그의 이름, Latex사용 여부, 미리보기 여부, 태그 보여주기 여부 등을 설정할 수 있는 파일입니다. 자신의 SNS계정 버튼도 추가할 수 있고, description, pagination의 갯수 등의 사항들도 설정할 수 있습니다.   index.html  블로그 메인 페이지에 보여지는 컨텐츠를 작성하는 파일입니다. 저는 아무것도 입력하지 않았기에 디폴트로 작성되어 있던 fastpages 소개 텍스트를 삭제했습니다.   _pages/about.md  블로그의 자기소개 페이지 About페이지의 내용을 작성하는 파일입니다. markdown 포맷으로 작성하면 됩니다.   포스트를 저장하는 디렉토리는 아래 종류에 따라 달라지므로 디렉토리를 구분해서 작성하시면 됩니다.  _notebooks/ _posts/ _word/ images/    주의! 포스트를 작성할 때는 notebook, markdown, word 파일 포맷에 상관 없이 무조건 YYYY-mm-dd-{아무 이름} 형태로 작성되어야 fastpages가 이를 인식하고 파일을 html로 변환합니다.\n\u0026amp;nbsp\n블로그 로컬에서 확인하기 배포 또는 포스트 최종 업로드 전, 내가 작성한 포스트가 블로그에 잘 보여지는 지 확인하고 싶을 때가 있습니다. 이때는 make server 명령어를 칩니다. localhost:4000 에서 블로그가 생성되었음을 확인할 수 있습니다. (단, docker가 작동 중에 있을 때만 가능합니다.)\n    마무리 하며 기술 블로그를 초반에 시작했을 때는 정말 의욕은 많이 앞섰지만, 늘 \u0026ldquo;이 디자인은 마음에 들지 않아, 이건 이런 기능이 없어, 이건 계속 오류가 생겨\u0026quot;와 같은 쓸데 없는 생각들이 불쑥불쑥 튀어나왔던 것 같습니다. 하지만, 이번 기회에 fastpages가 아주 깔끔하면서도 만족스러워 다짜고짜 블로그를 바꿔버렸습니다. 이제 과거에 작성했던 글들도 슬슬 옮겨놔야할 것 같네요! 새로운 술은 새 포대에 담으라고 했던가요? 새로운 블로그를 만들었으니 글도 술술 써지기를 기대해봅니다!\n Reference  https://github.com/fastai/fastpages  ","permalink":"https://lucaseo.github.io/posts/2020-11-29-fastpages/","summary":"항상 내 맘 같지 않은 기술 블로그 개발이 비교적 익숙하지 않은 데이터분석가 또는 데이터사이언티스트가 Github Pages를 활용하여 기술블로그를 운영하기 위해서는 몇 가지 난관이 있습니다. 자료를 찾아보면 주로 Jekyll, Hugo, Hexo, Gatsby.JS 와 같이 낯설고 어려운 프레임워크을 사용해야 합니다. 튜토리얼은 간신히 따라갔다 하더라도, 기존의 테마를 내 입맛에 맞게 커스터마이징을 하거나 기능을 추가하기 위해서는 html, CSS 또는 NodeJS 같은 프레임워크를 알아야 합니다. 마음에 드는 테마가 있어도, 오랜 기간 관리가 되지 않아 Latex 엔진이 제대로 작동하지 않는 경우가 생기기도 하죠.","title":"[KR] fastpages에서 블로그 시작하기"},{"content":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp\n점검해보자 1. 회사일만 하지 않기 모호하지만, 매우 중요한 항목이다. 작년에는 회사에서 기존에 하던 일과 말도 안 되는 정부과제, 그리고 갑작스러운 혁신금융선정의 삼위일체가 합심하여 나에게 전신마사지를 해주는 바람에 야근과 주말출근을 밥 먹듯이 했다. 결국 기존에 받는 연차 15일과 야근, 주말 근무에 대한 대체휴무가 쌓여, 연말에는 잔여 연차가 23일이 되었다(??) 1년 간 쉰 기억이 나지 않고, 기존 연차보다 잔여연차가 더 많은 상태로 1년을 마무리 하게 되어, 이게 대체 누구를 위한 무슨 짓인가 싶기도 했다. (모든 게 힘듦으로 가득하던 시절 그래서 슬기롭게 내 생활을 챙기기로 했다. 그렇다고 아주 여유롭진 않지만, 너무 힘들지는 않을 만큼, 어떻게 보면 좀 노하우가 생긴 것 같은 삶을 살고 있다.\n\u0026amp;nbsp\n2. 데이터 사이언스 대회 도전   데이콘: 원자력 발전소 상태 판단 알고리즘 대회. 데이콘에서 주관하는 원자력 발전소 상태 판단 대회에 참가했다. 자세한 후기는 이미 블로그에서도 작성한 바 있다. 대회에서 중간에 포기하지 않고 꾸준히 참여했다는 점에서 소기의 목적을 달성했다고 생각하고, 좀 더 열심히 했다면 더 좋은 성적을 거둘 수 있었을 것이라 생각한다.\n  카카오 아레나: Melon Playlist Continuation. 사실 카카오 아레나의 멜론 플레이리스트 추천 대회는 정말 많은 관심을 가지고 있는 분야인 만큼 의욕이 매우 높았던 대회였다. 추천시스템에 대한 개념도 잘 없었기 때문에, 참가와 동시에 추천시스템에 대한 공부를 차근차근 하고자 했다. 하지만 참가만 걸어 놓은 상태로 그 이상의 제출은 해보지 못 하고 대회가 마무리 되어 많은 아쉬움이 남는다. 애초에 제출 이전부터 선행되어야 하는 추천시스템에 대한 공부가 지지부진 했기 때문에, 내가 생각하기에도 변명의 여지가 없는 좀 한심한 결과가 나오게 됐다.\n  \u0026amp;nbsp\n3. 간단한 웹어플리케이션 구축 연초부터 사이드 프로젝트로 여러가지 역할을 수행할 수 있는 유연한 웹어플리케이션을 구축하고자 하는 와중에 Streamlit을 경험하고, 이것저것 시도해보게 되었다. 글또에서도 관련된 포스팅을 작성한 바 있다. 프론트엔드에 대한 걱정 없이 순수 파이썬으로 매우 간편하게 개발할 수 있기 때문에, 실제로 직장 업무에서도 개발팀이 참고할 수 있도록 데이터 QC, 레이블링 기능에 대한 프로토타입이나 대시보드 등을 만들어서 사내 공유하거나 함께 사용하고 있다.\n\u0026amp;nbsp\n4. 독일 가족 방문하기 마지막으로 독일을 방문한건 2018년. 올해만큼은 꼭 시간을 내서 누나의 가족들이 사는 독일을 방문하고자 했지만 망할 COVID-19 때문에 패스하게 되었다. 내가 가야 그나마 조카들이 한국말을 덜 까먹는데, 이렇게 조카들의 한국말은 더욱 뒷걸음질 치는건가 싶다. 언제쯤 가능하게 될까. 이건 기약이 없어서 더욱 안타까운 항목이다.\n\u0026amp;nbsp\n5. 글또 솔직히 말하자면, 글또 생활은 생각보다 어렵다. 글을 잘 쓰는 것도 어렵고, 내가 잘 모르는 내용에 대해 정독하고 피드백이나 감상을 남기는 것도 쉽지는 않다. 모자라서 그런건 아니고 아마 처음해봐서 그럴 수도 있다. 그래서 더더욱 차감 없는 슬기로운 글또 생활을 영위하고자 노력하고 있다. 꾸역꾸역 \u0026hellip; 이라는 의성어는 좀 뉘앙스가 그렇지만, 어쨌든 그런 비슷한 상황이다. 나쁘게 말하면 꾸역꾸역이고 좋게 말하면 조금조금씩 발전을 꾀하는 존버라고 할 수 있겠다. 아무튼 현재까지는 예치금 차감은 없다. 요태까지 그래와고 아패로도 개속.\n\u0026amp;nbsp\n(+) 지난 3월 방송통신대 정보통계학과 3학년에 편입했다. 해외 대학원을 알아보던 중, 비전공자인 나는 데이터사이언스와 직접적인 연관이 있는 학과목의 수강학점이 없기 때문에 지원부터가 제한이 있다는 걸 알게 되어서, 이 부분을 타개해나갈 방법을 찾고 있었다. 경영과 연계된 MIS관련 대학원, 부분 수강, MOOC를 통한 크레딧 등 다양한 방법을 알아봤지만, 역시 편법은 없었고, 3학년으로 편입하여 정식 대학과목을 수강하는 것이 가장 확실했다. 직장생활과 병행하는 방법은 방송통신대 뿐이었는데, 한 학기 등록금이 30만원대로 매우 저렴했기 때문에, 부담 없이 경험해보는 셈 치고 편입을 결정했다.\n방송통신대는 대학교 특성상 COVID-19로 인한 타격도 1도 없이 매우 정상적으로 학사일정이 진행되었다. (1은 있었다고 볼 수도 있다. 기말고사만큼은 오프라인으로 이루어지는데, 이 조차 과제물로 전환되었기 때문.)\n계획에 없었던 편입이고 주변 사람들도 우려를 많이 표했지만, 직장생활 병행과 대학원 진학을 위한 장기 계획의 일부분인 만큼, 잃은 것보다는 얻은 것이 많은 한 학기였다. 방송통신대에 대한 자세한 후기는 이번 2학기가 끝난 이후 포스팅할 계획이다.\n\u0026amp;nbsp\n마무리 하며 난 나름 뭔가 많이 했다고 생각했는데, 막상 돌아보니 매우 소박했다는 걸 느꼈다. 동시에 남은 5개월과 2021년에 대한 구상도 할 수 있게 되어 의미있는 회고였다. 역시 글또를 하지 않았다면 이런 글도 남기지 않았을 거란 생각이 들기도 한다. 남은 5개월은 또 얼마나 빨리 지나갈까. 적절한 타이밍에 남은 연차를 마저 소진하고 (잘 휴식하겠다는 뜻) 글또도 잘 마무리 할 수 있는 (예치금 차감 당하지 않겠다는 뜻) , 그리고 방송통신대 두번 째 학기도 초심 잃지 않고 수강할 수 있는 (A+를 받겠다는 뜻) 하반기가 되었으면 한다.\n","permalink":"https://lucaseo.github.io/posts/2020-08-02-review-2020-1st-half/","summary":"한 게 뭐 있다고 벌써 8월이야 시간은 정말 경이로울 정도록 빨리 간다. 사실 상반기에 대한 회고글도 6월이 지난 직후 작성했어야 했는데, 순식간에 8월이 되어, 더 늦기 전에 작성해야겠다는 생각이 들었다. 한살 한살 더 먹어갈 수록 \u0026ldquo;주춤\u0026quot;하면 시간은 이미 지나있더라\u0026hellip; 이번 6개월은 잘 살았는지 잘 모르겠다. 한번 알아보자.\n\u0026amp;nbsp\n누구나 그럴싸한 계획은 있다. OOO 전까지는 난 이런 일을 하겠노라 생각했던 시기가 있었다.\n 회사일만 하지 않기 데이터 사이언스 대회 참가하기 내 기술스택에 간단한 웹어플리케이션 추가하기 독일 가족 방문하기 예치금 차감 없는 글또 생활하기  \u0026amp;nbsp","title":"[KR] 2020년 상반기가 지났다"},{"content":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)\n대회 개요    수력 원자력 공사인데 배경은 풍력 발전 ... ?   \u0026amp;nbsp\n원자력발전소 상태 판단 대회는 한국수력원자력(주)에서 제공한 발전소 모의 운전 데이터를 통해 원자력 발전소의 상태를 판단하는 것이 태스크로 주어진다.\n평가지표 평가 지표는 Log loss 이다. Log loss 값은 0 ~ 1 사이로 산출되며, 낮고 0에 가까울 수록 모델의 예측력이 좋음을 의미한다. (데이콘 측 평가지표 설명 영상)\n$$ \\text{logloss}(\\cdot) = \\frac{-1}{N}\\sum_i^N \\sum_j^M y_{ij} \\log{p_{ij}} $$\n데이터셋 원자력 발전소 모의 데이터는 기본적으로 828개의 발전소 운전 Train 데이터 파일과 각 파일에 부여된 198가지 상태 레이블이 매핑된 Label 파일이 주어진다. 압축을 해제하면 총 81GB에 달했다.\n![]({{ site.baseurl }}/images/2020-07-05-review-dacon-nuclear-competition/2.png)\n   \u0026amp;nbsp\n각각의 파일에 저장된 Train 데이터셋 위의 그림과 같으며, 발전소가 10분 동안 작동한, 즉 1행 당 1초 즉, 600행으로 이루어진 데이터가 주어진다. 모든 데이터셋은 발전소의 상태가 변하기 전 디폴트 상태_A(999) 와 상태가 변한 후 상태_B 데이터를 담고 있으며, 상태_A에서부터 시작된다. 데이터는 0초에서 15초 사이에서 상태가 변하기 시작한다. 따라서 데이터 상태의 변화가 0초에서 발생한다는 말은 상태_A가 없는 좌측의 데이터셋과 같고, 그 이외에는 우측과 같다고 보면 된다.\n   \u0026amp;nbsp\n다만 위의 그림과 같이, 실제로는 몇 초부터 상태가 변하는지에 대한 정보가 주어지지 않기 때문에, 각 데이터셋이 좌측과 같은지 우측과 같은지는 한 눈에 판단이 어렵다.\n데이터 전처리 EDA 과정에서 의미 있는 인사이트를 도출해내지 못 했다. 그리고 대회 경험이 적고 시간이 촉박했던 관계로, 바로 전처리에 돌입했다.\n Label이 999 인 경우 제외한다. 10분 간의 운전 데이터 기록 컬럼 내 unique한 값이 \u0026lt; 10 인 컬럼은 제외한다. 데이터셋 중 str타입의 데이터가 발생할 경우 NaN 치환한다. 마지막으로 NaN 데이터는 0으로 채운다. Train / Eval 데이터셋은 3:1의 비율로 분리한다.  #1 train = train[train[\u0026#39;label\u0026#39;]!=999].reset_index(drop=True) train_label = train.label train = train.drop([\u0026#39;id\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;label\u0026#39;], axis=1) #2 with open(\u0026#39;filter_col.txt\u0026#39;, \u0026#39;r\u0026#39;) as filehandle: list_ = filehandle.readlines() list_ = [col.replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) for col in list_] train= train[list_] # 3 for col in train.columns: if train[col].dtype != \u0026#39;float64\u0026#39;: train[col] = pd.to_numeric(train[col], errors=\u0026#39;coerce\u0026#39;) #4 train = train.fillna(value=0, axis=1) #5 X_train, X_valid = train_test_split(train, test_size = .25, random_state=42) y_train, y_valid = train_test_split(train_label, test_size = .25, random_state=42) 모델 모델은 LightGBM을 선택했다. Tabular dataset의 분류 문제에는 나무모형 기반의 모델이 가장 적합했고, LightGBM, XGBoost, Random Forest를 초기 테스트 한 결과 LightGBM이 가장 빠르고 성능이 좋았기 때문이다.\n부스팅 방법으로는 가장 기본적인 Gradient Boosted Decision Tree를 선택했다. 또한 데이터셋이 커서 연산이 큰 문제와 오버피팅을 방지하기 위해 bagging_fraction, feature_fraction 파라미터의 설정으로 데이터셋의 행과 열을 0.5 비율로 고정하여 학습 중 샘플링할 수 있도록 했다.\n그 외 objective, num_class, metric과 같이 대회의 목적에 맞게 변경한 파라미터를 제외하고는 default를 가져온 것들이 대부분이다.\nimport lightgbm as lgb X_train = X_train.to_numpy() X_valid = X_valid.to_numpy() y_train = y_train.to_numpy() y_valid = y_valid.to_numpy() train_data = lgb.Dataset(X_train, label=y_train) #, feature_name=X_train.columns) valid_data = lgb.Dataset(X_valid, label=y_valid) #, feature_name=X_valid.columns) param = { \u0026#39;objective\u0026#39;: \u0026#39;multiclass\u0026#39;, \u0026#39;num_class\u0026#39;: 198, \u0026#39;boosting\u0026#39;:\u0026#39;gbdt\u0026#39;, \u0026#39;num_leaves\u0026#39;:32, \u0026#39;max_depth\u0026#39;:20, \u0026#39;min_data_in_leaf\u0026#39;:20, \u0026#39;metric\u0026#39;:\u0026#39;multi_logloss\u0026#39;, \u0026#39;learning_rate\u0026#39; : 0.01, \u0026#39;verbose\u0026#39; : -1, \u0026#39;bagging_freq\u0026#39; : 1, \u0026#39;bagging_fraction\u0026#39; : 0.5, \u0026#39;feature_fraction\u0026#39; : 0.5, } evals_result={} num_round = 2000 lgbst = lgb.train(params=param, train_set=train_data, num_boost_round=num_round, valid_sets=[valid_data], evals_result=evals_result, early_stopping_rounds=1000, verbose_eval=10) lgbst.save_model(\u0026#39;model_lgb.txt\u0026#39;, num_iteration=lgbst.best_iteration) 결과 결과는 가채점 기준 36위 / 201팀, 최종 데이터셋 기준 채점 및 중복 및 부정 제출 등의 여부가 판결 뒤 산출된 최종 순위는 16위 / 187팀를 기록했다. 전체 참가팀만 놓고 보면 1091팀이지만, 실제로 제출한 팀은 20%에 그친 것을 확인할 수 있었다.\n   소감 무엇보다 아쉬운점은 초반의 의욕과는 달리, 데이터사이언스 대회의 best practice를 실습해보지 못 했고, 제출과 점수에 급급한 채로 마무리 했다는 점이다. 시간 부족과 의사소통 부재로 EDA 깊이 있게 하지 못 했고, Hyperparameter tuning과 grid search를 제대로 시행하지 못 햇다.\n마지막으로, 이번 원자력발전소 상태 판단 대회 참가는 데이터사이언스 관련 대회 경험을 쌓기 위해서였지만, 한편으로는 전혀 접하지 못 했던 원자력발전소 관련 데이터를 접하고 발전소 관련 도메인을 조금이나마 얻기 위함이기도 했다. 그러나 데이터셋의 컬럼은 모두 비식별 처리가 되어 있어 무엇을 의미하는지 데이터의 특성과 정보에 대한 접근이 불가했다는 점이 대회를 참가하는 와중에 흥미가 조금 깎이게 된 요인이 되지 않았나 하는 생각이 든다.\n어쨌거나 저쨌거나 완주를 했고, 상위 10% 내 라는 기대도 하지 않았던 성적으로 마무리를 했기 때문에 여기에 의의를 두며, 다음에 참가하는 데이터사이언스 대회는 이번에 아쉬웠던 점들이 꼭 보완될 수 있도록 다짐을 해본다.\n","permalink":"https://lucaseo.github.io/posts/2020-07-05-review-dacon-nuclear-competition/","summary":"연초, 의욕으로 가득하던 시기에 지인의 권유로 데이콘(Dacon.io)에서 주관하고 한국수력원자력에서 주최한 원자력발전소 상태판단 경진대회에 참가하게 되었다. 지인의 지인도 합류하여 팀은 총 3인으로 구성되었다. 하지만 개개인의 일정과 생업으로 인해서 진행은 각자 하되, 진행사항이나 인사이트 등은 수시로 공유하고, 제출은 팀의 이름으로 제출하는 형식으로 진행되었다. (초기의 으쌰으쌰하던 분위기와 달리 흐지부지된 감이 없잖아 있었다. 팀당 제출횟수가 하루 3회로 제한되었기 때문에, 이럴 줄 알았으면 애초에 각자의 이름으로 혼자 해도 됐겠다 싶기도 했다. 하지만 결과론적인 총평이기 때문에 패스)","title":"[KR] 데이콘 원자력발전소 상태 판단 대회 후기"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다. (영상)\n원본의 Figure. The Wagon Wheel Effect에서 하단의 sampling rate 슬라이더를 조절하며 왼쪽의 시계방향으로 회전하는 물체와, 오른쪽의 sampling rate에 따른 스냅샷(혹은 샘플)를 관찰해보자\n   \u0026amp;nbsp\n직관적으로 봤을 때, 왼쪽과 마찬가지로 오른쪽 스냅샷도 시계방향으로 회전하는 듯한 모션을 취하기 위해서는 최소한 왼쪽에서 한번 회전하는 동안 2번 이상의 스냅샷을 찍어야 한다. 만약 sampling rate을 엄청 낮게 설정한다면 스냅샷은 기존의 회전 방향과는 반대로 반시계 방향으로 찍히게 된다.\n회전 방향이 결정되지 않고 정지해 있는 듯한 특이한 케이스도 있다. 만약 sampling rate을 한바퀴당 1번으로 설정한다면 스냅샷에는 움직임이 없을 것이고, sampling rate을 한바퀴당 2번으로 설정한다면 스냅샷은 그냥 앞뒤로만 움직이기 때문에 진행 방향을 유추할 수 없게 될 것이다.\nSine Wave Aliasing : Multiples of the sampling rate Sive wave에는 다음과 같은 법칙이 있다.\n sampling rate이 \\(SR\\) 헤르쯔와 정수 \\(K\\)가 주어졌을 때,\n\\(F\\) 라는 frequency를 지닌 sine wave가 있고, \\(F+(k * SR)\\) 의 진동수를 지닌 sine wave를 샘플링했다고 할 때, 둘은 서로 구별이 가능하지 않다.\n 예를 들어, Sampling rate이 6Hz라고 했을 때, 다음 두 그룹의 sive wave는 서로 구별이 가능하지 않다.\n 샘플링을 거친 1Hz의 sine wave 샘플링을 거친 다음 세 가지의 sine wave  \\( 1+(1 * 6) = 7Hz \\) \\( 1+ (2 * 6) = 13Hz \\) \\( 1+(3 * 6) = 19Hz \\)    (사실 여기까지는 조금 이해가 안 갔는데 \u0026hellip;)\n원본의 Figure. All Sampled Sine Waves Have Aliases을 보면 더욱 직관적이다. Figure 1.의 Time domain 그래프는 1Hz sine wave(파랑색)와, 그 외 sine wave(회색)을 비교하고 있다. 샘플링을 거치고 나면(Frequency domain 그래프), 샘플링이 표현하는 부분은 파랑색과 회색 다 일치하기 때문에, 샘플링을 한 뒤에는 각각 어떤 frequency 였는지 구분을 할 수 가 없게 된다.\n   \u0026amp;nbsp\n이 말인즉슨, 샘플링을 거친 sine wave는 무한대의 alias를 가진다는 것이다. 그저 기존의 frequency에 sampling rate의 배수만 더해주면 기존 frequency에 대한 새로운 alias가 형성된다. 따라서 이 법칙은, 어떠한 신호라도 샘플링을 거치면 다른 샘플링 신호와 구별이 가능하지 않는 상황이 올 수 있다는 것을 의미한다.\n","permalink":"https://lucaseo.github.io/posts/2020-06-20-dsp-basic-s01-9/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Wagon Wheel Effect 빠르게 회전하는 바퀴나 물체를 보면 처음에는 반시계 방향으로 회전하는가 싶더니, 어느 순간부터 반대로 시계방향으로 회전하는 것 같은 환영을 볼 수 있다. 아니면 분명히 바퀴는 빠르게 회전하는데, 느리게 회전하는 것처럼 보일 때도 있다. 바로 undersampling과 alias 로 인해 발생하는 현상인데, 명칭은 Wagon Wheel Effect(마차바퀴현상)라 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Wagon Wheel Effect \u0026 Aliasing"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.\n 특정 신호 내 가장 높은 frequency를 알고 있다면, 샘플링 주기를 그 두배로 설정하면 된다는 뜻이다. 예를 들어 300Hz sine wave 를 샘플링하려 한다면, 우리는 두배 이상 즉, 600Hz 를 샘플링해야 한다. 반대로 샘플링 주파수가 두배보다 작을 경우, 간섭이 일어나며 앞에서 언급한 aliasing(참고)이 발생하게 된다.\n신호 위주가 아닌 sampling rate 위주로 법칙을 정하면 다음과 같다.\n 샘플링 주기 FS hertz에 대하여, FS/2 혹은 그 이상인 frequency를 가진 신호는 적절하게 샘플링할 수 없다.\n 특히 sampling rate의 절반 즉, FS/2 라는 값은 Nyquist Limit 또는 Nyquist Frequency라고도 불리며, 샘플링 법칙을 논할 때 매우 중요하게 다뤄지는 값이다.\n원글의 Figure 1.(링크)의 Sampling period 값을 조절하면서 직관적으로 이해해보자. Sampling rate이 16Hz일 경우, FS/2는 8이다. 모든 네가지 sine wave의 경우 8을 넘지 않으므로 샘플링이 적절하게 가능하다.\n   \u0026amp;nbsp\n하지만 sampling rate을 4Hz으로 설정하면 FS/2의 값은 2이다. 따라서 2Hz, 3Hz, 4Hz의 sine wave는 제대로 샘플링 되지 않는다.\n   \u0026amp;nbsp\n지난 글에서 인간은 20Hz ~ 20,000Hz 사이의 소리만 들을 수 있다고 말한 바 있다. 따라서 샘플링 법칙에 따라 인간이 들을 수 있는 범위 내에서 음악의 sampling rate를 정하고자 하면, 20,000의 두배 즉 40,000Hz 이상이 되어야 한다. 오디오와 음악의 주파수가 40,000Hz 근처(정확하게 말하자면 44,100Hz)인 것은 이 이유 때문이다.\n\u0026amp;nbsp\nNyquist Frequency를 초과하는 경우 샘플링 법칙은 Nyquist Frequency(샘플링주기의 절반)에 한해서는 어떤 신호든 정확히 샘플링할 수 있고, 반대로 샘플링하고자 하는 샘플이 Nyquist frequency보다 높은 frequency를 가지고 있다면, 이 샘플링이 제대로 이루어지지 않는다는 것을 알게 되었다. 그렇다면 Nyquist Frequency를 초과하는 신호는 샘플링이 이루어지지 않는 이유에 대해서 알아보자.\n원글의 Figure 1.(링크)에서 sampling rate은 24Hz이고 Nyquist frequcny는 24Hz의 절반인 12Hz이다. 플레이 했을 때, 신호가 Nyquist frequency를 지나면, 신호의 샘플들은 하나의 파랑색의 파형 뿐 아니라 새롭게 생성된, 점점 감소하는 frequency 형태의 회색의 파형까지도 sampling 결과를 통해 표현이 가능하게 된다.\n   \u0026amp;nbsp\n이렇게 신호의 frequency를 제한하지 않고 Nyquist limit을 넘도록 허용하면, 샘플링된 신호는 Nyquist frequency가 투영된 새로운 sine wave를 표현하게 된다. 결국 실제로는 존재하지도 않는 회색 신호를 샘플링에 포함시키게 되는 결과를 낳게 되는 것이다.\n다음 포스팅에서 alias에 대한 다른 예시를 더 깊게 다루어서 정리를 할 예정이다.\n Additional material https://youtu.be/5wyYgy6LPyQ\n","permalink":"https://lucaseo.github.io/posts/2020-06-07-dsp-basic-s01-8/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Nyquist-Shannon Sampling Theorem 신호처리에서 Oversampling과 undersampling을 방지하고도, 여전히 신호를 잘 표현할 수 있는 sampling rate(샘플링주기)는 어떻게 선택할 수 있을까? 샘플링 주기는 주로 나이퀴스트-섀넌 샘플링 법칙(Nyquist-Shannon Sampling Theorum)를 따른다.\n이 샘플링 법칙은 다음과 같이 정의된다.\n 만일 어떠한 신호 그 어떤 frequency도 B hertz보다 높지 않다면, 1/(2B) 초 간격으로 샘플링을 하면 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 나이퀴스트 샘플링 법칙 (Nyquist Sampling Theorum)"},{"content":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.5 ~ 5점 사이로 평가를 내릴 수 있다. 넷플릭스에서 보유한 사용자 평가 데이터에는 MAP가 사용될 수 있지만, 왓챠에서는 MAP가 적절하지 않을 것으로 생각된다.\n반면에 nDCG(Normalized Discounted Cumulative Gain)의 경우 복수의 컨텐츠가 relevance를 가지고 있다고 하더라도, 그 정도(점수)에 따라 어떠한 컨텐츠가 더 관련있고, 덜 관련 있는지를 평가할 수 있다. 대부분의 검색과 추천시스템의 경우 다수의 사용자들은 1, 2페이지 또는 상위의 리스트만 참조할 것이기 때문에 상위 리스트 사이에서 변별력을 갖춰야 하는 경우 nDCG를 통한 평가가 설득력을 얻는다.\n\u0026amp;nbsp\nCumulative Gain (CG) nDCG에서는 우선 CG이란 개념이 등장한다. CG는 Cumulative Gain 이라는 이름에서도 알 수 있듯, 전체 추천된 리스트에 대하여 gain의 총 합을 구한 것이다.\n$$CP_{p} = \\sum_{i=1}^{p} rel_{i}$$\n \\(p\\) : 추천된 아이템 \\(rel_{i}\\) : i번 째 아이템의 relevance 정도. Gain 이라고 한다.  예시) 추천시스템의 결과와 Relevance\n   Rank Relevance     1 3   2 3   3 3   4 4   5 2   6 2    $$CG = 17$$\n\u0026amp;nbsp\nDiscounted Cumulateive Gain (DCG) DCG 에서는 각 추천된 아이템의 relevance를 log함수로 나누어 값을 구한다. log 함수로 나누어주는 부분은, 랭킹의 위치에 따른 페널티를 주는 효과를 가진다. 순위의 값이 클 수록(즉, 순위가 낮을 수록) DCG의 값은 작아진다. 하지만, 높은 순위의 경우 간격이 크고, 낮은 순위의 경우 실제 체감하는 차이는 낮다.\n$$DCG_{p} = \\sum_{i=1}^p \\frac{rel_i}{log_2(i+1)} = rel_1 + \\sum_{i=2}^p \\frac{rel_i}{log_2i}$$\n예시) 각 추천된 컨텐츠 당 Discounted Gain\n   Rank Relevance Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\)   2 3 \\(3 / \\log_2(2+1) = 1.89\\)   3 3 \\(3 / \\log_2(3+1) = 1.5\\)   4 4 \\(4 / \\log_2(4+1) = 1.72\\)   5 2 \\(2 / \\log_2(5+1) = 0.77\\)   6 2 \\(2 / \\log_2(6+1) = 0.71\\)    $$DCG = 9.59$$\n\u0026amp;nbsp\nNormalized DCG (nDCG) DCG는 현재 추천시스템이 추천한 결과에 대한 상태를 보여주는데, DCG만 놓고 볼 경우 추천된 아이템의 갯수에 따라 DCG가 다를 수 있으므로, 이를 0~1사이의 값으로 정규화 해줄 필요성이 있다. 따라서 현재 추천된 리스트의 결과에 기반한 DCG를 현재 추천된 결과의 가장 이상적인 형태를 가정했을 때의 DCG(ideal DCG, iDCG)로 나누어서 정규화한다.\n사용자가 컨텐츠를 평가하지 않은 경우와, 관련성이 아예 없는 경우 manual적으로 값을 0으로 설정하거나, 적절하게 imputation을 취해주어야 한다는 취약점이 있다. 하지만, nDCG는 relevance가 등급이나 범위로 매겨지거나(graded relevance), 이분법적인 경우(binary relevance) 둘 다 평가가 가능하다. 또한 log함수를 통해 순서에 대한 가중치가 주어지므로 추천시스템에 적용하기 매우 적절한 평가지표라고 할 수 있다.\n$$IDCG_p = \\sum_{i=1}^{|REL_p|} \\frac{2^{rel_i} - 1}{log_2(i+1)}$$\n$$nDCG_p = \\frac{DCG_p}{IDCG_p}$$\n예시) Ideal Relevance와 Discounted Gain\n   Rank Relevance Discounted Gain Ideal Relevance Ideal Discounted Gain     1 3 \\(3 / \\log_2(1+1) = 3\\) 4 4   2 3 \\(3 / \\log_2(2+1) = 1.89\\) 3 1.89   3 3 \\(3 / \\log_2(3+1) = 1.5\\) 3 1.5   4 4 \\(4 / \\log_2(4+1) = 1.72\\) 3 1.16   6 2 \\(2 / \\log_2(6+1) = 0.71\\) 2 0.71   5 2 \\(2 / \\log_2(5+1) = 0.77\\) 2 0.77    $$ nDCG_p = \\frac{DCG_p}{IDCG_p} = \\frac{9.59}{10.03} = 0.95 $$\nReference  위키피디아 MRR vs MAP vs NDCG: Rank-Aware Evaluation Metrics And When To Use Them  ","permalink":"https://lucaseo.github.io/posts/2020-05-10-normalized-discounted-cumulative-gain/","summary":"Precision, Recall 기반의 평가 방법의 한계 앞서 다루었던 MAP(Mean Average Precision)과 같은 추천시스템 평가 지표는 Precision, Recall을 기반으로 우선순위를 반영한 성능 평가 방법을 제시했다. MAP는 추천된 리스트 중 상위 K개에 대한 관련 여부가 명확하게 주어졌을 때 평가 지표로 사용될 수 있다. 하지만 관련(relevence) 여부가 명확하지 않거나, 관련 여부를 이분법으로 표현하지 않는 경우에는 적절하지 않다.\n당장 떠오르는 예로는 넷플릭스와 왓챠가 생각이 난다. 넷플릭스의 경우 사용자가 컨텐츠에 대해 [좋다 vs 안좋다]로 평가를 내릴 수 있지만, 왓챠의 경우에는 유자가 0.","title":"[KR] 추천시스템의 평가 지표 : nDCG"},{"content":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)\n추천 시스템을 통해 추천되는 아이템의 경우 추천의 정도가 동일하지 않다. 대부분의 추천 결과는 다음처럼 나올 수 있다고 생각해 볼 수 있다.\n 1순위 : 가장 관심을 가질만한 것.\n2순위 : 그 다음 차선책으로 관심을 가질만한 것.\n3순위 : 그 다음으로 사용자가 관심을 가질만한 것.\n4순위 : 또 그 다음 \u0026hellip; \u0026hellip;\n \u0026amp;nbsp\nMean Average Precision (MAP) 은 순서 또는 순위를 감안하는 부분을 반영하여 추천 시스템의 성능을 평가하는 지표로서, 과거 캐글의 Stander Product Recommendation, 카카오아레나의 브런치 사용자를 위한 글 추천 대회 등 추천 시스템 관련 컴퍼티션에서 채점 방식으로 적용되었다. 특히 분류 문제에서 흔히 언급되는 Precision과 Recall이 적용된 성능평가 방법으로, 아주 낯설지는 않다.\n\u0026amp;nbsp\nPrecision \u0026amp; Recall     Predict Positive Predict Negative     Actual Positive True Positive False Negative   Actual Negetave False Positive True Negative    MAP에 대한 개념는 Precision과 Recall에서부터 시작한다. 일반적으로 위와과 같이 confusion matrix가 있다고 할 때, Precision과 Recall은 다음과 같다. (더 자세한 설명은 링크를 참조하도록 하자)\n$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$$\n$$\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$$\n\u0026amp;nbsp\n추천시스템 관점에서의 Precision \u0026amp; Recall 추천시스템에서는 Precision과 Recall을 다음과 같이 해석할 수 있다. 추천시스템에서는 분자 부분을 relevant(관련있는) 라고 표현하기도 한다.\n     Precision 또는 \\(P\\):\n 추천한 아이템 중, 실제로 사용자의 관심사와 겹치는 아이템의 비율 \\(\\text{Precision} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{Total items from recommendation}}\\)    Recall 또는 \\(r\\):\n 실제로 사용자가 관심을 가진 아이템 중, 추천된 아이템이 겹치는 비율 \\(\\text{Recall} = \\frac{\\text{Items from recommendation that fit user\u0026rsquo;s interest}}{\\text{User\u0026rsquo;s interest}}\\)    \u0026amp;nbsp\nCutoff (@K) MAP에서는 Cutoff의 개념이 등장한다. Cutoff는 \u0026ldquo;잘라낸다\u0026quot;는 뜻으로, 쉽게 말하면 \u0026ldquo;상위 K개만 고려하고 그 아래로는 쳐내기\u0026rdquo; 라고 이해하면 된다. Cutoff를 가질 경우에는, @K 를 덧붙여서 표기한다.\n어떠한 사용자의 기록을 통해서 자동차 용품와 관련된 아이템을 추천한 결과가 다음과 같다고 하자.\n   순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 운전자 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    다음 예시의 추천시스템의 결과에 대하여 \\(k\\)개 Cutoff를 적용하여 Precision을 구한다면, 이를 Precision@K라고 한다. Precision@K는 Cutoff에 따라 달라질 수 있다.\n\u0026amp;nbsp\n Cutoff k=10인 경우 \\(\\rightarrow P(k=10) = 0.6\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답   8 자동차 보험 상품 오답   9 렌트카 이용권 오답   10 자동차 핸드폰 거치대 정답    \u0026amp;nbsp\n Cutoff k=3인 경우 \\(\\rightarrow P(k=3) = 1\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답    \u0026amp;nbsp\n Cutoff k=5인 경우 \\(\\rightarrow P(k=5) = 0.8\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답    \u0026amp;nbsp\n Cutoff k=7인 경우 \\(\\rightarrow P(k=7) = 0.714\\)     순위 추천 아이템 정답 / 오답     1 엔진 오일 정답   2 자동차 배터리 정답   3 차량용 방향제 정답   4 자동차 장난감 오답   5 자동차 워셔액 정답   6 초보운전 스티커 오답   7 타이어 정답    \u0026amp;nbsp\nAverage Precision (AP@K) Cutoff가 \\(K\\)개인 Average Precision(AP@K)은 Precision@K의 평균을 구하는 과정이다.\n$$ AP@K = \\frac{1}{m} \\sum_{j=1}^K P(j) \\cdot rel(j) \\dots \\begin{cases} rel(j)=1 \u0026amp; \\text{if } j^{th} \\text{ item is relevant}, \\cr rel(j)=0 \u0026amp; \\text{if } j^{th} \\text{ item is not relevant}, \\cr \\end{cases} $$\n \\(K\\) : Cutoff 갯수 \\(m\\) : 추천 아이템 중 relevance가 있는 아이템의 갯수 (number of relevant document) \\(j\\) : 전체 추천 아이템 리스트 중, 해당 추천 아이템의 index \\(P(j)\\) : \\(j\\)번째 까지의 precision값 \\(rel(j)\\) : \\(j\\)번째의 relevance 여부  \u0026amp;nbsp\n위에서 예시로 들었던 자동차용품 추천결과를 통해, [\\(AP@5\\), \\(AP@7\\), \\(AP@9\\), \\(AP@10\\)] 을 계산해 보았다. 특히 \\(AP@7\\) 와 \\(AP@9\\) 의 결과에서 관찰할 수 있듯이, \\(\\frac{1}{m}\\)에서 \\(m\\)은 relevance가 있는 경우만을 포함하기 때문에, 뒤에서 오답이 추가되어도 AP의 값이 페널티를 받지는 않는다.\n$$AP@5 = \\frac{1}{4} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5}\\right) = 0.95$$\n$$AP@7 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} \\right) = 0.90$$\n$$AP@9 = \\frac{1}{5} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 \\right) = 0.90$$\n$$AP@10 = \\frac{1}{6} \\cdot \\left(\\frac{1}{1} + \\frac{2}{2} + \\frac{3}{3} + 0 + \\frac{4}{5} + 0 + \\frac{5}{7} + 0 + 0 + \\frac{6}{10}\\right) = 0.85$$\n\u0026amp;nbsp\n또한, 아래의 예시에서 Case A와 Case B를 비교해보면, 순위가 높은 추천 아이템이 정확할 수록 높은 AP값이 계산되므로, 추천의 순서 또는 순위가 평가 지표에 영향을 끼침을 알 수 있다.\n$$AP@5 = \\frac{1}{3} \\cdot \\left(\\frac{1}{1} + 0 + \\frac{2}{3} + 0 + \\frac{3}{5}\\right) = 0.75 \\dots \\text{(Case A)}$$\n$$AP@5 = \\frac{1}{3} \\cdot \\left(0 + \\frac{1}{2} + 0 + \\frac{2}{4} +\\frac{3}{5} \\right) = 0.53 \\dots \\text{(Case B)}$$\n\u0026amp;nbsp\nMean Average Precision (MAP@K) AP는 각각의 사용자(또는 쿼리)에 대하여 계산한 것이므로, 각 사용자에 따라 AP값이 산출된다. Mean Average Precision(MAP)은 AP값들의 Mean을 구한 것으로, 식은 다음과 같다.\n$$MAP@K = \\frac{1}{U} \\sum_{u=1}^{U} (AP@K)_u$$\n \\(U\\) : 총 사용자의 수  \u0026amp;nbsp \u0026amp;nbsp\n마무리하며 이번 MAP에 대해 알아보았다. 다음 포스트에서는 역시나 추천시스템의 평가지표로 자주 등장하는 DCG(Discounted Cumulative Gain)에 대해서 공부하고 정리 할 예정이다.\nAP, MAP의 파이썬 코드로 된 구현체는 링크를 통해 참조할 수 있다.\n\u0026amp;nbsp \u0026amp;nbsp\nReference   https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\n  http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n  http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html\n  ","permalink":"https://lucaseo.github.io/posts/2020-04-23-mean-average-precision/","summary":"추천 시스템의 평가 지표 \u0026hellip; ? 추천 시스템은 이름에서도 알 수 있듯, 어떤 사용자가 관심을 가질 법한 아이템을 추천하는 알고리즘이다. 추천 시스템의 성능은 어떻게 평가할 수 있을까? 추천시스템에 대해 깊게 생각하지 않았을 적에는 분류 문제에서 성능을 평가하는 것과 비슷하다고 생각했다. \u0026ldquo;사용자가 관심을 가질만한 아이템이 맞다 또는 아니다.\u0026quot; 를 측정한다면, 우리에게 익숙한 precision, recall 등으로 생각해볼 수도 있을 것 같다.\n하지만, 분류 성능 지표에서는 추천의 순서나 순위가 고려되지 않는다. (역시 어줍잖게 생각하면 안 돼 \u0026hellip;)","title":"[KR] 추천시스템의 평가 지표 : MAP"},{"content":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움. 하지만 아무도 두려워하지 않는 듯해 보여서 더 두려운 고요한 두려움.\n\u0026ldquo;대체 언제까지 슬랙으로 엑셀 파일을 주고 받아야 하는가?\u0026ldquo;에 대한 해답을 찾던 중, 구글 스프레드시트(Google Spreadsheet)와 연동하는 것으로 몇가지 고민을 해결할 수 있게 되었다. gspread라는 라이브러리를 찾게 되었다. 별 거 아닌데, 왜 여태 사용해보지 않았을까. 이거다 싶었다.\n   바로 적용한 아주 간단한 예시를 들자면, 리서쳐들이 구글 스프레드시트에서 작업한 것들을 DB에 적재하는 과정을 자동화할 수 있었고, 데이터를 누가 작업했고, 업데이트가 되었는지에 대한 여부 또한 스프레드시트를 기반으로 작업하게 되니 해결 되었다. Python과 gspread 통해서 구글 스프레드시트와 연동하는 과정을 다룬 사내 튜토리얼을 다시 정리해보았다.\n\u0026amp;nbsp\n1. 튜토리얼 1.1. GCP에서 사용자 인증 설정 Python으로 스프레드시트를 연동하기 위해서는, GCP(Google Cloud Platform)에서 사용자 인증과 API 사용 인증이 준비되어야 한다.\n\u0026amp;nbsp\n1.1.1. 구글 클라우드 플랫폼에 로그인  회사에서는 팀원들과 드라이브를 공유하기에 Gsuite 계정으로 로그인했다.     \u0026amp;nbsp\n1.1.2. 새로운 프로젝트 생성  구글 드라이브, 구글 스프레드시트와 연동할 새로운 프로젝트를 새로 생성한다.              \u0026amp;nbsp\n1.1.3. 구글 드라이브 API 사용 설정  새로 생성한 프로젝트를 선택하고, API 개요 이동하여 구글 드라이브 API 사용을 설정한다.                     \u0026amp;nbsp\n1.1.4. 구글 드라이브 API에 대한 인증정보 생성.  구글 드라이브 API를 사용함에 있어서 필요한 사용자 인증정보를 추가한다. 새 서비스 계정을 만들면, 서비스 계정 및 Key가 JSON 형태의 파일이 받아지게 된다.  특히 추후 파이썬 코드 내에서 Key가 필요하게 되니, 적절한 디렉토리에 저장하도록 한다.                 \u0026amp;nbsp\n1.1.5. 구글 스프레드시트 API 사용 설정  구글 드라이브 API 사용 설정한 방법과 동일하게, 구글 스프레드시트 API 사용설정을 활성화 한다.        \u0026amp;nbsp \u0026amp;nbsp\n1.2. 파이썬 패키지 설치  파이썬에서는 gspread와 oauth2client가 필요하다.  gspread: 파이썬을 통해 구글 스프레드시트와 연동하고, 제어할 수 있게 하는 패키지. oauth2client: OAuth2을 통해 사용자 인증을 하기 위해 설치함.   터미널 또는 CMD에서 pip를 통해 설치하도록 한다.  pip install gspread pip install --upgrade oauth2client \u0026amp;nbsp \u0026amp;nbsp\n1.3. 구글 스프레드시트 파일 설정  파이썬을 통해서 접근하고 연동하고자 하는 해당 구글 시트에서 사용자 인증 정보를 설정하다. 앞서 내려 받은 JSON파일에서 client_email의 값을 복사한 뒤, 해당 스프레드시트 우측 상단 공유를 클릭한 뒤 입력하여 권한을 부여한다..         복잡하지는 않지만, 스프레드시트마다 이 작업을 해주어야 한다는 점이 약간 번거로운 부분이다. 공유 권한부여까지 완료되었다면, 이제 파이썬으로 스프레드시트의 데이터를 읽어와보자!  \u0026amp;nbsp \u0026amp;nbsp\n1.4. 파이썬에서 테스트 1.4.1. 인증과 연동  사용자 인증 파일(JSON)을 통해 연동을 한다.  from oauth2client.service_account import ServiceAccountCredentials scope = [\u0026#34;https://spreadsheets.google.com/feeds\u0026#34;, \u0026#34;https://www.googleapis.com/auth/spreadsheets\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive.file\u0026#34;, \u0026#34;https://www.googleapis.com/auth/drive\u0026#34;] creds = ServiceAccountCredentials.from_json_keyfile_name(\u0026#34;{your_JSON_filename}.json\u0026#34;, scope) \u0026amp;nbsp\n1.4.2. 스프레드 시트 선택  테스트를 위해서 샘플 스프레드시트를 생성한 상태이다.  빈 시트인 시트1, 그리고 상장기업종목코드와 기업명이 담긴 sample_data 시트가 있다.        gspread 패키지를 통해 인증 후, 접근하고자 하는 시트의 이름을 패스한다.  import gspread spreadsheet_name = \u0026#34;{your target spreadsheet name}\u0026#34; client = gspread.authorize(creds) spreadsheet = client.open(spreadsheet_name) \u0026amp;nbsp\n1.4.3. 시트 불러오기 for sheet in spreadsheet.worksheets(): print(sheet) \u0026lt;Worksheet '시트1' id:1966713574\u0026gt; \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.4. 시트 선택하기  이름 또는 인덱스를 통해 시트를 선택할 수 있다.  ## by name sheet = spreadsheet.worksheet(\u0026#34;sample_data\u0026#34;) ## OR by index sheet = spreadsheet.get_worksheet(1) print(sheet) \u0026lt;Worksheet 'sample_data' id:0\u0026gt; \u0026amp;nbsp\n1.4.5. 시트 내 데이터 읽어오기  get_all_values() 함수는 시트 내 데이터를 모두 출력한다.  sheet.get_all_records()[:3] [{'CompanyStockCode': 'S030190', 'PA_CompanyID': 1, 'IA_CompanyID': 'ID00001', 'DelistStatus': '', 'CompanyKorName': 'NICE평가정보'}, {'CompanyStockCode': 'S038620', 'PA_CompanyID': 2, 'IA_CompanyID': 'ID00002', 'DelistStatus': '', 'CompanyKorName': '위즈코프'}, {'CompanyStockCode': 'S039020', 'PA_CompanyID': 3, 'IA_CompanyID': 'ID00003', 'DelistStatus': '', 'CompanyKorName': '이건홀딩스'}] \u0026amp;nbsp\n1.4.6. Pandas DataFrame 형태로 변환  간단한 함수를 작성하여 Pandas의 DataFrame형태로 데이터를 불러올 수 있다.  def gsheet2df(sheet): df = pd.DataFrame(columns=list(sheet.get_all_records()[0].keys())) for item in sheet.get_all_records(): df.loc[len(df)] = item return df df_sample_data = gsheet2df(sheet) df_sample_data.head()    \u0026amp;nbsp \u0026amp;nbsp\n2. 마무리하며 리서쳐와 데이터를 주고 받으며 일하던 와중에, gspread의 적용은 매우 빛과 소금 같은 시원한, 그리고 매우 신속했던 해결책이었다. 내 업무영역에 적용한 뒤, 바로 다른 분석가 동료들에게 튜토리얼을 통해 공유하는 세션을 가지기도 했던 만큼 적용되는 범위가 컸다.\n누구든 일을 하다 잠재적인 기술 부채를 맞닥뜨리면, 미래의 나를 위해서 어떻게든 털어내버리고 싶을 것이다. 하지만, 함께 해결해나갈 인력이 없는 환경에서 비개발자인 동료들도 불편해하지 않게끔 적절한 방법을 찾고 도입하는 것은 생각보다 쉽지 않았다. 답답할 때가 없지는 않지만, 이번 케이스처럼 또 하나하나씩 도입해나가면 더 매끄러워지지 않을까라는 생각을 해보며 또 다른 기술 부채를 맞닥뜨릴 마음의 준비를 해본다.\n\u0026amp;nbsp\n 3. Reference   gspread 공식 도큐먼트\n  Manage Google Spreadsheets with Python and Gspread\n  ","permalink":"https://lucaseo.github.io/posts/2020-04-12-python-spreadsheet-gspread/","summary":"0. Motivation Who\u0026rsquo;s Good에서는 ESG리서쳐와 분석가/개발자 간에 데이터를 주고 받는 일이 매우 빈번하다. 특히 기업 관련 뉴스 데이터와, 다양한 소스로부터 수집하는 ESG 관련 데이터에 대한 QC를 진행하고 결과를 DB에 적재하는 과정이 있다. 엑셀에서 작업한 데이터를 저장하고, 슬랙으로 전달하는 여러 단계와 여러 사람들을 거치다 보니 주고받은 파일명이 뒤죽박죽인 아주 원초(?)적인 문제부터, 데이터가 언제 업데이트 되었는지 추적이 불가능한 상황도 발생하면서 마음 한 켠에 찝찝함이 남아있는 나날이 계속 되었다. 언제 어디선가 불시에 문제가 생기지는 않을까 하는 두려움.","title":"[KR] Python으로 구글 스프레드시트 연동하기 (ft. gspread)"},{"content":"   Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.   Raw Data  테이블 형태의 데이터셋을 확인할 수 있다.   Map: Confirmed  1월 22일부터 현재까지 전세계의 확진자 현황을 지도로 시각화한다.       (데이터셋은 Johns Hopkins CSSE의 Github에 제공된 COVID-19 데이터를 사용하였다.)\n\u0026amp;nbsp\n 사전 준비 2: Heroku Heroku는 웹사이트나 어플리케이션을 빌드하고 배포할 수 있는 PaaS이다. 파이썬, 자바, 루비, PHP, Node.js, Go 등 여러 언어를 지원하고 있다. 소규모 어플리케이션이라면 무료로도 배포할 수 있다.\nHeroku 가입 Heroku를 통해 배포를 하기 위해서는 우선 계정이 있어야 한다. 아래 링크를 통해 Heroku 계정을 생성하자.\n https://www.heroku.com/  Heroku CLI 설치 설치 계정을 생성하였다면, 현재 개발 환경에 Heroku Command Line Interface를 설치한다. 아래의 링크에는 각자의 OS에 맞추어 Heroku CLI의 설치 가이드가 제공되어 있다.\n https://devcenter.heroku.com/articles/getting-started-with-python#set-up  CLI를 통한 로그인 설치가 완성되었다면, 쉘에서 heroku 커맨드를 사용할 수 있다. 다음 명령어로 Heroku에 로그인 해보자.\nheroku login    아래와 같은 결과가 출력되며, 브라우저를 열어 로그인 하도록 한다. Heroku에 대한 준비작업은 이로서 완료되었다.\n   \u0026amp;nbsp\n 배포를 위한 파일 생성하기 Heroku에 배포를 하기 위해서는 몇 가지 준비사항이 충족되어야 한다.\n app.py가 위치한 가상환경에서 Git Repository를 생성한다.  git init 배포를 위해 필요한 파일들을 생성한다.  파일 #1. requirements.txt pip freeze \u0026gt; requirements.txt 파일 #2. .gitignore venv *.pyc .DS_Store .env 파일 #3. setup.sh setup.sh에서는 Streamlit에 대한 config.toml 파일을 생성한다.\nmkdir -p ~/.streamlit/ echo \u0026#34;\\ [server]\\n\\ headless = true\\n\\ enableCORS=false\\n\\ port = $PORT\\n\\ \u0026#34; \u0026gt; ~/.streamlit/config.toml 파일 #4. Procfile Procfile에서는 Heroku로 하여금 웹어플리케이션을 시작시키기 위해 명령어를 순서대로 실행하도록 한다.\nweb: sh setup.sh \u0026amp;\u0026amp; streamlit run app.py (Procfile에 대한 자세한 사항은 다음 링크에서 참조할 수 있다.)\n\u0026amp;nbsp\n Heroku에 배포하기 배포를 위한 파일이 모두 준비되었다면, 이제 본격적으로 Heroku에 배포를 할 수 있게 되었다.\nHeroku App 생성 heroku create {your app name} 앞서 Heroku CLI를 생성했을 때와 같은 Heroku 로그인 과정을 거쳐, Heroku App이 생성 되었다.\n   생성된 App은 Heroku 사이트의 대시보드에서도 다음과 같이 확인이 가능하다.\n   Heroku repository 생성 Heroku는 배포를 위해 Git을 사용하므로, 방식도 같다.\ngit add . git commit -m \u0026#39;Init app boilerplate\u0026#39; git push heroku master       Heroku 의 프로세스 인스턴스를 1개로 제한하는 것을 권장한다. 프로세스가 많을 수록 유료가 될 수 있기 때문에, 무료인 상태에서 계속 사용하고 싶다면, 아래의 명령어를 실행시켜준다.\nheroku ps:scale web=1    Heroku 배포 확인 드디어 배포가 완료되었다! 이제 배포된 웹 어플리케이션을 https://your_app.herokuapp.com 에서 확인할 수 있다. 본 튜토리얼을 위해 배포된 웹 어플리케이션은 https://tuto-covid19-map.herokuapp.com/ 에서 확인할 수 있다.\n   \u0026amp;nbsp\n 마무리 하며 간단한 과정을 거쳐 웹 어플리케이션을 배포까지 해보았다. AWS와 같은 클라우드에 배포하는 과정은 익숙하지 않았고, 현재 Streamlit이 자체적으로 특정 배포 방식을 권장하고 있지는 않기에, Dash(Plotly에서 개발한 웹어플리케이션 프레임워크)에서 웹어플리케이션을 Heroku에 배포하는 가이드를 적용해 보았다. 큰 어려움 없이 배포를 완료하였기에, 많은 분들도 이 과정을 참고하시어 Streamlit을 활용한 다양한 프로젝트를 진행할 수 있으리라 생각된다.\n\u0026amp;nbsp\n 코드  Github  \u0026amp;nbsp\n참고자료  Dash 공식 페이지 Johns Hopkins CSSE Github Repository \u0026lt;em\u0026gt;Mapping the Spread of Coronavirus COVID-19 with python and Plotly\u0026lt;/em\u0026gt; by. Babak Fard  ","permalink":"https://lucaseo.github.io/posts/2020-03-29-deploy-streamlit-to-heroku/","summary":"Source: streamlit.io   Streamlit 배포하기 Streamlit의 주요 기능을 살펴보았던 지난 포스트에 이어, 이번 포스트에서는 Streamlit으로 만든 간단한 웹어플리케이션을 Heroku에 배포하는 과정을 다루어보고자 한다.\n\u0026amp;nbsp\n 사전 준비사항 들어가기에 앞서 2가지 사전 준비 사항이 있다.\n사전 준비 1: Streamlit 웹 어플리케이션 튜토리얼을 진행하기에 앞서, Streamlit기반의 아주 아주 간단한 시각화 웹 어플리케이션을 만들어보았다. 로컬에서 작동시킨 웹 어플리케이션은 다음과 같다. 해당 어플리케이션의 코드는 링크에서 참고 가능하다.\n Main Page  데이터셋에 대한 설명을 간단히 소개한다.","title":"[KR] Streamlit 웹 어플리케이션 배포하기 (feat. Heroku)"},{"content":" Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;\n웹개발을 몰라도 된다.\n 웹개발에 대해 아는 것이 전혀 없는 나 같은 사람도 페이지를 띄울 수 있다. 주로 사내용으로 이용되기 때문에 UI/UX적인 측면에서 뛰어나지 않아도 되기 때문에 일정 수준의 미적인 요소들이 기본적으로 적용되있는 점이 매우 편리하다. 간결하고 명확한 API 덕분에 다른 웹프레임워크와 비교해서 상대적으로 진입장벽이 낮다. 일정한 수준의 결과를 내기 위해 투자하는 시간이 매우 절약된다.  전달력이 매우 좋다.\n 웹어플리케이션은 사용자에게도 진입장벽이 낮다. 특히 interactive한 성향 덕분에, 슬라이드 포맷의 레포트나 자료보다 전달력과 만족도가 높은 것을 볼 수 있었다.  \u0026amp;nbsp\n덕분에 Streamlit은 조직 내부적으로 탐색적 데이터 분석(EDA) 결과를 공유하거나, 간단한 ML 모델을 배포하고 테스트를 하는 용도에 부합하는 툴이라고 할 수 있다. Streamlit API에서 제공하는 기능들을 간단하게 훑어보자.\n Streamlit 간단 맛 보기 설치 pip를 통해 설치하기 $ pip install streamlit \u0026amp;nbsp\n실행하기 Streamlit은 8501 포트에 앱이 실행된다. 일단 지금 아무 것도 없는 상황에서는 우측 상단 버튼만 있는 페이지를 볼 수 있다.\n$ streamlit run {your app}.py You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://{your_network}:8501    Streamlit 불러오기 Streamlit은 st라는 alias로 불러온다.\n# import Streamlit Library import streamlit as st 소스에 변경이 생길 경우 경우 상단에 알림이 뜬다. Rerun을 해주도록 하자.\n   \u0026amp;nbsp\n텍스트 출력 Header와 Text  Title, Header \u0026amp; Subheader  Header와 Subheader를 다음과 같이 달 수 있다. 다만 Header의 경우 subheader 레벨까지만 가능하다. Title, Header, Subheader이 각각 Header, Subheader, Subsubheader인 것으로 인식하면 될 것 같다.    ## Title st.title(\u0026#39;Streamlit Tutorial\u0026#39;) ## Header/Subheader st.header(\u0026#39;This is header\u0026#39;) st.subheader(\u0026#39;This is subheader\u0026#39;) ## Text st.text(\u0026#34;Hello Streamlit! 이 글은 튜토리얼 입니다.\u0026#34;)    \u0026amp;nbsp\nMarkdown Streamlit도 Dash와 마찬가지로 Markdown을 지원한다.\n## Markdown syntax st.markdown(\u0026#34;# This is a Markdown title\u0026#34;) st.markdown(\u0026#34;## This is a Markdown header\u0026#34;) st.markdown(\u0026#34;### This is a Markdown subheader\u0026#34;) st.markdown(\u0026#34;- item 1\\n\u0026#34; \u0026#34; - item 1.1\\n\u0026#34; \u0026#34; - item 1.2\\n\u0026#34; \u0026#34;- item 2\\n\u0026#34; \u0026#34;- item 3\u0026#34;) st.markdown(\u0026#34;1. item 1\\n\u0026#34; \u0026#34; 1. item 1.1\\n\u0026#34; \u0026#34; 2. item 1.2\\n\u0026#34; \u0026#34;2. item 2\\n\u0026#34; \u0026#34;3. item 3\u0026#34;) 2020-03-13-intro-to-streamlit/4\n\u0026amp;nbsp\nLatex Latex의 경우 백슬래시(\\)를 빈번히 사용되기 때문에, 일반 string 대신 raw string을 붙여주는 편이 좋다.\n## Latex st.latex(r\u0026#34;Y = \\alpha + \\beta X_i\u0026#34;) ## Latex-inline st.markdown(r\u0026#34;회귀분석에서 잔차식은 다음과 같습니다 $e_i = y_i - \\hat{y}_i$\u0026#34;)    \u0026amp;nbsp\n메세지와 에러메세지, 예외처리 메세지 기본적으로 포맷된 메세지 박스 기능을 제공한다.\n## Error/message text st.success(\u0026#34;Successful\u0026#34;) st.info(\u0026#34;Information!\u0026#34;) st.warning(\u0026#34;This is a warning\u0026#34;) st.error(\u0026#34;This is an error!\u0026#34;) st.exception(\u0026#34;NameError(\u0026#39;Error name is not defined\u0026#39;)\u0026#34;)    \u0026amp;nbsp \u0026amp;nbsp\n데이터프레임과 테이블 출력. 데이터를 출력하는 방법에는 3가지 방법이 있다.\n  st.table:\n 단순히 입력 테이블 전체를 리턴한다.    st.dataframe:\n 적절히 10개의 행을 기준으로 스크롤을 통해 데이터를 관찰 할 수 있고 각 열마다 정렬도 가능하다. 각 테이블의 우측 상단의 확대 버튼을 통해 테이블을 더 크게 볼 수 있고,    st.write:\n st.dataframe과 똑같은 결과를 리턴한다.    ## Load data import pandas as pd from sklearn.datasets import load_iris iris = load_iris() iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df[\u0026#39;target\u0026#39;] = iris[\u0026#39;target\u0026#39;] iris_df[\u0026#39;target\u0026#39;] = iris_df[\u0026#39;target\u0026#39;].apply(lambda x: \u0026#39;setosa\u0026#39; if x == 0 else (\u0026#39;versicolor\u0026#39; if x == 1 else \u0026#39;virginica\u0026#39;)) ## Return table/dataframe # table st.table(iris_df.head()) # dataframe st.dataframe(iris_df) st.write(iris_df)    \u0026amp;nbsp \u0026amp;nbsp\n이미지, 오디오, 비디오 파일 출력. 이미지, 영상, 오디오 파일을 열어서 재생할 수 있다.\n st.image : 파이썬 이미지 라이브러리와 함께 쓸 수 있다. st.video :  파일의 포맷을 지정해야 하며, 디폴트로는 video/mp4가 설정되어 있다. start_time 변수를 통해 재생시작점을 조절할 수 있다.   st.audio :  파일 포맷은 audio/wav 가 디폴트로 설정되어 있다. 마찬가지로 start_time 변수를 통해 재생시작점을 조절할 수 있다.    ##Show image from PIL import Image img = Image.open(\u0026#34;files/example_cat.jpeg\u0026#34;) st.image(img, width=400, caption=\u0026#34;Image example: Cat\u0026#34;) ## Show videos vid_file = open(\u0026#34;files/example_vid_cat.mp4\u0026#34;, \u0026#34;rb\u0026#34;).read() st.video(vid_file, start_time=2) ## Play audio file. audio_file = open(\u0026#34;files/loop_w_bass.mp3\u0026#34;, \u0026#34;rb\u0026#34;).read() st.audio(audio_file, format=\u0026#39;audio/mp3\u0026#39;, start_time=10)    \u0026amp;nbsp \u0026amp;nbsp\n위젯 st.checkbox - 체크박스 ## Checkbox if st.checkbox(\u0026#34;Show/Hide\u0026#34;): st.write(\u0026#34;체크박스가 선택되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.radio - 라디오버튼 ## Radio button status = st.radio(\u0026#34;Select status.\u0026#34;, (\u0026#34;Active\u0026#34;, \u0026#34;Inactive\u0026#34;)) if status == \u0026#34;Active\u0026#34;: st.success(\u0026#34;활성화 되었습니다.\u0026#34;) else: st.warning(\u0026#34;비활성화 되었습니다.\u0026#34;)    \u0026amp;nbsp\nst.selectbox - 드랍다운 선택 ## Select Box occupation = st.selectbox(\u0026#34;직군을 선택하세요.\u0026#34;, [\u0026#34;Backend Developer\u0026#34;, \u0026#34;Frontend Developer\u0026#34;, \u0026#34;ML Engineer\u0026#34;, \u0026#34;Data Engineer\u0026#34;, \u0026#34;Database Administrator\u0026#34;, \u0026#34;Data Scientist\u0026#34;, \u0026#34;Data Analyst\u0026#34;, \u0026#34;Security Engineer\u0026#34;]) st.write(\u0026#34;당신의 직군은 \u0026#34;, occupation, \u0026#34; 입니다.\u0026#34;)       \u0026amp;nbsp\nst.multiselect - 드랍다운 다중 선택 ## MultiSelect location = st.multiselect(\u0026#34;선호하는 유투브 채널을 선택하세요.\u0026#34;, (\u0026#34;운동\u0026#34;, \u0026#34;IT기기\u0026#34;, \u0026#34;브이로그\u0026#34;, \u0026#34;먹방\u0026#34;, \u0026#34;반려동물\u0026#34;, \u0026#34;맛집 리뷰\u0026#34;)) st.write(len(location), \u0026#34;가지를 선택했습니다.\u0026#34;)       \u0026amp;nbsp\nst.slider - 슬라이더 ## Slider level = st.slider(\u0026#34;레벨을 선택하세요.\u0026#34;, 1, 5)    \u0026amp;nbsp\nst.button - 버튼 ## Buttons if st.button(\u0026#34;About\u0026#34;): st.text(\u0026#34;Streamlit을 이용한 튜토리얼입니다.\u0026#34;)    \u0026amp;nbsp\n텍스트 입력 # Text Input first_name = st.text_input(\u0026#34;Enter Your First Name\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;first_name\u0026#39;): result = first_name.title() st.success(result) # Text Area message = st.text_area(\u0026#34;메세지를 입력하세요.\u0026#34;, \u0026#34;Type Here ...\u0026#34;) if st.button(\u0026#34;Submit\u0026#34;, key=\u0026#39;message\u0026#39;): result = message.title() st.success(result)    \u0026amp;nbsp\n날짜와 시간 입력 ## Date Input import datetime today = st.date_input(\u0026#34;날짜를 선택하세요.\u0026#34;, datetime.datetime.now()) the_time = st.time_input(\u0026#34;시간을 입력하세요.\u0026#34;, datetime.time())       \u0026amp;nbsp\n코드와 JSON 출력  with st.echo(): 이하의 코드는 코드블럭으로 출력된다.  ## Display Raw Code - one line st.subheader(\u0026#34;Display one-line code\u0026#34;) st.code(\u0026#34;import numpy as np\u0026#34;) # Display Raw Code - snippet st.subheader(\u0026#34;Display code snippet\u0026#34;) with st.echo(): # 여기서부터 아래의 코드를 출력합니다. import pandas as pd df = pd.DataFrame() ## Display JSON st.subheader(\u0026#34;Display JSON\u0026#34;) st.json({\u0026#39;name\u0026#39; : \u0026#39;민수\u0026#39;, \u0026#39;gender\u0026#39;:\u0026#39;male\u0026#39;, \u0026#39;Age\u0026#39;: 29})    \u0026amp;nbsp\n사이드바 st.sidebar에서도 대부분의 위젯을 지원하므로, 다양하게 사이드바를 구성할 수 있다. (단, st.echo, st.spinner, st.write제외)\n## Sidebars st.sidebar.header(\u0026#34;사이드바 메뉴\u0026#34;) st.sidebar.selectbox(\u0026#34;메뉴를 선택하세요.\u0026#34;, [\u0026#34;데이터\u0026#34;, \u0026#34;EDA\u0026#34;, \u0026#34;코드\u0026#34;])    \u0026amp;nbsp\n차트 그리기 Streamlit은 자체 내장된 기본적인 차트 외 matplotlib, plot.ly, altair, vega_ilte, bokeh, deck_gl, pydeck, graph_viz 등 다양한 시각화 패키지를 지원한다.\n(Streamlit은 EDA 용도로 많이 사용되는 만큼, 시각화 부분은 따로 다룰 계획이다.)\n## Plotting st.subheader(\u0026#34;Matplotlib으로 차트 그리기\u0026#34;) iris_df[iris_df[\u0026#39;target\u0026#39;]==\u0026#39;virginica\u0026#39;][\u0026#39;petal length (cm)\u0026#39;].hist() st.pyplot()    \u0026amp;nbsp \u0026amp;nbsp\n 마무리 Streamlit의 API를 훑어보면서, 전체적으로 많은 부분이 간결하고, 쉽다고 느껴졌다. Flask나 Django로 개발하는 개발자 입장에서 Streamlit 같은 프레임워크는 자유도가 제한된다고 느껴질 수도 있겠다. 하지만 등장한지 얼마 안 되는 만큼 커뮤니티 포럼에서는 활발한 토론과 기능 추가에 대한 요청이 이어지고 있는 중이다. Streamlit 개발자들이 적극적으로 피드백을 반영하는 모습을 보이고 있으므로, 앞으로의 발전이 더 기대된다.\n이번 소개 글에 이어, 개인적으로 Streamlit으로 개발하면서 얻은 팁이나, 클라우드 또는 Heroku에 배포하는 과정, 데이터사이언스 프로젝트를 위해 웹어플리케이션을 구성하는 팁 등을 시리즈로 작성할 예정이다. 나처럼 웹개발은 모르지만 데이터분석 결과를 그럴 듯하게 구성하고 싶은 분들께 도움이 되었으면 한다.\n\u0026amp;nbsp\n코드  Github  \u0026amp;nbsp\n참고자료  Streamlit 공식 페이지  ","permalink":"https://lucaseo.github.io/posts/2020-03-13-intro-to-streamlit/","summary":"Streamlit은 데이터사이언스/ML 프로젝트를 간단하게 배포할 수 있는 웹어플리케이션으로, 최근에 많은 관심을 받고 있습니다. 이번 포스트에서는 Streamlit의 간단한 소개와 기본 기능들을 훑어보겠습니다.\n  2020-03-13-intro-to-streamlit/streamlit_logo.png \u0026ldquo;Source: streamlit.io\u0026rdquo;)    Source: streamlit.io   Streamlit 이란? Streamlit(스트림릿)은 2019년 하반기에 갑작스레 등장한(?) 파이썬 기반의 웹어플리케이션 툴이다. Medium 플랫폼에서 Streamlit이라는 키워드가 보이는 글이 추천되는 것을 자주 보게 되었는데, \u0026ldquo;데이터사이언스/머신러닝 프로젝트를 웹 어플리케이션에 배포\u0026quot;하는데 아주 편리한 툴이라는 설명이 눈길을 사로 잡았다.\n\u0026amp;nbsp\n나에게 있어 Streamlit나 Dash 같은 웹어플리케이션의 장점을 꼽자면;","title":"[KR] 파이썬 웹어플리케이션 맛보기 (feat. Streamlit)"},{"content":" 신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다. 회사에 시니어가 없어서 그런지 이 루트로 도움을 많이 받고 있음을 체감한다. 자연스럽게 ‘나도 할 수 있을까?’ 라는 생각이 들었다. 데이터를 다루는 일을 하고 있지만, 내가 하는 일을, 내가 아는 것들을 글로서 풀어낼 수 있을지 스스로 물어보면 돌아오는 답은 ‘자신이 없다’ 였다. 솔직히 아직은 자신이 많이 없다.\n   \u0026#34;핑계는 수백만개라도 댈 수 있죠\u0026#34; Source: The Office   \u0026amp;nbsp\n이런 와중에 글또라는 모임은 1년 전부터 여러 커뮤니티와 변성윤님을 통해 몇 차례 접하고 있었다. 글 쓰는 또라이가 세상을 바꾼다라니… 너무 멋있다고 생각했다 (역시 대단한 사람들!). 마침 글또 4기의 모집 글을 접하게 되었고, 글쓰는 것에 대해 여러 생각을 하는 중 타이밍이 맞았다. 다른 때 같았으면 모집글만 보고 “내가 저런 거 할 시간이 어딨어” 등등 갖가지 이유를 대며 안 할 수도 있긴 했다. 그래서 일단 질렀다. 글또 지원은 2020년에 강림하신 첫 지름신이었다.\n글또를 통해 이루고자 하는 것 글쓰는 꾸준한 습관 몇 년 전부터 글을 써보고자 다양한 플랫폼에서 시도했었다. 페이스북, 워드프레스 블로그, 인스타그램, 힙합엘이…. 여러 가지 이유가 있었겠지만, 가장 주요한 이유를 냉정하게 꼽아보자면 끈기가 부족했다. 글 쓰는 체력이 부족하고, 쓰다가 아니다 싶으면 접어버리는 안 좋은 습관도 있다는 것을 인지하게 됐다.\n반면에 글또는 다 함께 정한 규칙과 약간의 강제성이 있고, 글을 읽고 피드백을 해주는 분들이 있다. 나 또한 다른 분들의 글을 읽고 피드백을 해야 한다. 지금 나에게 필요한 것은 약간의 강제성을 부여받고 다른 멋진 분들을 통해 자극을 받는 것이니까, fit이 딱 맞는다. 예치금을 삭감당하지 않겠다. 기필코.\n\u0026amp;nbsp\nClear할 때까지 글또를 통해서 얻고자 또 다른 목표는, 글쓰기를 통해 이해도와 설명력을 끌어올리는 것이다. 설명력은 높은 이해를 기반으로 이루어지는 것으로 생각하는데, 여태까지 회사에서 일하고 개인적으로 공부를 하면서 늘 어중간하게 알고 있는 것 같은 찝찝함을 버리지 못했다. 반면에 공부를 잘하는 친구들은 (1) 교과서만 봤어요, (2)질문을 많이 했어요, (3)다른 친구들에게 (혹은 부모님을 앉혀놓고) 설명을 해주면서 스스로 공부가 많이 됐어요 라고 들 많이 하더라. 생각해보면 난 학창 시절에 저런 걸 하나도 안 했다. 하지만 글또를 통해서 나 자신이 더 공부하고 성장을 할 수 있을 것이라 기대한다. 무언가에 대해 글을 쓰고 남에게 보이려면, 어쭙잖게 쓰면 안 된다는 것은 본능적으로 알고 있으니까.\n\u0026amp;nbsp\n글또 4기에서 쓰고자 하는 글 글또에 참여하시는 다른 분들을 보고, 나는 어떤 글을 써볼지 고민해봤다. 무엇보다 내가 배우고 느낀 점을 기록하는 것이 현시점에서는 우선이라 신속한 정보 전달의 목적은 2순위로 두기로 했다. 일단 떠오르는 주제가 여러 가지 있지만 좀 더 구체적인 정리가 더 필요하다.\n\u0026amp;nbsp\n스스로 돌아보는 반기 별 회고 글\n 회고를 하지 않다 보니 작년 한 해 동안 대체 무엇을 한 건지…. 남는 게 많이 없었고 개인적으로 허무하다는 느낌을 많이 받았다. 이번에는 회고 글을 정리하면서 작년과 무엇이 다른지 한번 느껴보고 싶다.  \u0026amp;nbsp\n회사에서 삽질한 경험\n 시니어가 없고, 늘 개인적으로 고군분투하다 보니까 이러한 경험도 글로 남기면 좋겠다고 생각해본다. 나만 그런 것은 아닐 테니까. 그리고 증거로 남겨야겠다.  \u0026amp;nbsp\n데이터 사이언스 대회 참가 후기\n 한 번도 on-going 데이터 사이언스 대회에 참가해 본 적이 없는데, 이 또한 갖가지 핑계를 대며 하지 않았던 거라고 자평해본다. 데이터 사이언스 대회 참가를 통해 배운 점, 삽질한 점들을 돌아보고 정리해보고자 한다.  \u0026amp;nbsp\n개발 관련 책 리뷰\n 책은 자주 많이 사는데, 늘 훑어보기만 했던 게 좀 찝찝했다. 책에 대한 리뷰글을 틈틈이 쓰고자 한다. 출판사 분들 지켜봐 주세요  \u0026amp;nbsp\n글을 마치며 작년 말 느꼈던 성장에 대한 고민, 답답함을 기반으로 올해 1, 2월 사이 내린 결정들이 있는데, 글또 참여도 그중 하나였다. 벌린 일이 많아 걱정은 되지만, 앞으로 글또에서 만나게 될 많은 멋진 분들이 계시니까 아주 큰 걱정은 아니라고 생각된다. 개복치처럼 터지지 말고, 꾸준히 존버하자.\n","permalink":"https://lucaseo.github.io/posts/2020-02-23-init-geultto-4th/","summary":"신년을 맞이하기 직전, 신년을 맞이한 직후 우리는 온갖 계획과 다짐을 세운다. 어느 시점에서부터인가 계획을 세우는 게 무의미하다고 생각이 들기도 했지만, 올해도 어김 없이 그럴 듯한 다짐을 해본다. 또 속아 넘어가는 기분이지만, 이번만큼은 다르다고 해두자.\n 글또를 참여하게 됐다.    Source: 글또   \u0026amp;nbsp\n데이터 직군으로 취업을 한 이후로 많은 사람들이 그랬던 것처럼 나도 일하면서 필요하거나 막히는 것들에 대한 해결책을 능력자들의 블로그나 브런치, Medium과 같은 플랫폼을 통해 접하고 있다.","title":"[KR] 글또 4기 다짐글"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.) 원을 두 바퀴, 세 바퀴 돌면 720도, 1080도가 되는데 이런 식의 계산은 편리하지가 않다. 두바퀴는 2 * 2pi randians으로 계산하는 것이 더 편하다고 하는데 일단 지켜보자.\n아래의 Figure 1.에서처럼 45도의 각도는 45/360 * 2pi = 0.785 radians 로 표현할 수 있다.\n   \u0026amp;nbsp\n삼각함수 복습 삼각함수를 좀 복습해야겠다. 위의 그림의 삼각형을 보고 다음과 같이 부른다면\n Hypotenuse 빗변 (파랑, 원의 반지름) : r Adjacent 밑변 (빨강, x축 길이) : x Opposite 높이 (초록, y축 길이) : y  피타고라스의 정리를 통해 다음과 같이 말할 수 있다.\n$$x^2 + y^2 = r^2$$\n그렇다면, sine, cosine도 다음과 같이 구할 수 있다.\n$$\\sin = \\dfrac{y}{r}$$\n$$\\cos = \\dfrac{x}{r}$$\n단위원(unit circle)를 기준으로 본다면 r=1 이므로 다음과 같다.\n$$\\sin = \\dfrac{y}{1}$$\n$$\\cos = \\dfrac{x}{1}$$\n","permalink":"https://lucaseo.github.io/posts/2020-01-25-dsp-basic-s01-7/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  The Unit Circle : Trigonometry review unit circle(단위원)과 radian(라디안) Sine wave와 cosine wave를 설명하는 과정에서 단위원을 따라 회전하는 선의 길이와 움직임을 묘사했었다. 이 단위원의 둘레는 phase(위상)라고 하는데, X축과 회전하는 선이 이루는 각도라고 생각할 수 있다. 아니, 각도를 사용하지 않고 라디안(radians)을 사용하도록 하자.\n우리는 원의 둘레를 360도라고 배웠는데, 이번에는 2pi radians(라디안)이라고 불러보자(원의 둘레는 2pi라는 것도 배운 바 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Trigonomatry"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.\n      \u0026amp;nbsp\n선이 회전하는 속도는 곧 frequency로 나타낼 수 있다. 앞서 frequency는 1초에 몇 사이클을 회전할 수 있는지를 통해 측정한다고 했는데, 원문의 Figure 1.을 통해서 확인할 수 있듯이 frequency가 높을 수록 회전하는 선이 속력이 빨라지고, 주기도 빨라지게 된다. 반대로 frequency가 낮으면 주기가 느려진다.\n      \u0026amp;nbsp\nThe Cosine Wave : The Counterpart to Sine 반대로 cosine wave(코사인파)는 원을 그리며 회전하는 선과 X축에서 수평의 길이와 관계 있다. 사실 sine wave와 cosine wave는 90도로 회전하면 서로 같은 모양을 가지고 있다.\n   \u0026amp;nbsp\nSine wave와 cosine wave는 밀접한 관계를 맺고 있으며, 둘 다 주기성을 띄는 주기함수(periodic signal)임을 이해하는 것이 매우 중요하다.\n","permalink":"https://lucaseo.github.io/posts/2020-01-23-dsp-basic-s01-6/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Periodic Movement and the Circle Sine wave(사인파, 정현파)는, 어떠한 선이 원을 그리며 회전할 때의 모습으로 묘사할 수 있다. Sine wave는 회전하는 선과 Y축 수직의 길이가 밀접한 관계를 보인다.\nsine wave의 amplitude로 볼 수 있다. 수직의 길이가 길 수록 amplitude의 폭이 크고, 짧을 수록 amplitude의 폭이 작다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sine Wave"},{"content":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,\ndf = pd.DataFrame({ \u0026#39;A\u0026#39;: [1,2,3], \u0026#39;B\u0026#39;: [\u0026#34;goodbye\u0026#34;, \u0026#34;cruel\u0026#34;, \u0026#34;world\u0026#34;], \u0026#39;C\u0026#39;: [False, True, False] }) df.info() 결과물 출력 비교\npandas 0.x.x\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): A 3 non-null int64 B 3 non-null object C 3 non-null bool dtypes: bool(1), int64(1), object(1) memory usage: 179.0+ bytes pandas 1.0.0\n\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 3 entries, 0 to 2 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 A 3 non-null int64 1 B 3 non-null object 2 C 3 non-null object dtypes: int64(1), object(2) memory usage: 200.0+ bytes \u0026amp;nbsp\n.to_markdown() : DataFrame을 Markdown 형식으로 DataFrame을 바로 Markdown 형식으로 출력할 수 있게 되었다. 문서화 작업을 할 때 항상 markdown table generator 같은 도구를 썼었는데, 매우 반갑고 편리한 기능!\ndf = pd.DataFrame({\u0026#34;A\u0026#34;: [1, 2, 3], \u0026#34;B\u0026#34;: [1, 2, 3]}, index=[\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) print(df.to_markdown()) | | A | B | |:---|----:|----:| | a | 1 | 1 | | a | 2 | 2 | | b | 3 | 3 | \u0026amp;nbsp\ningore_index : index reset 파라미터 추가 기존에는 정렬이나 중복값 제거 후 .reset_index(drop=True)를 추가적으로 해줘야 했으나, ignore_index 파라미터를 통해 index를 리셋할 수 있게 되었다. default는 False. 다음 기능들에서 찾아볼 수 있다.\n .sort_values() .sort_index() .drop_duplicates()  \u0026amp;nbsp\npd.NA : 새로운 missing value의 실험 기존에 Pandas에서 missing value를 처리할 때는 np.nan이나 None이라는 싱글턴이 사용되었다. 그러나 data type이 float일 때는 np.nan , object일 경우에는 np.nan이나 None, datetime일 경우에는 np.NaT가 사용된다. 데이터타입마다 Null 데이터의 표현이 각기 달랐기 때문에, pd.NA는 datatype이 각기 달라도 missing data를 통일되게 표현할 수 있기 위해 도입되었다. 현재는 pd.NA는 data type 중 integer, boolean, 그리고 새로 도입된 string에서 사용 가능하다. pd.NA 값은 \u0026lt;NA\u0026gt;로 리턴된다. 실험적으로 도입했다고 하니, 지켜보면 좋을 듯.\n\u0026amp;nbsp\nstring : 새로운 data type 도입 기존에는 object 이라는 data type으로 뭉뚱그려진 느낌이 있었으나, string 이라고 따로 지정할 수 있게 됨으로서 EDA나 wrangling 측면에서 더욱 편해질 것 같다. (pd.NA도 확인 가능함)\npd.Series([\u0026#39;abc\u0026#39;, None, \u0026#39;def\u0026#39;], dtype=pd.StringDtype()) 0 abc 1 \u0026lt;NA\u0026gt; 2 def Length: 3, dtype: string \u0026amp;nbsp\nbool : missing value 표현 가능 기존에 boolean은 True / False 만 표기가 가능했으나, Pandas 1.0.0 에서는 missing value도 가능하다. (pd.NA도 가능함)\npd.Series([True, False, None], dtype=pd.BooleanDtype()) 0 True 1 False 2 \u0026lt;NA\u0026gt; Length: 3, dtype: boolean \u0026amp;nbsp\n Reference https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html\n","permalink":"https://lucaseo.github.io/posts/2020-01-16-pandas-new-release/","summary":"Major Release !! Pandas 1.0.0 import pandas as pd로 우리에게 익숙한 Pandas. 데이터 분석을 위한 라이브러리라는 사실을 모르는 사람은 거의 없을 것이다. 하지만 부끄럽게도 나는 판다스의 버전조차 모른 상태로 여태껏 공식 문서와 Stackoverflow를 통해서만 사용하고 있었다. 마침 1월 9일 Pandas 1.0.0이 배포되었고, 이번 기회에 1.0.0에서 평소 자주 썼던 부분들을 위주로 중요한 업데이트들을 훑어보고 정리해보고자 한다.\n dataframe.info() 깔끔해진 DataFrame summary DataFrame 요약 기능이 조금 보기 좋은 형태로 개선되었다.\n다음과 같은 예제 DataFrame이 있다고 할 때,","title":"[KR] Pandas 1.0.0 : 바뀐 점을 ARABOJA"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다. 음색이라는 개념은 음의 높이나 강도와는 별개의 특성이다. 플룻과 바이올린으로 똑같은 음을 연주하더라도, 소리는 서로 확연이 다른 것과 연관이 있다. 음색은 주로 배음(overtone)과 고조파(harmonic)의 유무에 따라 결정 된다.\n대부분의 음악은 fundamental frequency와 다수의 fundamental frequency에 위치한 harmonics로 이루어져 있다. 예를 들어 A4의 fundamental frequency는 440Hz이고, harmonics는 880Hz, 1320Hz, 1760Hz \u0026hellip; 의 주기로 이루어져 있다. 악기들은 대개 연주가 될 때, fundamental frequency와 harmonics에 위치한 소리를 생성한다.\n신호처리에서 sound wave를 논할 때는 주로 3가지 기본 sound wave를 지칭한다.\n sine wave: overtone이 없는 순음(pure tone) squire wave: fundamental frequency와 fundamental frequency의 홀수 배음 harmonics saw wave: fundamental frequency와 fundamental frequency의 전체 harmonics  신호를 관찰해보면 fundamental frequency가 가장 소리가 큰 부분을 차지하고 있고, harmonics는 frequency가 증가할 수록 감소한다.\n원본의 실습에서는 각기 다른 sound wave의 소리를 직접 들어보고 이에 따른 스펙트럼(spectrum)을 확인할 수 있다. 스펙트럼이란 특정 신호 안에 담긴 frequency들을 시각화된 형태이다.\n   \u0026amp;nbsp\n실제로 실행을 하고 스펙트럼을 확인해보면, 앞에서 설명한 바와 같이 Sine Wave는 단 하나의 440Hz frequency만 있는 순음이다. 반대로 Noise Wave는 어떠한 형태의 frequency도 구별을 할 수가 없다.\nSquare Wave 와 Saw Wave를 보면 Sine Wave를 여러번 반복한 형태와 비슷하게 보인다. 또한 spectrum에서도 여러 frequency 가 공존하는 것을 볼 수 있다.\n뒤 따르는 내용에서는 어떻게 하면 여러 개의 sine wave가 모여 complex wave(복합파형)를 이루는 지를 알아보자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-15-dsp-basic-s01-5/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Timbre Harmonics, Overtones, and Wave Shapes 물체가 반복적으로 패턴을 보이며 진동할 경우, 우리의 귀는 pressure wave(압력파)를 음조(tone)이나 음의 높이(pitch)로 해석한다. 반대로 물체의 진동이 반복적이지 않거나, 예측 불가한 패턴으로 진동할 경우 우리의 귀는 이를 소음(noise)나 조성이 없는 형태(atonal)로 받아들이게 된다.\n각기 다른 패턴의 진동은 곧 각기 다른 음색(timbre)으로 연결 된다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Timbre"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다. 이런 밀고 당김의 연쇄작용이 진동을 발생하고 공기 중으로 전파가 나아갈 수 있게 한다.\n원문의 Figure 1.을 직접 참고해보기를 바란다.\n   \u0026amp;nbsp\n왼쪽 상단의 회색 직사각형이 파동의 진원지인 물체이고 무수히 많은 점들이 공기 중의 입자라고 보자. 원문의 이미지에서 이 입자들의 움직임을 잘 관찰해보면 물체의 움직임에 따라 특정 공간 내에서만 좌우로 움직이고 입자 자체가 자리를 이동하지는 않는 것을 볼 수 있다. 움직이는 물체로 인해 입자들도 비슷한 움직을을 보이게 되는데, 이것을 공명한다고 한다.\n입자 예시 아래 보이는 곡선은 기압의 정도를 나타낸다. 곡선이 수평축의 위를 지나갈 때는 공기의 입자들이 서로 컴팩트하게 모여있어 기압이 높은 상태(compression: 압축, 고밀도)이고, 반대인 경우 기압이 낮은 상태(rarefaction: 저밀도, 희박한 상태)이다.\n진동의 속도를 frequency(주파수)라고 한다. Frequency는 1초에 몇 사이클을 지났는지를 측정한 것이고, Hertz(Hz : 헤르쯔)라는 단위를 쓴다. Compression과 rarefaction의 반복 사이클을 1초에 몇번 왔다갔다 했는지를 측정하면 frequency를 구할 수 있다. 아래 Figure 1a.를 보면, compression과 rarefaction을 지나간 사이클 1번 이라고 볼 수 있겠다.\n   \u0026amp;nbsp\nFigure 1.을 통해 이것저것 시도해 보다 보면, frequency가 증가할 수록 wavelength(파장)이 줄어드는 것을 볼 수 있다. 파형(wave)의 길이(length)는 각 파형의 꼭지점이나 골짜기 사이의 거리를 측정하면 되는데, 이 거리는 frequency와 반비례 한다. 또한, frequency가 아무리 변해도 파형이 이동하는 속도는 일정하다. 해수면을 기준으로 소리는 공기에서 340m/s, 물과 같은 물질에서는 1500m/s 정도로 이동한다.\n직접 들어보자 Figure 1.처럼 부드럽게 진동하는 파형의 경우, 우리의 청각은 이 파형을 순음(pure tone)으로 받아들인다. Frequency가 낮은 음파는 낮은 음역(bass), 높은 frequency는 높은 음역(treble)이라고 하는데, 청각이 뛰어난 사람의 경우에 20 ~20,000Hz의 음역대를 들을 수 있다. 나이가 들어가면서 청력이 저하되면 들을 수 있는 음역대도 당연히 줄어들게 된다. 원문의 Figure 2.를 통해 여러가지 frequency를 한번 시도해보자.\n(실행이 되지 않는다면 아래의 링크에서 따로 실험해보자. 20,000 이상의 소리를 들어보자. 나는 들리지 않는다.)\nOnline Tone Generator - generate pure tones of any frequency\n   \u0026amp;nbsp\nX축을 보면, frequency의 눈금이 linear하지 않고 logarithmic한 것을 알 수 있다.\n440Hz는 우리가 흔히 말하는 음 \u0026ldquo;라\u0026quot;인데 A4라고 표현한다. 880Hz도 마찬가지로 \u0026ldquo;라\u0026quot;인데 한 옥타브가 높은 A5라고 한다. 여기에서 자세한 설명은 없지만, 왜 간격이 linear하지 않고 logarithmic한지 직관적으로 느낌은 알 수 있을 듯 하다\n우리의 귀는 대부분 20~8,000Hz 사이의 음역대에 반응하고, 사람의 목소리는 300~3,000Hz 사이에 속한다. 88건반 피아노는 22~4,000Hz의 기본 주파수의 소리를 내는데, 그 이상의 소리를 내는 경우도 있으며, 이를 overtone이라 한다.\n","permalink":"https://lucaseo.github.io/posts/2020-01-13-dsp-basic-s01-4/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Definition and Waves \u0026lsquo;사운드\u0026rsquo;란 공기나 물 같은 매질을 통해 전파되는 공기 압력의 파동이다. 어떤 물체가 진동을 하면 그 즉시 주변에 있는 입자들을 밀고 당기게 되는데, 이 입자들의 움직임과 압력으로 인해 이웃한 입자들로 퍼져나가거나, 빈 공간이 생기면서 압력이 낮아지고 주위의 다른 입자들이 당겨져 공간이 채워지는 움직임을 한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sound Waves"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다. 그 외의 대답은 모두 거짓말이다.\n현재 우리가 가지고 있는 이산신호의 맥락에서 봤을 때, 65분 당시의 고도에 대한 기록은 어디에도 없으므로, 우리는 확신을 가지고 이야기 할 수가 없다.\naltitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] 위와 같이 10분 주기로 기록된 현재의 이산신호는 빨강, 파랑, 주황, 초록으로 기록된 아래의 다양한 비행고도의 변화를 모두 표현(represent)할 수 있다. 연속신호로 봤을 때는 고도의 변화가 제각기 다르지만, 샘플링 주기만 봤을 때는 모두 같은 신호로 볼 수 있다. 따라서 60분과 70분의 점을 이어서 31,000 이라고 답을 하는 것은 엄청난 착각을 불러일으킬 수 있다.\n   각기 다른 네 가지 연속신호를 나타내는 빨강, 파랑, 주황, 초록 곡선      네 가지 연속신호를 10분 샘플링 주기의 이산신호로 표현했을 때   \u0026amp;nbsp\nAlias 위의 빨강, 파랑, 주황, 초록 곡선은 10분 샘플링 주기의 이산신호로 표현 될 때 서로 구별이 불가능하며, 이를 각기 서로의 alias라고 부른다. 위의 네가지 곡선은 서로 다르지만, 10분 주기로 샘플링 되었을 때, 정확히 똑같아 보이기 때문이다.\n본 자료에서 두 점을 선으로 잇지 말라고 강조하고 있으나, 이해하기 쉽도록 시각화를 하기 위해서 앞으로도 지속적으로 점과 점 사이를 선으로 이어서 예를 들 것임. 글쓴이가 선을 잇는다고 해서 선을 잇는 행동은 하지 말 것 추천함.\nRemoving Uncertainty : Frequency and Context 샘플과 샘플 사이에 간격이 있다면, 불확실성(uncertainty)가 동반된다. 샘플 간격 사이에 벌어지는 급격한(rapid fluctuation) 변화에 대한 정보가 손실 될 수 있기 때문에, 간격이 크면 클 수록 우리는 샘플링이 된 현재의 이산신호가 실제 현상을 그대로 표현한다고 확신할 수가 없게 된다. 이러한 불확실성을 줄이는 방법은 반대로 간격을 줄이는 것 즉, 샘플링 주기를 줄이는 것이다.\n비행고도 예시에서, 샘플링 주기를 10분이 아닌 5분 단위로 설정한다고 해보자. 이렇게 되면 샘플링 주기가 적용된 파랑색 곡선의 이산신호를 봤을 때, 빨강, 주황, 초록 곡선은 더 이상 파랑 곡선의 alias라고 할 수 없다. (이제는 서로 구별할 수 있다!)\n   \u0026amp;nbsp\n샘플링 주기를 무작정 줄일 수도 있겠지만, 샘플링에는 감수해야할 부분도 있다. 샘플들이 저장되는 메모리가 확보되어야 한다는 것. 따라서 샘플링 주기를 줄일 때에는 꼭 주의를 기울어야 한다.\n샘플링 주기를 제대로 설정하는 방법은 말은 쉽지만 정보의 손실이 없을 정도로 설정하는 것이다. 만약 필요한 만큼보다 더 자잘하게 샘플링 주기를 설정한다면 오버샘플링(oversampling)이 되어 메모리나 연산 리소스를 낭비하게 되고, 너무 뜸하게 설정하면 언더샘플링(undersampling)으로 정보 손실이 발생할 수 있게 된다.\n샘플링에 대한 이론은 frequency(주파수) 개념을 이해하면 좀 더 쉬워질 수 있을 것 같다. 다음에는 주파수에 대한 개념을 접할 수 있는 sound wave(음파)를 한번 알아보자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-12-dsp-basic-s01-3/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  이산신호 해석하기 Don\u0026rsquo;t connect the dots! 이산신호를 다룰 경우, 섣불리 각 점을 이어 interpolation(보간법: 중간 값을 채워 넣음)을 해서는 안 된다. 지난 번 비행 고도의 예시를 들어 보자.\n   \u0026amp;nbsp\n누군가 65분 당시의 고도를 물어본다면, 어떻게 답할 수 있을까? 우리는 당장 60분, 70분의 두 기록을 가지고 있을 뿐이니까, 그냥 단순히 두 점 사이에 선을 그어 31,000 이라고 답을 하고 싶을 수 있지\u0026hellip;만 그럴 때는 그냥 **모른다(I don\u0026rsquo;t know)**고 하는 게 정확하다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Sampling \u0026 Aliasing"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다. (\u0026lt;strong\u0026gt;원문 Figure 1. 참조\u0026lt;/strong\u0026gt;)\n   altitude = [0, 6000, 15000, 20000, 35000, 32000, 31000, 31000, 27000, 12000, 3500, 1200] Figure 1.의 파랑색 점 하나하나가 샘플이며, 그래프말고도 다음과 같이 이산값의 리스트로도 표현하고, 인덱싱(indexing)을 할 수도 있다.\n altitude[4] → 35,000 altitude[8] → 27,000  Sampling Period (샘플링 주기) 샘플링 주기(sampling period)는 연속적인 신호 사이의 지속기간(duration)을 뜻한다. 위의 비행 고도 샘플링 예시에서 각 고도를 기록하는 주기를 10분으로 놨었는데, 이때 샘플링 주기가 10분이고, 아래와 같이 나타낼 수 있다.\n$$\\text{sampling period} = 10\\text{ minutes} / 1\\text{ sample}$$\n샘플링 주기를 알 수 있다면, index에 샘플링 주기를 곱함으로서 몇 번 째 샘플이 언제 기록 되었는지를 추적(?)할 수 있다.\n$$\\text{time of 3rd sample} = 2 \\cdot 10 \\text{ minutes} / 1 \\text{ sample} = 20 \\text{ minutes}$$\n이산신호를 해석하기 위해서는 context를 파악하는 것이 매우 중요하다. 샘플링 주기를 알아야지만, 이산신호의 값들이 의미를 가지고 make sense할 수 있는 것이고, sampling period를 알지 못 한다면, 이러한 값들이 의미를 잃게 된다.\n추가적으로 \u0026hellip;    원본의 Figure 1.에서 비행기의 실제 고도를 주의깊게 봤다면, 60-70분 사이 급격한 하강 후 고도를 회복하는 부분이 있었음을 관찰할 수 있는데, 우리의 샘플링 주기는 10분이었기 때문에 정작 샘플링 당시에는 기록되지 못 했다. 샘플링 주기가 적절하지 않았기 때문에 중요한 정보가 손실 된 것이다.\n따라서 특정 물리적인 현상을 관측하기 위한 이산신호를 기록하려면, 이산신호가 그 현상을 제대로 나타낼 수 있도록 샘플링 주기를 적절하게 선택해야 한다. 이렇듯 샘플링 주기의 결정은 신호처리 분야에서도 매우 중요하게 다루는 부분 중 하나이다.\n다음 번에는 신호주기를 매우 뜸하게 설정 했을 때 나타날 수 있는 결과에 대해 좀 더 알아보도록 하자.\n","permalink":"https://lucaseo.github.io/posts/2020-01-11-dsp-basic-s01-2/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  Discrete Signals (이산 신호) Sampling and Signal Notation(샘플링과 신호의 표기)\n\u0026amp;nbsp\nSampling 어떠한 분량을 주기적으로 측정하는 행위를 샘플링(sampling), 그렇게 측정된 각각의 값을 샘플(sample)이라고 한다. 이산신호는 연속신호를 샘플링한 샘플의 모음이라고 보면 된다.\n예를 들어 두시간 동안 비행하는 비행기의 고도를 측정할 때, 10분마다 한번씩 고도를 잰다고 하면, 이것이 바로 비행기 고도를 샘플링 함으로서 이산신호를 생성하는 것이라고 볼 수 있다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: Discrete Signals"},{"content":" 본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다. 컴퓨터는 인간과 비교해서 겁나 빠르다는 장점이 있지만 또 반대로 컴퓨터는 겁나 단순해서 오직 이산값(discrete values)만 읽고 처리가 가능하다.\n하지만 현실은 그렇지 않다. 실생활에서 발생하는 대부분 연속적(continuous) 신호이다. 따라서 컴퓨터에서 분석하기 이전에 연속 신호를 이산적인, 디지털의, 딱딱 떨어지는 값으로 변환(translate)하는 과정을 거쳐야 한다.\n\u0026lt;strong\u0026gt;원문의 Figure2\u0026lt;/strong\u0026gt;를 보면 이산 신호를 통해서 연속 신호를 완벽하게 재현하는 것은 불가능 해보일 수 있다. 그리고 실제로 이는 근사화(approximation)에 그친다고 주장하는 사람도 있다.\n      하지만 DSP를 공부한다면, 이산 지점를 이용해서 연속 신호를 완벽하게 표현하는 것이 불가능하지 않다고 하니, 열심히 공부해보자!\n","permalink":"https://lucaseo.github.io/posts/2020-01-10-dsp-basic-s01-1/","summary":"본 글은 Ableton사에서 소프트웨어 개발자로 재직 중인 Jack Schaedler님의 DSP 입문 자료 \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;Seeing Circles, Sines And Signals\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; 를 통해 공부하면서 다시 풀어서 정리한 내용입니다.\n  신호란? Continuous(연속) VS. Discrete(이산)\n 신호(signal)은 물리적 현상 및 행동을 묘사한다.  시간의 흐름에 따른 신호 → time-domain signal 시간에 흐름에 따라 바뀌는 것들의 예시  비행기의 고도 변화 도시의 온도 변화 자동차의 속도      \u0026amp;nbsp\nDSP (digital signal processing) DSP는 real-world signal을 컴퓨터에서 측정, 기록, 처리, 분석하기 위한 모든 과정을 포함하는 영역을 말한다.","title":"[KR] 비전공자의 DSP 맛보기 시즌 1: 신호란?"},{"content":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.name: my-local-es node.name: local ### For Response by External Request network.host: 0.0.0.0 ### For Head http.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/elasticsearch-6.6.1 $ nohup bin/elasticsearch \u0026amp; \u0026amp;nbsp\n정상적으로 실행 중인지 확인 $ ps ax | grep elasticsearch $ curl localhost:9200 http://localhost:9200 실행\n\u0026amp;nbsp\n 02. 키바나 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-linux-x86_64.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/kibana/kibana-6.6.1-darwin-x86_64.tar.gz \u0026amp;nbsp\n압축 풀기 $ tar -xzvf kibana-6.6.1-linux-x86_64.tar.gz $ rm kibana-6.4.0-linux-x86_64.tar.gz \u0026amp;nbsp\n환경 설정 $ cd elasticstack/kibana-6.6.1-linux-x86_64/config $ vi kibana.yml server.host : 0.0.0.0 elasticsearch.url : \u0026#34;http://localhost:9200\u0026#34; kibana.index : \u0026#34;.kibana\u0026#34; \u0026amp;nbsp\n실행 $ cd elasticstack/kibana-6.6.1-linux-x86_64 $ bin/kibana \u0026amp;nbsp\n실행확인 http://localhost:5601 실행\n","permalink":"https://lucaseo.github.io/posts/2020-01-09-elasticsearch-kibana-local-env/","summary":"로컬 머신에서의 디렉토리 만들기 $ mkdir elasticstack $ cd elasticstack \u0026amp;nbsp\n01. Elasticsearch(엘라스틱서치) 설치 다운로드 Linux\n$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz MacOS\n$ curl -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.tar.gz 이하 리눅스 기준\n\u0026amp;nbsp\n압축 풀기 $ tar -xzvf elasticsearch-6.6.1.tar.gz $ rm elasticsearch-6.6.1.tar.gz \u0026amp;nbsp\nHeap 사이즈 조정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi jvm.options -Xms2g -Xmx2g \u0026amp;nbsp\n클러스터 정보 / 접근 IP 설정 $ cd ~ $ cd elastic/elasticsearch-6.6.1./config $ vi elasticsearch.yml ### For ClusterName \u0026amp; Node Name cluster.","title":"[KR] 로컬 환경에 엘라스틱서치, 키바나 설치하기"},{"content":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인. 특히 로그를 운반하는 역할.   Beats 데이터 수집, 운송 Logstash와 비슷하나, 변환 기능이 제외되어 있음. 보다 가볍게 사용할 수 있음.    이외에 Elastic Cloud와 X-pack이 추가로 있으며, 기업을 대상으로 한 Enterprise 솔루션도 확대되고 있는 추세다.\n X-pack의 경우 유료이며, 보안을 강화하여 유저에게 권한까지 부여 가능하다. X-pack의 머신러닝 기능은 현재로서는 데이터의 이상징후를 탐지하는 수준이다.  Elastic, 어떻게 좋은가?   Near Realtime\n 데이터 색인 후 약 1초 후 Refresh 시점부터 거의 실시간으로 검색결과에 반영됨    Fast\n 기본적으로 모든 Field에 대해 Indexing(색인) 처리를 하기 때문에 검색 처리 시간이 짧다    Horizontal Scalability\n Elastic Cluster에 Elasticsearch Node를 1개씩 추가하며 수평적으로 확장하기에 용이하다    Distributed Operation\n 데이터를 조각(shard)로 세분화 하여 분산 저장하기 때문에 처리 속도가 향상된다.    Replica Shard\n 데이터 조각을 복제하여 다른 Node에도 저장하기 때문에, 특정 Node가 다운되거나 손실이 생겨도 데이터 유실 없이 운영할 수 있다.    Elastic Stack에서의 용어 비교    RDBMS Excel Elastic Elastic에서의 개념     Database Excel File Index 최상위 데이터 계층. Document의 덩어리   Table Sheet Type Document를 담고 있는 컨테이너 (*)   Row Row Document 데이터 검색을 위한 최소의 단위   Column Column Field JSON으로 이루어진 데이터의 property   Schema 없음 Mapping Index Document의 저장 규칙을 의미     RDBMS, Excel과는 달리 엘라스틱에서는 1 Index에 1개의 Type만 할당되어 사실상 의미가 사라진 상태이며, 7.0버전으로 업그레이드 시 Type이란 개념은 폐지 될 예정이다.  Elastic의 Work Flow      Elasticsearch\n Mapping 설정    Logstash\n 데이터 전처리 \u0026amp; 전송    Elasticsearch\n 데이터 저장    Kibana\n Index 등록 EDA 차트 선택 Aggregation 선택 데이터 시각화 대시보드 생성    ","permalink":"https://lucaseo.github.io/posts/2020-01-05-elastic-stack/","summary":"Elastic Stack이란 Elastic Stack 이란 모든 유형의 데이터(특히 비정형 데이터)를 저장, 실시간으로 검색, 분석 및 시각화 할 수 있도록 도와주는 Elastic의 오픈소스 서비스 제품이다. 기존에 Elasticsearch, Logstash, Kibana를 같이 묶어 ELK 라는 서비스명으로 제공하기 시작했고, 현재 Beats가 포함되어 Elastic Stack 혹은 ELK Stack이란 이름으로 서비스가 제공되고 있다.\nElastic Stack의 구성    \u0026amp;nbsp\n   종류 기능 특이점     Elasticsearch 데이터 검색, 분석, 저장    Kibana 데이터 시각화, 분석    Logstash 데이터 수집, 변환, 운송 데이터 처리 파이프라인.","title":"[KR] 엘라스틱스택(Elastic Stack) 소개"},{"content":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.\n","permalink":"https://lucaseo.github.io/posts/2020-01-02-first-post/","summary":"꾸준히 하지 못 했지만, 꾸준히 해보려고 합니다\u001f. 시작이 반이니까요.","title":"[KR] 첫 포스팅"}]